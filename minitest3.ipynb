{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import chinese_calendar as cc\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from uni2ts.model.moirai import MoiraiModule\n",
    "from copy import deepcopy\n",
    "from types import SimpleNamespace\n",
    "import chinese_calendar as cc\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'  \n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'  \n",
    "else:\n",
    "    DEVICE = 'cpu'  \n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready. D= 128 | spatial in_ch= 3\n"
     ]
    }
   ],
   "source": [
    "BASE = {\n",
    "    # ===== Paths =====\n",
    "    \"data\": {\n",
    "        \"DATA_DIR\": \"UrbanEV/data\",\n",
    "        \"FN\": {\n",
    "            \"occ\": \"occupancy.csv\", \"dur\": \"duration.csv\", \"vol\": \"volume.csv\",\n",
    "            \"e_price\": \"e_price.csv\", \"s_price\": \"s_price.csv\",\n",
    "            \"weather\": \"weather_central.csv\",\n",
    "            \"inf\": \"inf.csv\", \"adj\": \"adj.csv\", \"dist\": \"distance.csv\", \"poi\": \"poi.csv\",\n",
    "        },\n",
    "    },\n",
    "\n",
    "    # ===== Data/windows =====\n",
    "    \"data_mode\": {\n",
    "        \"USE_DUMMY_DATA\": True, \"DUMMY_NUM_ZONES\": 2, \"DUMMY_NUM_HOURS\": 48,\n",
    "        \"TIMEZONE\": \"Asia/Shanghai\", \"L\": 24, \"H\": 3,  # inputs window / horizon\n",
    "        \"BASELINE_OCC_ONLY\": True,  # spatial 3ch vs 9ch\n",
    "    },\n",
    "\n",
    "    # ===== Shared dims/invariants =====\n",
    "    \"dims\": {\n",
    "        \"D\": 128,  # final embedding dim used across modules\n",
    "    },\n",
    "\n",
    "    # ===== Backbones & Embedders =====\n",
    "    \"moirai\": {\n",
    "        \"use\": True,\n",
    "        \"frozen\": True,                 # False면 파인튜닝\n",
    "        \"save_path\": None,             # Placeholder, will be set later\n",
    "        \"adapter\": {                   # Moirai 출력→D 어댑터(선택)\n",
    "            \"use\": True,\n",
    "            \"out_dim\": 128,\n",
    "            \"hidden\": 128, \"dropout\": 0.10,\n",
    "        },\n",
    "    },\n",
    "\n",
    "    \"embed\": {\n",
    "        # 각 모달별 1D-CNN 임베더 (출력은 모두 D로 맞추기)\n",
    "        \"weather\": {\"in_ch\": 9, \"hidden\": 64, \"k\": 5, \"dropout\": 0.10, \"frozen\": True},\n",
    "        \"price\":   {\"in_ch\": 2, \"hidden\": 64, \"k\": 5, \"dropout\": 0.10, \"frozen\": True},\n",
    "        \"spatial\": {\"in_ch\": 3, \"hidden\": 64, \"k\": 5, \"dropout\": 0.10, \"frozen\": True},  # 9ch일 땐 아래 derive에서 수정\n",
    "        \"time\":    {\"in_ch\": 5, \"hidden\": 64, \"k\": 5, \"dropout\": 0.10, \"frozen\": True},\n",
    "        \"static\":  {\"in_dim\": None, \"hidden\": 128, \"dropout\": 0.10, \"frozen\": True},     # in_dim은 로딩 후 설정\n",
    "    },\n",
    "\n",
    "    # ===== Align (Proj+LN+Drop+Gate) =====\n",
    "    \"align\": {\n",
    "        \"use_gate\": True, \"p_drop\": 0.10,\n",
    "    },\n",
    "\n",
    "    # ===== Fusion =====\n",
    "    \"fusion\": {\n",
    "        \"type\": \"cross_attn\",   # 나중에 \"film\", \"concat_mlp\" 등으로 교체 가능\n",
    "        \"NHEAD\": 8,             # D % NHEAD == 0 권장\n",
    "        \"dropout\": 0.10,\n",
    "    },\n",
    "\n",
    "    # ===== Head (Multi-Horizon Regression) =====\n",
    "    \"head\": {\n",
    "        \"H\": 3, \"hidden\": 512, \"dropout\": 0.10, \"nonneg\": True,\n",
    "    },\n",
    "\n",
    "    # ===== Train =====\n",
    "    \"train\": {\n",
    "        \"seed\": 42,\n",
    "        \"batch_size\": 64,\n",
    "        \"lr\": 1e-3,\n",
    "        \"epochs\": 10,\n",
    "        \"optimizer\": \"adamw\",\n",
    "        \"weight_decay\": 1e-4,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Set save_path after BASE is defined\n",
    "BASE[\"moirai\"][\"save_path\"] = (\n",
    "    \"moirai_occ_embeddings.pt\" if BASE[\"data_mode\"][\"BASELINE_OCC_ONLY\"] else \"moirai_other_embeddings.pt\"\n",
    ")\n",
    "\n",
    "# === 실험용 오버라이드: 이번 실험에서만 바꿀 값들 ===\n",
    "EXP = {\n",
    "    # 예: spatial을 9ch로 바꾸고 weather/price를 학습 가능하게\n",
    "    # \"data_mode\": {\"BASELINE_OCC_ONLY\": False},\n",
    "    # \"embed\": {\n",
    "    #     \"weather\": {\"frozen\": False, \"hidden\": 128, \"k\": 7},\n",
    "    #     \"price\":   {\"frozen\": False},\n",
    "    # },\n",
    "    # \"fusion\": {\"NHEAD\": 4},\n",
    "    # \"head\": {\"hidden\": 1024, \"dropout\": 0.2},\n",
    "}\n",
    "\n",
    "def deep_update(base: dict, over: dict):\n",
    "    out = deepcopy(base)\n",
    "    for k, v in over.items():\n",
    "        if isinstance(v, dict) and k in out and isinstance(out[k], dict):\n",
    "            out[k] = deep_update(out[k], v)\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "# === 머지 & 파생값 채우기 ===\n",
    "_cfg = deep_update(BASE, EXP)\n",
    "\n",
    "# 파생: spatial in_ch\n",
    "spatial_ch = 3 if _cfg[\"data_mode\"][\"BASELINE_OCC_ONLY\"] else 9\n",
    "_cfg[\"embed\"][\"spatial\"][\"in_ch\"] = spatial_ch\n",
    "\n",
    "# 파생: static in_dim (데이터 로드 후 알 수 있음) → 일단 None 유지\n",
    "# 파생: fusion NHEAD 유효성\n",
    "D = _cfg[\"dims\"][\"D\"]\n",
    "if _cfg[\"fusion\"][\"type\"] == \"cross_attn\":\n",
    "    assert D % _cfg[\"fusion\"][\"NHEAD\"] == 0, \"D는 NHEAD로 나눠떨어지는 것이 안전합니다.\"\n",
    "\n",
    "cfg = SimpleNamespace(**_cfg)  # dot-access\n",
    "print(\"Config ready. D=\", cfg.dims[\"D\"], \"| spatial in_ch=\", cfg.embed[\"spatial\"][\"in_ch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =========================\n",
    "# UrbanEV 데이터 유틸 함수 (보강판)\n",
    "# =========================\n",
    "# Note: Assumes `cfg` (SimpleNamespace) is available from the configuration code\n",
    "\n",
    "def normalize_zone_ids(ids):\n",
    "    \"\"\"\n",
    "    Zone IDs를 문자열로 통일:\n",
    "    - str 변환 → 공백 제거 → 끝의 '.0' 제거\n",
    "    Args:\n",
    "        ids: Iterable of zone IDs (str, int, or float)\n",
    "    Returns:\n",
    "        list[str]: Normalized zone IDs\n",
    "    \"\"\"\n",
    "    return [str(x).strip().replace(\".0\", \"\") for x in ids]\n",
    "\n",
    "def align_hourly(df, tz=None):\n",
    "    \"\"\"\n",
    "    DatetimeIndex를 1시간 단위로 맞추고, 타임존을 고정한다.\n",
    "    - 누락된 시간은 생성하고 ffill로 채움\n",
    "    - 남은 NaN은 0으로 채움\n",
    "    Args:\n",
    "        df: pandas.DataFrame with DatetimeIndex\n",
    "        tz: str, timezone (default: cfg.data_mode[\"TIMEZONE\"])\n",
    "    Returns:\n",
    "        pandas.DataFrame: Aligned DataFrame\n",
    "    \"\"\"\n",
    "    tz = tz or cfg.data_mode[\"TIMEZONE\"]\n",
    "    if df.index.tz is None:\n",
    "        df.index = df.index.tz_localize(tz)\n",
    "    else:\n",
    "        df.index = df.index.tz_convert(tz)\n",
    "    df = df.sort_index().asfreq(\"1h\")  # 1시간 간격\n",
    "    df = df.ffill().fillna(0.0)       # 결측 처리\n",
    "    return df\n",
    "\n",
    "def subset_dummy_timeseries(df, use_dummy=None, n_zones=None, n_hours=None, is_zone_table=True):\n",
    "    \"\"\"\n",
    "    더미 모드일 때 시계열 데이터에서 일부만 선택한다.\n",
    "    Args:\n",
    "        df: pandas.DataFrame, input data\n",
    "        use_dummy: bool, whether to use dummy mode (default: cfg.data_mode[\"USE_DUMMY_DATA\"])\n",
    "        n_zones: int, number of zones to select (default: cfg.data_mode[\"DUMMY_NUM_ZONES\"])\n",
    "        n_hours: int, number of hours to select (default: cfg.data_mode[\"DUMMY_NUM_HOURS\"])\n",
    "        is_zone_table: bool, whether df has zone columns (default: True)\n",
    "    Returns:\n",
    "        pandas.DataFrame: Subset DataFrame\n",
    "    \"\"\"\n",
    "    use_dummy = use_dummy if use_dummy is not None else cfg.data_mode[\"USE_DUMMY_DATA\"]\n",
    "    n_zones = n_zones if n_zones is not None else cfg.data_mode[\"DUMMY_NUM_ZONES\"]\n",
    "    n_hours = n_hours if n_hours is not None else cfg.data_mode[\"DUMMY_NUM_HOURS\"]\n",
    "\n",
    "    if use_dummy:\n",
    "        if is_zone_table:\n",
    "            if df.shape[1] < n_zones:\n",
    "                print(f\"Warning: Data has {df.shape[1]} zones, but {n_zones} requested\")\n",
    "            df = df.iloc[:, :min(n_zones, df.shape[1])]\n",
    "        if df.shape[0] < n_hours:\n",
    "            print(f\"Warning: Data has {df.shape[0]} rows, but {n_hours} requested\")\n",
    "        df = df.iloc[:min(n_hours, df.shape[0])]\n",
    "    return df\n",
    "\n",
    "def check_same_columns(dfs):\n",
    "    \"\"\"\n",
    "    여러 DataFrame들이 동일한 zone column 순서를 갖는지 확인.\n",
    "    Args:\n",
    "        dfs: list[pandas.DataFrame], DataFrames to check\n",
    "    Raises:\n",
    "        AssertionError: If column orders differ\n",
    "    \"\"\"\n",
    "    cols = None\n",
    "    for df in dfs:\n",
    "        if df is None:\n",
    "            continue\n",
    "        if cols is None:\n",
    "            cols = list(df.columns)\n",
    "        else:\n",
    "            assert list(df.columns) == cols, \"Zone column 순서가 일치하지 않습니다.\"\n",
    "\n",
    "def read_timeseries_csv(path, tz=None):\n",
    "    \"\"\"\n",
    "    첫 열이 timestamp, 나머지 열이 zone id인 시계열 CSV를 읽는다.\n",
    "    Args:\n",
    "        path: str or Path, path to CSV file\n",
    "        tz: str, timezone (default: cfg.data_mode[\"TIMEZONE\"])\n",
    "    Returns:\n",
    "        pandas.DataFrame: Aligned DataFrame with normalized zone IDs\n",
    "    \"\"\"\n",
    "    path = Path(cfg.data[\"DATA_DIR\"]) / path if not str(path).startswith(cfg.data[\"DATA_DIR\"]) else Path(path)\n",
    "    try:\n",
    "        df = pd.read_csv(path, index_col=0, parse_dates=[0])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {path} not found\")\n",
    "        return None\n",
    "    df = align_hourly(df, tz=tz)\n",
    "    df.columns = normalize_zone_ids(df.columns)\n",
    "    return df\n",
    "\n",
    "def read_square_noindex_csv(path, as_float=False, force_diag_zero=True):\n",
    "    \"\"\"\n",
    "    행 인덱스 없이 열 헤더만 있는 정방행렬 CSV를 읽는다.\n",
    "    Args:\n",
    "        path: str or Path, path to CSV file\n",
    "        as_float: bool, convert to float (default: False)\n",
    "        force_diag_zero: bool, set diagonal to zero (default: True)\n",
    "    Returns:\n",
    "        pandas.DataFrame: Square matrix with normalized zone IDs\n",
    "    \"\"\"\n",
    "    path = Path(cfg.data[\"DATA_DIR\"]) / path if not str(path).startswith(cfg.data[\"DATA_DIR\"]) else Path(path)\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=0)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {path} not found\")\n",
    "        return None\n",
    "\n",
    "    dump_cols = [c for c in df.columns if str(c).startswith(\"Unnamed\")]\n",
    "    if dump_cols:\n",
    "        df = df.drop(columns=dump_cols)\n",
    "\n",
    "    df.columns = normalize_zone_ids(df.columns)\n",
    "    df.index = df.columns.copy()\n",
    "\n",
    "    n_rows, n_cols = df.shape\n",
    "    if n_rows != n_cols:\n",
    "        raise ValueError(f\"정방 행렬이 아님: rows={n_rows}, cols={n_cols}\")\n",
    "\n",
    "    df = df.astype(float if as_float else int)\n",
    "    if force_diag_zero:\n",
    "        np.fill_diagonal(df.values, 0)\n",
    "    return df\n",
    "\n",
    "def read_inf_csv_rows_are_zones(path):\n",
    "    \"\"\"\n",
    "    inf.csv: 행=zone ID, 열=['longitude','latitude','charge_count','area','perimeter']\n",
    "    Args:\n",
    "        path: str or Path, path to CSV file\n",
    "    Returns:\n",
    "        pandas.DataFrame: (Z,5) DataFrame with normalized zone IDs\n",
    "    \"\"\"\n",
    "    path = Path(cfg.data[\"DATA_DIR\"]) / path if not str(path).startswith(cfg.data[\"DATA_DIR\"]) else Path(path)\n",
    "    expected = [\"longitude\", \"latitude\", \"charge_count\", \"area\", \"perimeter\"]\n",
    "    try:\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {path} not found\")\n",
    "        return None\n",
    "\n",
    "    df.index = normalize_zone_ids(df.index)\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "    rename_map = {\n",
    "        \"Longitude\": \"longitude\",\n",
    "        \"LATITUDE\": \"latitude\",\n",
    "        \"Charge_Count\": \"charge_count\",\n",
    "        \"Area\": \"area\",\n",
    "        \"Perimeter\": \"perimeter\"\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    for c in expected:\n",
    "        if c not in df.columns:\n",
    "            print(f\"Warning: Column {c} missing in {path}, filling with 0.0\")\n",
    "            df[c] = 0.0\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    return df[expected]\n",
    "\n",
    "def read_poi_csv(path):\n",
    "    \"\"\"\n",
    "    poi.csv: 행=primary_types, 열=['longitude','latitude']\n",
    "    Args:\n",
    "        path: str or Path, path to CSV file\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with ['longitude','latitude']\n",
    "    \"\"\"\n",
    "    path = Path(cfg.data[\"DATA_DIR\"]) / path if not str(path).startswith(cfg.data[\"DATA_DIR\"]) else Path(path)\n",
    "    try:\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {path} not found\")\n",
    "        return None\n",
    "\n",
    "    cols_l = {c.lower().strip(): c for c in df.columns}\n",
    "    lon_src = cols_l.get(\"longitude\") or cols_l.get(\"lon\") or cols_l.get(\"lng\")\n",
    "    lat_src = cols_l.get(\"latitude\") or cols_l.get(\"lat\")\n",
    "    if lon_src and lon_src != \"longitude\":\n",
    "        df = df.rename(columns={lon_src: \"longitude\"})\n",
    "    if lat_src and lat_src != \"latitude\":\n",
    "        df = df.rename(columns={lat_src: \"latitude\"})\n",
    "    for c in [\"longitude\", \"latitude\"]:\n",
    "        if c not in df.columns:\n",
    "            print(f\"Warning: Column {c} missing in {path}, filling with NA\")\n",
    "            df[c] = pd.NA\n",
    "    return df[[\"longitude\", \"latitude\"]]\n",
    "\n",
    "def expand_rain_onehot(df_weather):\n",
    "    \"\"\"\n",
    "    weather.csv에서 nRAIN(0~3)을 원핫 벡터(4채널)로 변환.\n",
    "    Args:\n",
    "        df_weather: pandas.DataFrame with 'nRAIN' column\n",
    "    Returns:\n",
    "        pandas.DataFrame: [T,P0,P,U,Td,rain_0,rain_1,rain_2,rain_3]\n",
    "    \"\"\"\n",
    "    assert \"nRAIN\" in df_weather.columns, \"weather.csv에 nRAIN이 필요합니다.\"\n",
    "    n_rain = df_weather[\"nRAIN\"].fillna(0).astype(int)\n",
    "    if n_rain.max() > 3 or n_rain.min() < 0:\n",
    "        print(f\"Warning: nRAIN values out of range [0,3]: {n_rain.min()} to {n_rain.max()}\")\n",
    "        n_rain = n_rain.clip(0, 3)\n",
    "    rain_oh = pd.get_dummies(n_rain, prefix=\"rain\")\n",
    "    for k in range(4):\n",
    "        col = f\"rain_{k}\"\n",
    "        if col not in rain_oh.columns:\n",
    "            rain_oh[col] = 0\n",
    "    rain_oh = rain_oh[[f\"rain_{k}\" for k in range(4)]]\n",
    "\n",
    "    base_cols = [\"T\", \"P0\", \"P\", \"U\", \"Td\"]\n",
    "    base = df_weather.copy()\n",
    "    for c in base_cols:\n",
    "        if c not in base.columns:\n",
    "            print(f\"Warning: Column {c} missing in weather data, filling with 0.0\")\n",
    "            base[c] = 0.0\n",
    "    base = base[base_cols]\n",
    "\n",
    "    df = pd.concat([base, rain_oh], axis=1)\n",
    "    assert df.shape[1] == 9, f\"Weather 채널이 9개여야 합니다, got {df.shape[1]}\"\n",
    "    return df\n",
    "\n",
    "def make_time_features(index, tz=None):\n",
    "    \"\"\"\n",
    "    시간 관련 특성 5채널: [hour_sin, hour_cos, dow_sin, dow_cos, is_off_cn]\n",
    "    Args:\n",
    "        index: pandas.DatetimeIndex\n",
    "        tz: str, timezone (default: cfg.data_mode[\"TIMEZONE\"])\n",
    "    Returns:\n",
    "        np.ndarray: (T, 5)\n",
    "    \"\"\"\n",
    "    tz = tz or cfg.data_mode[\"TIMEZONE\"]\n",
    "    idx = index.tz_convert(tz) if index.tz is not None else index.tz_localize(tz)\n",
    "\n",
    "    hour = idx.hour.values\n",
    "    dow = idx.dayofweek.values\n",
    "\n",
    "    hour_sin = np.sin(2 * np.pi * hour / 24)\n",
    "    hour_cos = np.cos(2 * np.pi * hour / 24)\n",
    "    dow_sin = np.sin(2 * np.pi * dow / 7)\n",
    "    dow_cos = np.cos(2 * np.pi * dow / 7)\n",
    "\n",
    "    # Vectorized workday check\n",
    "    dates = pd.to_datetime(idx)\n",
    "    is_workday = np.array([cc.is_workday(date.to_pydatetime()) for date in dates], dtype=bool)\n",
    "    is_off_cn = (~is_workday).astype(float)\n",
    "\n",
    "    feats = np.stack([hour_sin, hour_cos, dow_sin, dow_cos, is_off_cn], axis=1)\n",
    "    return feats  # (T,5)\n",
    "\n",
    "def make_W(adj_df, dist_df, eps=1e-6, clip_max=None):\n",
    "    \"\"\"\n",
    "    인접행렬과 거리행렬을 이용해 가중치 행렬 W 계산.\n",
    "    Args:\n",
    "        adj_df: pandas.DataFrame, adjacency matrix\n",
    "        dist_df: pandas.DataFrame, distance matrix\n",
    "        eps: float, small value to avoid division by zero\n",
    "        clip_max: float, optional max value for weights\n",
    "    Returns:\n",
    "        np.ndarray: (Z,Z) normalized weight matrix\n",
    "    \"\"\"\n",
    "    assert adj_df.shape == dist_df.shape, \"Adjacency and distance matrices must have same shape\"\n",
    "    assert list(adj_df.columns) == list(dist_df.columns), \"Column names must match\"\n",
    "    assert list(adj_df.index) == list(dist_df.index), \"Index names must match\"\n",
    "\n",
    "    adj = adj_df.values.astype(float)\n",
    "    dist = dist_df.values.astype(float)\n",
    "    invd = 1.0 / (dist + eps)\n",
    "    invd *= adj\n",
    "    np.fill_diagonal(invd, 0.0)\n",
    "    if clip_max is not None:\n",
    "        invd = np.clip(invd, 0, clip_max)\n",
    "\n",
    "    rowsum = invd.sum(axis=1, keepdims=True)\n",
    "    iso = (rowsum.squeeze() == 0)\n",
    "    if iso.any():\n",
    "        for i in np.where(iso)[0]:\n",
    "            invd[i, i] = 1.0\n",
    "        rowsum = invd.sum(axis=1, keepdims=True)\n",
    "\n",
    "    W = invd / rowsum\n",
    "    return W\n",
    "\n",
    "def make_spatial_features(occ_df, dur_df, vol_df, W, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Spatial features 생성: 3ch (occ only) 또는 9ch (occ+dur+vol).\n",
    "    Args:\n",
    "        occ_df: pandas.DataFrame, occupancy data\n",
    "        dur_df: pandas.DataFrame, duration data (None if 3ch)\n",
    "        vol_df: pandas.DataFrame, volume data (None if 3ch)\n",
    "        W: np.ndarray, weight matrix (Z,Z)\n",
    "        eps: float, small value to avoid division by zero\n",
    "    Returns:\n",
    "        np.ndarray: (Z,T,3) or (Z,T,9) depending on cfg.data_mode[\"BASELINE_OCC_ONLY\"]\n",
    "    \"\"\"\n",
    "    occ = occ_df.values.astype(float)\n",
    "    nbr_occ = occ @ W\n",
    "    gap_occ = occ - nbr_occ\n",
    "    ratio_occ = occ / (nbr_occ + eps)\n",
    "    \n",
    "    if cfg.data_mode[\"BASELINE_OCC_ONLY\"]:\n",
    "        arr = np.stack([nbr_occ, gap_occ, ratio_occ], axis=-1)  # (T,Z,3)\n",
    "        return np.transpose(arr, (1,0,2))  # (Z,T,3)\n",
    "\n",
    "    assert dur_df is not None and vol_df is not None, \"dur_df and vol_df required for 9ch\"\n",
    "    check_same_columns([occ_df, dur_df, vol_df])\n",
    "\n",
    "    dur = dur_df.values.astype(float)\n",
    "    vol = vol_df.values.astype(float)\n",
    "\n",
    "    nbr_dur = dur @ W\n",
    "    nbr_vol = vol @ W\n",
    "    gap_dur = dur - nbr_dur\n",
    "    gap_vol = vol - nbr_vol\n",
    "    ratio_dur = dur / (nbr_dur + eps)\n",
    "    ratio_vol = vol / (nbr_vol + eps)\n",
    "\n",
    "    chs = [nbr_occ, nbr_dur, nbr_vol,\n",
    "           gap_occ, gap_dur, gap_vol,\n",
    "           ratio_occ, ratio_dur, ratio_vol]\n",
    "    arr = np.stack(chs, axis=-1)  # (T,Z,9)\n",
    "    return np.transpose(arr, (1,0,2))  # (Z,T,9)\n",
    "\n",
    "def build_windows(index, L=None, H=None, step=1, use_future=False):\n",
    "    \"\"\"\n",
    "    슬라이딩 윈도우 인덱스 생성.\n",
    "    Args:\n",
    "        index: pandas.DatetimeIndex\n",
    "        L: int, input window length (default: cfg.data_mode[\"L\"])\n",
    "        H: int, horizon length (default: cfg.data_mode[\"H\"])\n",
    "        step: int, stride\n",
    "        use_future: bool, include horizon in window\n",
    "    Returns:\n",
    "        list[tuple]: [(start, end), ...]\n",
    "    \"\"\"\n",
    "    L = L if L is not None else cfg.data_mode[\"L\"]\n",
    "    H = H if H is not None else cfg.data_mode[\"H\"]\n",
    "    \n",
    "    Tlen = len(index)\n",
    "    out = []\n",
    "    last_t = Tlen - 1 - (H if use_future else 0)\n",
    "    if L > Tlen:\n",
    "        print(f\"Warning: Window length {L} exceeds index length {Tlen}\")\n",
    "        return []\n",
    "    for t in range(L-1, last_t+1, step):\n",
    "        s = t - (L - 1)\n",
    "        out.append((s, t))\n",
    "    return out\n",
    "\n",
    "def _zone_radius_km(area_m2, perimeter_m, use_perimeter=True, radius_scale=1.0, max_radius_km=None):\n",
    "    \"\"\"\n",
    "    Calculate zone radius (km) from area (m²) and perimeter (m).\n",
    "    Args:\n",
    "        area_m2: np.ndarray, zone areas in square meters\n",
    "        perimeter_m: np.ndarray, zone perimeters in meters\n",
    "        use_perimeter: bool, use perimeter-based radius\n",
    "        radius_scale: float, scaling factor for radius\n",
    "        max_radius_km: float, optional max radius\n",
    "    Returns:\n",
    "        np.ndarray: Radii in km\n",
    "    \"\"\"\n",
    "    r_area = np.sqrt((np.maximum(area_m2, 0.0) / 1e6) / np.pi)\n",
    "    if use_perimeter:\n",
    "        r_per = (np.maximum(perimeter_m, 0.0) / 1000.0) / (2 * np.pi)\n",
    "        r = np.minimum(r_area, r_per)\n",
    "    else:\n",
    "        r = r_area\n",
    "    r = r * float(radius_scale)\n",
    "    if max_radius_km is not None:\n",
    "        r = np.minimum(r, float(max_radius_km))\n",
    "    return r\n",
    "\n",
    "def _haversine_matrix_km(Plon, Plat, Zlon, Zlat):\n",
    "    \"\"\"\n",
    "    Haversine distance matrix (P×Z, km).\n",
    "    Args:\n",
    "        Plon: np.ndarray, POI longitudes\n",
    "        Plat: np.ndarray, POI latitudes\n",
    "        Zlon: np.ndarray, zone longitudes\n",
    "        Zlat: np.ndarray, zone latitudes\n",
    "    Returns:\n",
    "        np.ndarray: (P,Z) distance matrix in km\n",
    "    \"\"\"\n",
    "    R = 6371.0\n",
    "    lon1 = np.radians(Plon)[:, None]  # (P,1)\n",
    "    lat1 = np.radians(Plat)[:, None]  # (P,1)\n",
    "    lon2 = np.radians(Zlon)[None, :]  # (1,Z)\n",
    "    lat2 = np.radians(Zlat)[None, :]  # (1,Z)\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    return 2 * R * np.arcsin(np.sqrt(a))  # (P,Z)\n",
    "\n",
    "def poi_to_zone_counts_within_area(df_poi, df_inf_z5, zones,\n",
    "                                   use_perimeter=True,\n",
    "                                   radius_scale=1.0,\n",
    "                                   max_radius_km=None,\n",
    "                                   ensure_types=None):\n",
    "    \"\"\"\n",
    "    Count POIs within circular buffer of each zone.\n",
    "    Args:\n",
    "        df_poi: pandas.DataFrame, POI data with ['longitude','latitude']\n",
    "        df_inf_z5: pandas.DataFrame, zone info with ['longitude','latitude','charge_count','area','perimeter']\n",
    "        zones: list[str], zone IDs\n",
    "        use_perimeter: bool, use perimeter-based radius\n",
    "        radius_scale: float, scaling factor for radius\n",
    "        max_radius_km: float, optional max radius\n",
    "        ensure_types: list[str], ensure these POI types in output\n",
    "    Returns:\n",
    "        pandas.DataFrame: Zone×type count matrix\n",
    "    \"\"\"\n",
    "    if df_poi is None or len(df_poi) == 0:\n",
    "        return pd.DataFrame(index=zones)\n",
    "\n",
    "    zones = normalize_zone_ids(zones)\n",
    "    ztab = df_inf_z5.reindex(zones)\n",
    "    for c in [\"longitude\", \"latitude\", \"area\", \"perimeter\"]:\n",
    "        ztab[c] = pd.to_numeric(ztab[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    Zlon = ztab[\"longitude\"].to_numpy(float)\n",
    "    Zlat = ztab[\"latitude\"].to_numpy(float)\n",
    "    Zarea = ztab[\"area\"].to_numpy(float)\n",
    "    Zperi = ztab[\"perimeter\"].to_numpy(float)\n",
    "\n",
    "    r_km = _zone_radius_km(Zarea, Zperi, use_perimeter=use_perimeter,\n",
    "                           radius_scale=radius_scale, max_radius_km=max_radius_km)\n",
    "\n",
    "    poi = df_poi.copy()\n",
    "    poi = poi.dropna(subset=[\"longitude\", \"latitude\"])\n",
    "    if len(poi) == 0:\n",
    "        return pd.DataFrame(index=zones)\n",
    "\n",
    "    Plon = pd.to_numeric(poi[\"longitude\"], errors=\"coerce\").to_numpy()\n",
    "    Plat = pd.to_numeric(poi[\"latitude\"], errors=\"coerce\").to_numpy()\n",
    "    Ptyp = poi.index.astype(str).to_numpy()\n",
    "\n",
    "    dist = _haversine_matrix_km(Plon, Plat, Zlon, Zlat)\n",
    "    mask = dist <= r_km[None, :]\n",
    "\n",
    "    p_idx, z_idx = np.where(mask)\n",
    "    if len(p_idx) == 0:\n",
    "        return pd.DataFrame(0.0, index=zones,\n",
    "                            columns=(ensure_types if ensure_types is not None else []))\n",
    "\n",
    "    zones_arr = np.array(zones)\n",
    "    z_taken = zones_arr[z_idx]\n",
    "    t_taken = Ptyp[p_idx]\n",
    "\n",
    "    assign = pd.DataFrame({\"zone\": z_taken, \"type\": t_taken, \"cnt\": 1})\n",
    "    counts = assign.pivot_table(index=\"zone\", columns=\"type\", values=\"cnt\",\n",
    "                                aggfunc=\"sum\", fill_value=0)\n",
    "\n",
    "    if ensure_types is not None:\n",
    "        for t in ensure_types:\n",
    "            if t not in counts.columns:\n",
    "                counts[t] = 0\n",
    "        counts = counts[ensure_types]\n",
    "\n",
    "    counts = counts.reindex(index=zones).fillna(0).astype(float)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zones: 2 | Time steps: 48\n",
      "Weather 9ch: (48, 9)   (T, 9)\n",
      "Time 5ch:    (48, 5)   (T, 5)\n",
      "W matrix:    (2, 2)       (Z, Z)\n",
      "Spatial 3ch: (2, 48, 3)   (Z, T, 3)\n",
      "Static dim:  6      (inf[['charge_count', 'area', 'perimeter']] + POI types = 3)\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 3: Load data & prepare tensors (dummy/full, weather 9ch, time 5ch, spatial (3ch|9ch), static+POI) =====\n",
    "# 이 셀은 (B, C, L) 텐서를 만들지 않습니다. 다음 셀에서 윈도우 슬라이싱에 사용할 \"재료\"들만 준비합니다.\n",
    "# 필요 유틸: read_timeseries_csv, subset_dummy_timeseries, check_same_columns, normalize_zone_ids,\n",
    "#            read_square_noindex_csv, read_inf_csv_rows_are_zones, read_poi_csv,\n",
    "#            expand_rain_onehot, make_time_features, make_W, make_spatial_features,\n",
    "#            poi_to_zone_counts_within_area, _zone_radius_km\n",
    "\n",
    "# 0) 경로 준비\n",
    "occ_path = os.path.join(cfg.data[\"DATA_DIR\"], cfg.data[\"FN\"][\"occ\"])\n",
    "dur_path = os.path.join(cfg.data[\"DATA_DIR\"], cfg.data[\"FN\"][\"dur\"])\n",
    "vol_path = os.path.join(cfg.data[\"DATA_DIR\"], cfg.data[\"FN\"][\"vol\"])\n",
    "ep_path  = os.path.join(cfg.data[\"DATA_DIR\"], cfg.data[\"FN\"][\"e_price\"])\n",
    "sp_path  = os.path.join(cfg.data[\"DATA_DIR\"], cfg.data[\"FN\"][\"s_price\"])\n",
    "w_path   = os.path.join(cfg.data[\"DATA_DIR\"], cfg.data[\"FN\"][\"weather\"])\n",
    "adj_path = os.path.join(cfg.data[\"DATA_DIR\"], cfg.data[\"FN\"][\"adj\"])\n",
    "dst_path = os.path.join(cfg.data[\"DATA_DIR\"], cfg.data[\"FN\"][\"dist\"])\n",
    "inf_path = os.path.join(cfg.data[\"DATA_DIR\"], cfg.data[\"FN\"][\"inf\"])\n",
    "poi_path = os.path.join(cfg.data[\"DATA_DIR\"], cfg.data[\"FN\"][\"poi\"])\n",
    "\n",
    "# 1) 시계열 로드 + 더미/전체 전환\n",
    "df_occ = read_timeseries_csv(occ_path)\n",
    "df_dur = read_timeseries_csv(dur_path)\n",
    "df_vol = read_timeseries_csv(vol_path)\n",
    "df_ep  = read_timeseries_csv(ep_path)\n",
    "df_sp  = read_timeseries_csv(sp_path)\n",
    "df_w   = read_timeseries_csv(w_path)\n",
    "\n",
    "# Check for None returns from file loading\n",
    "for df, name in [(df_occ, \"occ\"), (df_dur, \"dur\"), (df_vol, \"vol\"),\n",
    "                 (df_ep, \"e_price\"), (df_sp, \"s_price\"), (df_w, \"weather\")]:\n",
    "    if df is None:\n",
    "        raise FileNotFoundError(f\"Failed to load {name} data\")\n",
    "\n",
    "# Apply dummy mode if enabled\n",
    "df_occ = subset_dummy_timeseries(df_occ, is_zone_table=True)\n",
    "df_dur = subset_dummy_timeseries(df_dur, is_zone_table=True)\n",
    "df_vol = subset_dummy_timeseries(df_vol, is_zone_table=True)\n",
    "df_ep  = subset_dummy_timeseries(df_ep,  is_zone_table=True)\n",
    "df_sp  = subset_dummy_timeseries(df_sp,  is_zone_table=True)\n",
    "df_w   = subset_dummy_timeseries(df_w,   is_zone_table=False)\n",
    "\n",
    "# 시계열 표들 zone 열 순서 일치 확인\n",
    "check_same_columns([df_occ, df_dur, df_vol, df_ep, df_sp])\n",
    "\n",
    "# zone 목록/개수/시간길이\n",
    "zones = list(df_occ.columns)\n",
    "Z = len(zones)\n",
    "Tlen = len(df_occ)\n",
    "\n",
    "# 2) 정방 행렬(adj, distance) 로드 후 zones 순서로 정렬\n",
    "df_adj = read_square_noindex_csv(adj_path, as_float=False)\n",
    "df_dist = read_square_noindex_csv(dst_path, as_float=True)\n",
    "if df_adj is None or df_dist is None:\n",
    "    raise FileNotFoundError(\"Failed to load adjacency or distance matrix\")\n",
    "df_adj = df_adj.loc[zones, zones]\n",
    "df_dist = df_dist.loc[zones, zones]\n",
    "\n",
    "# 3) 정적 표(inf) 로드 + reindex\n",
    "df_inf = read_inf_csv_rows_are_zones(inf_path)\n",
    "if df_inf is None:\n",
    "    print(\"Warning: inf.csv failed to load, using default zeros\")\n",
    "    df_inf = pd.DataFrame(0.0, index=zones,\n",
    "                          columns=[\"longitude\", \"latitude\", \"charge_count\", \"area\", \"perimeter\"])\n",
    "df_inf = df_inf.reindex(zones)\n",
    "\n",
    "# 4) POI 표 로드\n",
    "df_poi = read_poi_csv(poi_path)\n",
    "if df_poi is None:\n",
    "    print(\"Warning: poi.csv failed to load, using None\")\n",
    "\n",
    "# 5) 파생 특성: Weather 9채널, Time 5채널\n",
    "df_w9 = expand_rain_onehot(df_w)  # (T, 9)\n",
    "assert df_w9.shape[1] == cfg.embed[\"weather\"][\"in_ch\"], f\"Weather channels must be {cfg.embed['weather']['in_ch']}\"\n",
    "time5 = make_time_features(df_occ.index)  # (T, 5) np.ndarray\n",
    "assert time5.shape[1] == cfg.embed[\"time\"][\"in_ch\"], f\"Time channels must be {cfg.embed['time']['in_ch']}\"\n",
    "time6 = time5  # Compatibility\n",
    "\n",
    "# 6) 공간 가중치 W 및 Spatial (3ch | 9ch)\n",
    "W = make_W(df_adj, df_dist, eps=1e-6, clip_max=None)  # (Z, Z)\n",
    "spatial_Z_T = make_spatial_features(\n",
    "    df_occ,\n",
    "    None if cfg.data_mode[\"BASELINE_OCC_ONLY\"] else df_dur,\n",
    "    None if cfg.data_mode[\"BASELINE_OCC_ONLY\"] else df_vol,\n",
    "    W\n",
    ")  # (Z, T, 3|9)\n",
    "assert spatial_Z_T.shape[2] == cfg.embed[\"spatial\"][\"in_ch\"], f\"Spatial channels must be {cfg.embed['spatial']['in_ch']}\"\n",
    "\n",
    "# 7) 정적 특성(Static): inf + POI type counts(원형 버퍼 근사) 결합\n",
    "base_cols = [\"charge_count\", \"area\", \"perimeter\"]  # Include perimeter for consistency\n",
    "inf_z5 = df_inf  # (Z,5)\n",
    "\n",
    "# POI 설정 (cfg에 추가 가능)\n",
    "POI_USE_PERIMETER = True\n",
    "POI_RADIUS_SCALE = 1.0\n",
    "POI_MAX_RADIUS_KM = None\n",
    "PRIMARY_TYPES = ['lifestyle services', 'business and residential', 'food and beverage services']\n",
    "\n",
    "if df_poi is not None and len(df_poi) > 0:\n",
    "    poi_counts = poi_to_zone_counts_within_area(\n",
    "        df_poi=df_poi,\n",
    "        df_inf_z5=inf_z5,\n",
    "        zones=zones,\n",
    "        use_perimeter=POI_USE_PERIMETER,\n",
    "        radius_scale=POI_RADIUS_SCALE,\n",
    "        max_radius_km=POI_MAX_RADIUS_KM,\n",
    "        ensure_types=PRIMARY_TYPES\n",
    "    )\n",
    "else:\n",
    "    poi_counts = pd.DataFrame(index=zones, columns=PRIMARY_TYPES).fillna(0.0)\n",
    "\n",
    "static_df = pd.concat([inf_z5[base_cols], poi_counts], axis=1).fillna(0.0)\n",
    "static_arr = static_df.values.astype(float)  # (Z, F_static)\n",
    "F_static = static_arr.shape[1]\n",
    "cfg.embed[\"static\"][\"in_dim\"] = F_static  # Update cfg with static dimension\n",
    "\n",
    "# 8) 요약 출력\n",
    "print(f\"Zones: {Z} | Time steps: {Tlen}\")\n",
    "print(f\"Weather {cfg.embed['weather']['in_ch']}ch: {df_w9.shape}   (T, {cfg.embed['weather']['in_ch']})\")\n",
    "print(f\"Time {cfg.embed['time']['in_ch']}ch:    {time5.shape}   (T, {cfg.embed['time']['in_ch']})\")\n",
    "print(f\"W matrix:    {W.shape}       (Z, Z)\")\n",
    "print(f\"Spatial {cfg.embed['spatial']['in_ch']}ch: {spatial_Z_T.shape}   (Z, T, {cfg.embed['spatial']['in_ch']})\")\n",
    "print(f\"Static dim:  {F_static}      (inf[{base_cols}] + POI types = {poi_counts.shape[1]})\")\n",
    "\n",
    "# 이 셀의 산출물:\n",
    "# - df_occ, df_dur, df_vol        (T, Z)\n",
    "# - df_ep, df_sp                  (T, Z)\n",
    "# - df_w9                         (T, 9)\n",
    "# - time5                         (T, 5)  (np.ndarray)  # (호환: time6 = time5)\n",
    "# - df_adj, df_dist               (Z, Z)\n",
    "# - W                             (Z, Z)\n",
    "# - spatial_Z_T                   (Z, T, 3|9)\n",
    "# - static_arr                    (Z, F_static)\n",
    "# - zones, Z, Tlen, F_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_id     : (50,) (B,)\n",
      "window      : (50,) (B,)\n",
      "x_local_occ : (50, 1, 24) (B, 1, 24)\n",
      "x_price     : (50, 2, 24) (B, 2, 24)\n",
      "x_weather   : (50, 9, 24) (B, 9, 24)\n",
      "x_time      : (50, 5, 24) (B, 5, 24)\n",
      "x_spatial   : (50, 3, 24) (B, 3, 24)\n",
      "x_static    : (50, 6) (B, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ===== Cell 4: Window builder & lightweight Dataset (no target) =====\n",
    "# 목적: 슬라이딩 윈도우로 (B, C, L) 배치 텐서 생성 (타깃 없음, 임베딩 모듈 shape 체크용)\n",
    "\n",
    "# --------- helpers: slice functions (return channel-first C x L) ---------\n",
    "def slice_occ_block(df_occ, s, t):\n",
    "    \"\"\"\n",
    "    Slice occupancy data for a window.\n",
    "    Args:\n",
    "        df_occ: pandas.DataFrame, occupancy data (T, Z)\n",
    "        s: int, start index\n",
    "        t: int, end index (inclusive)\n",
    "    Returns:\n",
    "        np.ndarray: (Z, 1, L), occupancy data\n",
    "    \"\"\"\n",
    "    if df_occ is None or df_occ.empty:\n",
    "        raise ValueError(\"df_occ is None or empty\")\n",
    "    occ = df_occ.iloc[s:t+1].to_numpy(dtype=np.float32).T  # (Z, L)\n",
    "    return occ[:, None, :]                                 # (Z, 1, L)\n",
    "\n",
    "def slice_price_block(df_ep, df_sp, s, t):\n",
    "    \"\"\"\n",
    "    Slice price data (electricity and service) for a window.\n",
    "    Args:\n",
    "        df_ep: pandas.DataFrame, electricity price data (T, Z)\n",
    "        df_sp: pandas.DataFrame, service price data (T, Z)\n",
    "        s: int, start index\n",
    "        t: int, end index (inclusive)\n",
    "    Returns:\n",
    "        np.ndarray: (Z, 2, L), price data\n",
    "    \"\"\"\n",
    "    if df_ep is None or df_sp is None or df_ep.empty or df_sp.empty:\n",
    "        raise ValueError(\"df_ep or df_sp is None or empty\")\n",
    "    ep = df_ep.iloc[s:t+1].to_numpy(dtype=np.float32).T    # (Z, L)\n",
    "    sp = df_sp.iloc[s:t+1].to_numpy(dtype=np.float32).T\n",
    "    return np.stack([ep, sp], axis=1)                      # (Z, 2, L)\n",
    "\n",
    "def slice_weather_block(df_w9, s, t, Z):\n",
    "    \"\"\"\n",
    "    Slice weather data and broadcast to zones.\n",
    "    Args:\n",
    "        df_w9: pandas.DataFrame, weather data (T, 9)\n",
    "        s: int, start index\n",
    "        t: int, end index (inclusive)\n",
    "        Z: int, number of zones\n",
    "    Returns:\n",
    "        np.ndarray: (Z, 9, L), broadcasted weather data\n",
    "    \"\"\"\n",
    "    if df_w9 is None or df_w9.empty:\n",
    "        raise ValueError(\"df_w9 is None or empty\")\n",
    "    w = df_w9.iloc[s:t+1].to_numpy(dtype=np.float32).T     # (9, L)\n",
    "    return np.broadcast_to(w, (Z,)+w.shape)                # (Z, 9, L)\n",
    "\n",
    "def slice_time_block(time_feat, s, t, Z):\n",
    "    \"\"\"\n",
    "    Slice time features and broadcast to zones.\n",
    "    Args:\n",
    "        time_feat: np.ndarray, time features (T, C_time)\n",
    "        s: int, start index\n",
    "        t: int, end index (inclusive)\n",
    "        Z: int, number of zones\n",
    "    Returns:\n",
    "        np.ndarray: (Z, C_time, L), broadcasted time features\n",
    "    \"\"\"\n",
    "    if time_feat is None or time_feat.size == 0:\n",
    "        raise ValueError(\"time_feat is None or empty\")\n",
    "    tm = time_feat[s:t+1].astype(np.float32).T             # (C_time, L)\n",
    "    return np.broadcast_to(tm, (Z,)+tm.shape)              # (Z, C_time, L)\n",
    "\n",
    "def slice_spatial_block(spatial_Z_T, s, t):\n",
    "    \"\"\"\n",
    "    Slice spatial features for a window.\n",
    "    Args:\n",
    "        spatial_Z_T: np.ndarray, spatial features (Z, T, C_spatial)\n",
    "        s: int, start index\n",
    "        t: int, end index (inclusive)\n",
    "    Returns:\n",
    "        np.ndarray: (Z, C_spatial, L)\n",
    "    \"\"\"\n",
    "    if spatial_Z_T is None or spatial_Z_T.size == 0:\n",
    "        raise ValueError(\"spatial_Z_T is None or empty\")\n",
    "    return np.transpose(spatial_Z_T[:, s:t+1, :].astype(np.float32), (0, 2, 1))\n",
    "\n",
    "# --------- Dataset (lightweight, no target) ---------\n",
    "class EVEmbeddingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each sample = one zone-window. Returns channel-first tensors (C, L).\n",
    "    Keys:\n",
    "        - zone_id:      torch.LongTensor scalar\n",
    "        - window:       torch.LongTensor scalar\n",
    "        - x_local_occ:  (1, L), occupancy for Moirai (occ-only)\n",
    "        - x_local:      (3, L), occ+dur+vol (if not BASELINE_OCC_ONLY)\n",
    "        - x_price:      (2, L), electricity and service prices\n",
    "        - x_weather:    (9, L), weather features\n",
    "        - x_time:       (C_time, L), time features (usually 5)\n",
    "        - x_spatial:    (C_spatial, L), spatial features (3 or 9)\n",
    "        - x_static:     (F_static,), static features\n",
    "    \"\"\"\n",
    "    def __init__(self, windows, df_occ, df_dur, df_vol, df_ep, df_sp,\n",
    "                 df_w9, time_feat, spatial_Z_T, static_arr, zones):\n",
    "        super().__init__()\n",
    "        self.windows = windows\n",
    "        self.df_occ = df_occ\n",
    "        self.df_dur = df_dur if not cfg.data_mode[\"BASELINE_OCC_ONLY\"] else None\n",
    "        self.df_vol = df_vol if not cfg.data_mode[\"BASELINE_OCC_ONLY\"] else None\n",
    "        self.df_ep, self.df_sp = df_ep, df_sp\n",
    "        self.df_w9 = df_w9\n",
    "        self.time_feat = time_feat\n",
    "        self.spatial = spatial_Z_T\n",
    "        self.static_arr = static_arr\n",
    "        self.zones = zones\n",
    "        self.Z = len(zones)\n",
    "\n",
    "        # Sanity checks\n",
    "        if self.df_occ is None or self.df_occ.empty:\n",
    "            raise ValueError(\"df_occ is None or empty\")\n",
    "        T = self.df_occ.shape[0]\n",
    "        assert all(df is None or df.shape[0] == T for df in (self.df_dur, self.df_vol, self.df_ep, self.df_sp, self.df_w9)), \\\n",
    "            \"All time-series must share the same T length\"\n",
    "        assert self.time_feat.shape[0] == T, \"Time features T must match occ T\"\n",
    "        assert self.spatial.shape[0] == self.Z == self.static_arr.shape[0], \"Z mismatch among spatial/static/zones\"\n",
    "        assert self.spatial.shape[2] == cfg.embed[\"spatial\"][\"in_ch\"], f\"Spatial channels must be {cfg.embed['spatial']['in_ch']}\"\n",
    "        assert self.df_w9.shape[1] == cfg.embed[\"weather\"][\"in_ch\"], f\"Weather channels must be {cfg.embed['weather']['in_ch']}\"\n",
    "        assert self.time_feat.shape[1] == cfg.embed[\"time\"][\"in_ch\"], f\"Time channels must be {cfg.embed['time']['in_ch']}\"\n",
    "        assert self.static_arr.shape[1] == cfg.embed[\"static\"][\"in_dim\"], f\"Static dim must be {cfg.embed['static']['in_dim']}\"\n",
    "        if not cfg.data_mode[\"BASELINE_OCC_ONLY\"]:\n",
    "            assert self.df_dur is not None and self.df_vol is not None, \"df_dur and df_vol required for multi-channel\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.Z * len(self.windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        nW = len(self.windows)\n",
    "        z = idx // nW\n",
    "        w = idx % nW\n",
    "        s, t = self.windows[w]\n",
    "\n",
    "        # Slice data\n",
    "        occ_only = slice_occ_block(self.df_occ, s, t)[z]                      # (1, L)\n",
    "        if not cfg.data_mode[\"BASELINE_OCC_ONLY\"]:\n",
    "            local = slice_local_block(self.df_occ, self.df_dur, self.df_vol, s, t)[z]  # (3, L)\n",
    "        else:\n",
    "            local = occ_only  # Fallback to occ-only\n",
    "        price = slice_price_block(self.df_ep, self.df_sp, s, t)[z]            # (2, L)\n",
    "        weather = slice_weather_block(self.df_w9, s, t, self.Z)[z]            # (9, L)\n",
    "        timeb = slice_time_block(self.time_feat, s, t, self.Z)[z]             # (C_time, L)\n",
    "        spatial = slice_spatial_block(self.spatial, s, t)[z]                  # (C_spatial, L)\n",
    "        static = self.static_arr[z].astype(np.float32)                        # (F_static,)\n",
    "\n",
    "        sample = {\n",
    "            \"zone_id\": torch.tensor(z, dtype=torch.long),\n",
    "            \"window\": torch.tensor(w, dtype=torch.long),\n",
    "            \"x_local_occ\": torch.tensor(occ_only, dtype=torch.float32),\n",
    "            \"x_local\": torch.tensor(local, dtype=torch.float32),\n",
    "            \"x_price\": torch.tensor(price, dtype=torch.float32),\n",
    "            \"x_weather\": torch.tensor(weather, dtype=torch.float32),\n",
    "            \"x_time\": torch.tensor(timeb, dtype=torch.float32),\n",
    "            \"x_spatial\": torch.tensor(spatial, dtype=torch.float32),\n",
    "            \"x_static\": torch.tensor(static, dtype=torch.float32),\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "# Update collate_fn to handle x_local\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate samples into a batch with channel-first tensors (B, C, L).\n",
    "    Args:\n",
    "        batch: list[dict], list of samples from EVEmbeddingDataset\n",
    "    Returns:\n",
    "        dict: Batched tensors\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    out[\"zone_id\"] = torch.stack([b[\"zone_id\"] for b in batch], dim=0)  # (B,)\n",
    "    out[\"window\"] = torch.stack([b[\"window\"] for b in batch], dim=0)    # (B,)\n",
    "    for k, expected_ch in [\n",
    "        (\"x_local_occ\", 1),\n",
    "        (\"x_local\", 3 if not cfg.data_mode[\"BASELINE_OCC_ONLY\"] else 1),\n",
    "        (\"x_weather\", cfg.embed[\"weather\"][\"in_ch\"]),\n",
    "        (\"x_price\", cfg.embed[\"price\"][\"in_ch\"]),\n",
    "        (\"x_spatial\", cfg.embed[\"spatial\"][\"in_ch\"]),\n",
    "        (\"x_time\", cfg.embed[\"time\"][\"in_ch\"])\n",
    "    ]:\n",
    "        stacked = torch.stack([b[k] for b in batch], dim=0)  # (B, C, L) or (B, L, C)\n",
    "        if stacked.shape[1] != expected_ch:\n",
    "            stacked = stacked.transpose(1, 2)  # (B, L, C) -> (B, C, L)\n",
    "        assert stacked.shape[1] == expected_ch, f\"{k} must have {expected_ch} channels, got {stacked.shape[1]}\"\n",
    "        out[k] = stacked\n",
    "    out[\"x_static\"] = torch.stack([b[\"x_static\"] for b in batch], dim=0)  # (B, F_static)\n",
    "    assert out[\"x_static\"].shape[1] == cfg.embed[\"static\"][\"in_dim\"], f\"x_static shape mismatch: expected {cfg.embed['static']['in_dim']}, got {out['x_static'].shape[1]}\"\n",
    "    return out\n",
    "\n",
    "# Rebuild DataLoader with updated dataset\n",
    "windows = build_windows(df_occ.index)\n",
    "if len(windows) == 0:\n",
    "    raise ValueError(f\"No windows built. Check L({cfg.data_mode['L']}) <= T({len(df_occ)}).\")\n",
    "\n",
    "dataset = EVEmbeddingDataset(\n",
    "    windows=windows,\n",
    "    df_occ=df_occ,\n",
    "    df_dur=df_dur,\n",
    "    df_vol=df_vol,\n",
    "    df_ep=df_ep,\n",
    "    df_sp=df_sp,\n",
    "    df_w9=df_w9,\n",
    "    time_feat=time5,\n",
    "    spatial_Z_T=spatial_Z_T,\n",
    "    static_arr=static_arr,\n",
    "    zones=zones\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=cfg.train[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# --------- Smoke test: take one batch and print shapes ---------\n",
    "batch = next(iter(loader))\n",
    "print(\"zone_id     :\", tuple(batch[\"zone_id\"].shape), \"(B,)\")\n",
    "print(\"window      :\", tuple(batch[\"window\"].shape), \"(B,)\")\n",
    "print(\"x_local_occ :\", tuple(batch[\"x_local_occ\"].shape), f\"(B, 1, {cfg.data_mode['L']})\")\n",
    "print(\"x_price     :\", tuple(batch[\"x_price\"].shape), f\"(B, {cfg.embed['price']['in_ch']}, {cfg.data_mode['L']})\")\n",
    "print(\"x_weather   :\", tuple(batch[\"x_weather\"].shape), f\"(B, {cfg.embed['weather']['in_ch']}, {cfg.data_mode['L']})\")\n",
    "print(\"x_time      :\", tuple(batch[\"x_time\"].shape), f\"(B, {cfg.embed['time']['in_ch']}, {cfg.data_mode['L']})\")\n",
    "print(\"x_spatial   :\", tuple(batch[\"x_spatial\"].shape), f\"(B, {cfg.embed['spatial']['in_ch']}, {cfg.data_mode['L']})\")\n",
    "print(\"x_static    :\", tuple(batch[\"x_static\"].shape), f\"(B, {cfg.embed['static']['in_dim']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moirai Embeddings: 100%|██████████| 1/1 [00:00<00:00,  9.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_local total: (50, 128) | zones idx shape: (50,)\n",
      "Saved to: moirai_occ_embeddings.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from contextlib import contextmanager\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ===== Cell 5: Moirai(frozen) Feature Extractor with Adapter =====\n",
    "# Supports: \n",
    "# - Single-channel (occ-only) or multi-channel (occ, dur, vol)\n",
    "# - Multi-scale patch sizes\n",
    "# - Adapter to map Moirai embeddings to cfg.dims[\"D\"]\n",
    "\n",
    "# 1) Device setup\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2) Adapter module\n",
    "class MoiraiAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapter to map Moirai embeddings to cfg.dims[\"D\"].\n",
    "    Args:\n",
    "        in_dim: int, input dimension (Moirai embedding dim)\n",
    "        hidden: int, hidden dimension (from cfg.moirai[\"adapter\"][\"hidden\"])\n",
    "        out_dim: int, output dimension (from cfg.dims[\"D\"])\n",
    "        dropout: float, dropout rate (from cfg.moirai[\"adapter\"][\"dropout\"])\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden, out_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# 3) Load frozen Moirai backbone\n",
    "backbone = MoiraiModule.from_pretrained(\"Salesforce/moirai-1.0-R-base\").to(DEVICE).eval()\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# 4) Pick top-level encoder\n",
    "def pick_top_encoder(m):\n",
    "    \"\"\"\n",
    "    Find the top-level encoder module in the Moirai backbone.\n",
    "    Args:\n",
    "        m: nn.Module, Moirai backbone\n",
    "    Returns:\n",
    "        tuple: (encoder_name, encoder_module)\n",
    "    \"\"\"\n",
    "    if hasattr(m, \"encoder\"):\n",
    "        return \"encoder\", m.encoder\n",
    "    cand = [(n, mod) for n, mod in m.named_modules() if n == \"encoder\" or n.endswith(\".encoder\")]\n",
    "    if cand:\n",
    "        cand.sort(key=lambda x: len(x[0]))\n",
    "        return cand[0]\n",
    "    cand = [(n, mod) for n, mod in m.named_modules() if \"encoder\" in n.lower()]\n",
    "    if cand:\n",
    "        cand.sort(key=lambda x: len(x[0]))\n",
    "        return cand[0]\n",
    "    raise RuntimeError(f\"Could not find an encoder module in Moirai backbone: {list(m.named_modules())}\")\n",
    "\n",
    "@contextmanager\n",
    "def encoder_hook(module):\n",
    "    \"\"\"\n",
    "    Context manager for registering and removing a forward hook.\n",
    "    Args:\n",
    "        module: nn.Module, encoder module\n",
    "    Yields:\n",
    "        dict: Cache for hidden states\n",
    "    \"\"\"\n",
    "    cache = {\"h\": None}\n",
    "    def hook_fn(module, inp, out):\n",
    "        if torch.is_tensor(out) and out.ndim == 3:\n",
    "            cache[\"h\"] = out.detach()\n",
    "    handle = module.register_forward_hook(hook_fn)\n",
    "    try:\n",
    "        yield cache\n",
    "    finally:\n",
    "        handle.remove()\n",
    "\n",
    "# 5) Encode a batch with adapter\n",
    "@torch.no_grad()\n",
    "def encode_batch(batch_dict: dict, patch_sizes=[1], use_multi_channel=False, adapter=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Encode a batch using Moirai backbone and apply adapter.\n",
    "    Args:\n",
    "        batch_dict: dict, batch from DataLoader\n",
    "        patch_sizes: list[int], patch sizes for multi-scale encoding\n",
    "        use_multi_channel: bool, use occ+dur+vol (True) or occ-only (False)\n",
    "        adapter: nn.Module, adapter to map embeddings to cfg.dims[\"D\"] (optional)\n",
    "    Returns:\n",
    "        torch.Tensor: (B, D), adapted embeddings\n",
    "    \"\"\"\n",
    "    x_local = batch_dict[\"x_local\"].to(DEVICE) if use_multi_channel else batch_dict[\"x_local_occ\"].to(DEVICE)  # (B, C, L)\n",
    "    B, C, L = x_local.shape\n",
    "    target = x_local.transpose(1, 2).contiguous()  # (B, L, C)\n",
    "\n",
    "    # Initialize output embeddings\n",
    "    all_hs = []\n",
    "    \n",
    "    # Get encoder module\n",
    "    enc_name, enc_mod = pick_top_encoder(backbone)\n",
    "    \n",
    "    # Multi-scale encoding\n",
    "    for ps in patch_sizes:\n",
    "        with encoder_hook(enc_mod) as cache:\n",
    "            observed_mask = torch.ones((B, L, C), dtype=torch.bool, device=DEVICE)  # (B, L, C)\n",
    "            prediction_mask = torch.zeros((B, L), dtype=torch.bool, device=DEVICE)  # (B, L)\n",
    "            patch_size = torch.full((B, L), ps, dtype=torch.long, device=DEVICE)   # (B, L)\n",
    "            sample_id = torch.arange(B, device=DEVICE).unsqueeze(1).expand(B, L)   # (B, L)\n",
    "            time_id = torch.arange(L, device=DEVICE).unsqueeze(0).expand(B, L)     # (B, L)\n",
    "            variate_id = torch.zeros((B, L), dtype=torch.long, device=DEVICE)      # (B, L)\n",
    "\n",
    "            _ = backbone(\n",
    "                target=target,\n",
    "                observed_mask=observed_mask,\n",
    "                sample_id=sample_id,\n",
    "                time_id=time_id,\n",
    "                variate_id=variate_id,\n",
    "                prediction_mask=prediction_mask,\n",
    "                patch_size=patch_size,\n",
    "            )\n",
    "            hs = cache[\"h\"]\n",
    "            if hs is None:\n",
    "                raise RuntimeError(f\"Encoder hidden states not captured for patch_size={ps}\")\n",
    "            \n",
    "            # Handle (B, N, D_moirai) or (N, B, D_moirai)\n",
    "            if hs.shape[0] == B:\n",
    "                hs = hs.mean(dim=1)  # (B, D_moirai)\n",
    "            elif hs.shape[1] == B:\n",
    "                hs = hs.mean(dim=0)  # (B, D_moirai)\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unexpected hidden shape: {tuple(hs.shape)} (B={B})\")\n",
    "            all_hs.append(hs)\n",
    "    \n",
    "    # Aggregate multi-scale embeddings\n",
    "    emb = torch.stack(all_hs, dim=0).mean(dim=0)  # (B, D_moirai)\n",
    "    \n",
    "    # Apply adapter if provided\n",
    "    if adapter is not None:\n",
    "        emb = adapter(emb.to(DEVICE)).cpu()  # (B, D)\n",
    "        assert emb.shape[1] == cfg.dims[\"D\"], f\"Adapted embedding dim must be {cfg.dims['D']}, got {emb.shape[1]}\"\n",
    "    \n",
    "    return emb\n",
    "\n",
    "# 6) Run over the loader and save embeddings\n",
    "patch_sizes = [1, 4, 8] if cfg.moirai.get(\"multi_scale\", False) else [1]\n",
    "use_multi_channel = not cfg.data_mode[\"BASELINE_OCC_ONLY\"]\n",
    "\n",
    "# Infer Moirai embedding dimension from first batch\n",
    "first_batch = next(iter(loader))\n",
    "with torch.no_grad():\n",
    "    first_emb = encode_batch(first_batch, patch_sizes=patch_sizes, use_multi_channel=use_multi_channel)  # (B, D_moirai)\n",
    "D_moirai = first_emb.shape[1]\n",
    "\n",
    "# Initialize adapter if enabled\n",
    "adapter = None\n",
    "if cfg.moirai[\"adapter\"][\"use\"]:\n",
    "    adapter = MoiraiAdapter(\n",
    "        in_dim=D_moirai,\n",
    "        hidden=cfg.moirai[\"adapter\"][\"hidden\"],\n",
    "        out_dim=cfg.dims[\"D\"],\n",
    "        dropout=cfg.moirai[\"adapter\"][\"dropout\"]\n",
    "    ).to(DEVICE).eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moirai Embeddings: 100%|██████████| 1/1 [00:00<00:00,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_local total: (50, 128) | zones idx shape: (50,)\n",
      "Saved to: moirai_occ_embeddings.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_embs, all_zone_ids = [], []\n",
    "loader_iterator = tqdm(loader, desc=\"Moirai Embeddings\")\n",
    "for batch in loader_iterator:\n",
    "    emb = encode_batch(\n",
    "        batch,\n",
    "        patch_sizes=patch_sizes,\n",
    "        use_multi_channel=use_multi_channel,\n",
    "        adapter=adapter\n",
    "    )  # (B, D) or (B, D_moirai) if no adapter\n",
    "    all_embs.append(emb)\n",
    "    all_zone_ids.append(batch[\"zone_id\"].cpu())\n",
    "\n",
    "emb = torch.cat(all_embs, dim=0)  # (N_samples, D) or (N_samples, D_moirai)\n",
    "zones_taken = torch.cat(all_zone_ids, dim=0)  # (N_samples,)\n",
    "print(f\"h_local total: {tuple(emb.shape)} | zones idx shape: {tuple(zones_taken.shape)}\")\n",
    "print(f\"Saved to: {cfg.moirai['save_path']}\")\n",
    "\n",
    "torch.save({\"h_local\": emb, \"zone_id\": zones_taken}, cfg.moirai[\"save_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1dEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic 1D CNN Embedder.\n",
    "    Args:\n",
    "        in_ch: int, input channels (from cfg.embed[...][\"in_ch\"])\n",
    "        out_dim: int, output dimension (from cfg.dims[\"D\"])\n",
    "        hidden: int, hidden channels (from cfg.cnn[\"hidden\"])\n",
    "        kernel_size: int, convolutional kernel size (from cfg.cnn[\"kernel_size\"])\n",
    "        dropout: float, dropout rate (from cfg.cnn[\"dropout\"])\n",
    "    Input: (B, C_in, L)\n",
    "    Output: (B, out_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, out_dim: int, hidden: int, kernel_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, hidden, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden, hidden, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)  # Global average over L\n",
    "        self.proj = nn.Linear(hidden, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: torch.Tensor, (B, C_in, L)\n",
    "        Returns:\n",
    "            torch.Tensor, (B, out_dim)\n",
    "        \"\"\"\n",
    "        if x.ndim != 3:\n",
    "            raise ValueError(f\"Expected 3D input (B, C_in, L), got shape {x.shape}\")\n",
    "        h = self.conv(x)              # (B, hidden, L)\n",
    "        h = self.pool(h).squeeze(-1)  # (B, hidden)\n",
    "        h = self.proj(h)              # (B, out_dim)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_weather: torch.Size([50, 128]) (expected: (B, 128))\n",
      "h_price  : torch.Size([50, 128]) (expected: (B, 128))\n",
      "h_spatial: torch.Size([50, 128]) (expected: (B, 128))\n",
      "h_time   : torch.Size([50, 128]) (expected: (B, 128))\n",
      "Saved CNN embeddings to: moirai_occ_embeddings_cnn.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# ===== Cell 6: CNN1d Embedders for Weather, Price, Spatial, Time =====\n",
    "# Aligns with cfg.dims[\"D\"] and previous cells\n",
    "\n",
    "# 1) CNN1d Embedder class\n",
    "class CNN1dEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic 1D CNN Embedder.\n",
    "    Args:\n",
    "        in_ch: int, input channels (from cfg.embed[...][\"in_ch\"])\n",
    "        out_dim: int, output dimension (from cfg.dims[\"D\"])\n",
    "        hidden: int, hidden channels (from cfg.cnn.hidden)\n",
    "        kernel_size: int, convolutional kernel size (from cfg.cnn.kernel_size)\n",
    "        dropout: float, dropout rate (from cfg.cnn.dropout)\n",
    "    Input: (B, C_in, L)\n",
    "    Output: (B, out_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, out_dim: int, hidden: int, kernel_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, hidden, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden, hidden, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)  # Global average over L\n",
    "        self.proj = nn.Linear(hidden, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: torch.Tensor, (B, C_in, L)\n",
    "        Returns:\n",
    "            torch.Tensor, (B, out_dim)\n",
    "        \"\"\"\n",
    "        if x.ndim != 3:\n",
    "            raise ValueError(f\"Expected 3D input (B, C_in, L), got shape {x.shape}\")\n",
    "        h = self.conv(x)              # (B, hidden, L)\n",
    "        h = self.pool(h).squeeze(-1)  # (B, hidden)\n",
    "        h = self.proj(h)              # (B, out_dim)\n",
    "        return h\n",
    "\n",
    "# 2) Initialize embedders\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# CNN configuration (add to cfg if not present)\n",
    "if not hasattr(cfg, \"cnn\"):\n",
    "    cfg.cnn = SimpleNamespace(\n",
    "        hidden=64,\n",
    "        kernel_size=5,\n",
    "        dropout=0.1\n",
    "    )\n",
    "\n",
    "# Validate configuration\n",
    "required_embed_keys = [\"weather\", \"price\", \"spatial\", \"time\"]\n",
    "for key in required_embed_keys:\n",
    "    if key not in cfg.embed or \"in_ch\" not in cfg.embed[key]:\n",
    "        raise ValueError(f\"cfg.embed['{key}']['in_ch'] is missing\")\n",
    "if \"D\" not in cfg.dims:\n",
    "    raise ValueError(\"cfg.dims['D'] is missing\")\n",
    "if \"L\" not in cfg.data_mode:\n",
    "    raise ValueError(\"cfg.data_mode['L'] is missing\")\n",
    "if \"save_path\" not in cfg.moirai:\n",
    "    raise ValueError(\"cfg.moirai['save_path'] is missing\")\n",
    "\n",
    "embed_weather = CNN1dEmbedder(\n",
    "    in_ch=cfg.embed[\"weather\"][\"in_ch\"],\n",
    "    out_dim=cfg.dims[\"D\"],\n",
    "    hidden=cfg.cnn.hidden,\n",
    "    kernel_size=cfg.cnn.kernel_size,\n",
    "    dropout=cfg.cnn.dropout\n",
    ").to(DEVICE)\n",
    "\n",
    "embed_price = CNN1dEmbedder(\n",
    "    in_ch=cfg.embed[\"price\"][\"in_ch\"],\n",
    "    out_dim=cfg.dims[\"D\"],\n",
    "    hidden=cfg.cnn.hidden,\n",
    "    kernel_size=cfg.cnn.kernel_size,\n",
    "    dropout=cfg.cnn.dropout\n",
    ").to(DEVICE)\n",
    "\n",
    "embed_spatial = CNN1dEmbedder(\n",
    "    in_ch=cfg.embed[\"spatial\"][\"in_ch\"],\n",
    "    out_dim=cfg.dims[\"D\"],\n",
    "    hidden=cfg.cnn.hidden,\n",
    "    kernel_size=cfg.cnn.kernel_size,\n",
    "    dropout=cfg.cnn.dropout\n",
    ").to(DEVICE)\n",
    "\n",
    "embed_time = CNN1dEmbedder(\n",
    "    in_ch=cfg.embed[\"time\"][\"in_ch\"],\n",
    "    out_dim=cfg.dims[\"D\"],\n",
    "    hidden=cfg.cnn.hidden,\n",
    "    kernel_size=cfg.cnn.kernel_size,\n",
    "    dropout=cfg.cnn.dropout\n",
    ").to(DEVICE)\n",
    "\n",
    "# Set eval mode\n",
    "embed_weather.eval()\n",
    "embed_price.eval()\n",
    "embed_spatial.eval()\n",
    "embed_time.eval()\n",
    "\n",
    "# 3) Process a batch\n",
    "batch = next(iter(loader))  # From Cell 4 DataLoader\n",
    "x_weather = batch[\"x_weather\"].to(DEVICE)  # (B, 9, L)\n",
    "x_price = batch[\"x_price\"].to(DEVICE)      # (B, 2, L)\n",
    "x_spatial = batch[\"x_spatial\"].to(DEVICE)  # (B, 3|9, L)\n",
    "x_time = batch[\"x_time\"].to(DEVICE)        # (B, 5, L)\n",
    "\n",
    "# Validate input shapes\n",
    "assert x_weather.shape[1] == cfg.embed[\"weather\"][\"in_ch\"], f\"x_weather channels must be {cfg.embed['weather']['in_ch']}\"\n",
    "assert x_price.shape[1] == cfg.embed[\"price\"][\"in_ch\"], f\"x_price channels must be {cfg.embed['price']['in_ch']}\"\n",
    "assert x_spatial.shape[1] == cfg.embed[\"spatial\"][\"in_ch\"], f\"x_spatial channels must be {cfg.embed['spatial']['in_ch']}\"\n",
    "assert x_time.shape[1] == cfg.embed[\"time\"][\"in_ch\"], f\"x_time channels must be {cfg.embed['time']['in_ch']}\"\n",
    "assert x_weather.shape[2] == cfg.data_mode[\"L\"], f\"Input length must be {cfg.data_mode['L']}\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    h_weather = embed_weather(x_weather)  # (B, D)\n",
    "    h_price = embed_price(x_price)       # (B, D)\n",
    "    h_spatial = embed_spatial(x_spatial) # (B, D)\n",
    "    h_time = embed_time(x_time)          # (B, D)\n",
    "\n",
    "# 4) Print shapes and validate\n",
    "print(f\"h_weather: {h_weather.shape} (expected: (B, {cfg.dims['D']}))\")\n",
    "print(f\"h_price  : {h_price.shape} (expected: (B, {cfg.dims['D']}))\")\n",
    "print(f\"h_spatial: {h_spatial.shape} (expected: (B, {cfg.dims['D']}))\")\n",
    "print(f\"h_time   : {h_time.shape} (expected: (B, {cfg.dims['D']}))\")\n",
    "\n",
    "# 5) Save embeddings (optional, aligned with Cell 5)\n",
    "all_embs = {\n",
    "    \"h_weather\": [],\n",
    "    \"h_price\": [],\n",
    "    \"h_spatial\": [],\n",
    "    \"h_time\": [],\n",
    "    \"zone_id\": []\n",
    "}\n",
    "for batch in loader:\n",
    "    x_weather = batch[\"x_weather\"].to(DEVICE)\n",
    "    x_price = batch[\"x_price\"].to(DEVICE)\n",
    "    x_spatial = batch[\"x_spatial\"].to(DEVICE)\n",
    "    x_time = batch[\"x_time\"].to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_embs[\"h_weather\"].append(embed_weather(x_weather).cpu())\n",
    "        all_embs[\"h_price\"].append(embed_price(x_price).cpu())\n",
    "        all_embs[\"h_spatial\"].append(embed_spatial(x_spatial).cpu())\n",
    "        all_embs[\"h_time\"].append(embed_time(x_time).cpu())\n",
    "        all_embs[\"zone_id\"].append(batch[\"zone_id\"].cpu())\n",
    "\n",
    "# Concatenate and save\n",
    "for k in all_embs:\n",
    "    all_embs[k] = torch.cat(all_embs[k], dim=0)\n",
    "save_path = cfg.moirai[\"save_path\"].replace(\".pt\", \"_cnn.pt\")\n",
    "torch.save(all_embs, save_path)\n",
    "print(f\"Saved CNN embeddings to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_static: torch.Size([50, 128]) (expected: (B, 128))\n",
      "Saved static embeddings to: moirai_occ_embeddings_static.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# ===== Cell 7: StaticMLP Embedder for Static Features =====\n",
    "# Aligns with cfg.dims[\"D\"] and previous cells\n",
    "\n",
    "# 1) StaticMLP class\n",
    "class StaticMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP for embedding static features.\n",
    "    Args:\n",
    "        in_dim: int, input dimension (from cfg.embed[\"static\"][\"in_dim\"])\n",
    "        out_dim: int, output dimension (from cfg.dims[\"D\"])\n",
    "        hidden: int, hidden dimension (from cfg.mlp.hidden or cfg.cnn.hidden)\n",
    "        dropout: float, dropout rate (from cfg.mlp.dropout or cfg.cnn.dropout)\n",
    "    Input: (B, in_dim)\n",
    "    Output: (B, out_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, out_dim: int, hidden: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: torch.Tensor, (B, in_dim)\n",
    "        Returns:\n",
    "            torch.Tensor, (B, out_dim)\n",
    "        \"\"\"\n",
    "        if x.ndim != 2:\n",
    "            raise ValueError(f\"Expected 2D input (B, in_dim), got shape {x.shape}\")\n",
    "        return self.net(x)  # (B, out_dim)\n",
    "\n",
    "# 2) Initialize embedder\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# MLP configuration (use cfg.cnn if cfg.mlp is not present)\n",
    "if not hasattr(cfg, \"mlp\"):\n",
    "    if hasattr(cfg, \"cnn\"):\n",
    "        cfg.mlp = cfg.cnn  # Reuse CNN config\n",
    "    else:\n",
    "        cfg.mlp = SimpleNamespace(\n",
    "            hidden=128,\n",
    "            dropout=0.1\n",
    "        )\n",
    "\n",
    "# Validate configuration\n",
    "if \"static\" not in cfg.embed or \"in_dim\" not in cfg.embed[\"static\"]:\n",
    "    raise ValueError(\"cfg.embed['static']['in_dim'] is missing\")\n",
    "if \"D\" not in cfg.dims:\n",
    "    raise ValueError(\"cfg.dims['D'] is missing\")\n",
    "if \"save_path\" not in cfg.moirai:\n",
    "    raise ValueError(\"cfg.moirai['save_path'] is missing\")\n",
    "\n",
    "# Get F_static from cfg\n",
    "F_static = cfg.embed[\"static\"][\"in_dim\"]\n",
    "\n",
    "embed_static = StaticMLP(\n",
    "    in_dim=F_static,\n",
    "    out_dim=cfg.dims[\"D\"],\n",
    "    hidden=cfg.mlp.hidden,\n",
    "    dropout=cfg.mlp.dropout\n",
    ").to(DEVICE)\n",
    "embed_static.eval()\n",
    "\n",
    "# 3) Process a batch\n",
    "batch = next(iter(loader))  # From Cell 4 DataLoader\n",
    "x_static = batch[\"x_static\"].to(DEVICE)  # (B, F_static)\n",
    "\n",
    "# Validate input shape\n",
    "assert x_static.shape[1] == cfg.embed[\"static\"][\"in_dim\"], f\"x_static dim must be {cfg.embed['static']['in_dim']}, got {x_static.shape[1]}\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    h_static = embed_static(x_static)  # (B, D)\n",
    "\n",
    "# 4) Print shape and validate\n",
    "print(f\"h_static: {h_static.shape} (expected: (B, {cfg.dims['D']}))\")\n",
    "\n",
    "# 5) Save embeddings (aligned with Cell 5 and Cell 6)\n",
    "all_embs = {\n",
    "    \"h_static\": [],\n",
    "    \"zone_id\": []\n",
    "}\n",
    "for batch in loader:\n",
    "    x_static = batch[\"x_static\"].to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_embs[\"h_static\"].append(embed_static(x_static).cpu())\n",
    "        all_embs[\"zone_id\"].append(batch[\"zone_id\"].cpu())\n",
    "\n",
    "# Concatenate and save\n",
    "for k in all_embs:\n",
    "    all_embs[k] = torch.cat(all_embs[k], dim=0)\n",
    "save_path = cfg.moirai[\"save_path\"].replace(\".pt\", \"_static.pt\")\n",
    "torch.save(all_embs, save_path)\n",
    "print(f\"Saved static embeddings to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forecasting: 100%|██████████| 1/1 [00:00<00:00,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather attention weight: 0.2026\n",
      "price attention weight: 0.1994\n",
      "spatial attention weight: 0.2017\n",
      "time attention weight: 0.1954\n",
      "static attention weight: 0.2010\n",
      "Saved predictions to: moirai_occ_embeddings_predictions.pt\n",
      "y_pred shape: (50, 6) (expected: (N_samples, 6))\n",
      "attn_w shape: (50, 5) (expected: (N_samples, 5))\n",
      "zone_id shape: (50,)\n",
      "window shape: (50,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===== Cell 8: EV Forecasting Model =====\n",
    "# Integrates embeddings from Cells 5–7 and predicts multi-horizon output\n",
    "\n",
    "# 1) Model components\n",
    "class AlignProjNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Aligns and normalizes input embeddings.\n",
    "    Args:\n",
    "        D: int, input/output dimension (from cfg.dims[\"D\"])\n",
    "        p_drop: float, dropout rate (from cfg.model.dropout)\n",
    "        use_gate: bool, whether to use gating (from cfg.model.use_gate)\n",
    "    Input: (B, D)\n",
    "    Output: (B, D)\n",
    "    \"\"\"\n",
    "    def __init__(self, D: int, p_drop: float, use_gate: bool):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(D, D)\n",
    "        self.norm = nn.LayerNorm(D)\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        self.use_gate = use_gate\n",
    "        if use_gate:\n",
    "            self.gate = nn.Sequential(\n",
    "                nn.Linear(D, D//4),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(D//4, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, h):  # h: (B, D)\n",
    "        a = self.proj(h)\n",
    "        a = self.norm(a)\n",
    "        a = self.drop(a)\n",
    "        if self.use_gate:\n",
    "            g = self.gate(a)  # (B, 1)\n",
    "            a = a * g\n",
    "        return a  # (B, D)\n",
    "\n",
    "class CrossAttnFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Fuses local embedding with context embeddings using cross-attention.\n",
    "    Args:\n",
    "        D: int, embedding dimension (from cfg.dims[\"D\"])\n",
    "        nhead: int, number of attention heads (from cfg.model.nhead)\n",
    "        dropout: float, dropout rate (from cfg.model.dropout)\n",
    "    Input:\n",
    "        h_local: (B, D)\n",
    "        context_list: list of (B, D) tensors\n",
    "    Output:\n",
    "        f: (B, D), fused embedding\n",
    "        w: (B, M), attention weights (M = len(context_list))\n",
    "    \"\"\"\n",
    "    def __init__(self, D: int, nhead: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(D, D)\n",
    "        self.k_proj = nn.Linear(D, D)\n",
    "        self.v_proj = nn.Linear(D, D)\n",
    "        self.attn = nn.MultiheadAttention(D, nhead, dropout=dropout, batch_first=True)\n",
    "        self.out = nn.Linear(D, D)\n",
    "        self.norm = nn.LayerNorm(D)\n",
    "\n",
    "    def forward(self, h_local, context_list):\n",
    "        Q = self.q_proj(h_local).unsqueeze(1)  # (B, 1, D)\n",
    "        K = self.k_proj(torch.stack(context_list, dim=1))  # (B, M, D)\n",
    "        V = self.v_proj(torch.stack(context_list, dim=1))  # (B, M, D)\n",
    "        out, w = self.attn(Q, K, V)  # out: (B, 1, D), w: (B, 1, M)\n",
    "        f = self.out(out.squeeze(1))  # (B, D)\n",
    "        f = self.norm(f + h_local)  # Residual connection\n",
    "        return f, w.squeeze(1)  # (B, D), (B, M)\n",
    "\n",
    "class MultiHorizonHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Predicts multi-horizon output from fused embedding.\n",
    "    Args:\n",
    "        D: int, input dimension (from cfg.dims[\"D\"])\n",
    "        H: int, output horizon (from cfg.model.H)\n",
    "        hidden: int, hidden dimension (from cfg.model.head_hidden)\n",
    "        p_drop: float, dropout rate (from cfg.model.dropout)\n",
    "        nonneg: bool, enforce non-negative output (from cfg.model.nonneg)\n",
    "    Input: (B, D)\n",
    "    Output: (B, H)\n",
    "    \"\"\"\n",
    "    def __init__(self, D: int, H: int, hidden: int, p_drop: float, nonneg: bool):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(D),\n",
    "            nn.Linear(D, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden, H),\n",
    "        )\n",
    "        self.nonneg = nonneg\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        if self.nonneg:\n",
    "            y = torch.nn.functional.softplus(y)  # Enforce non-negative\n",
    "        return y  # (B, H)\n",
    "\n",
    "class EVForecastModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines modality embeddings and predicts multi-horizon output.\n",
    "    Args:\n",
    "        D: int, embedding dimension (from cfg.dims[\"D\"])\n",
    "        H: int, output horizon (from cfg.model.H)\n",
    "        nhead: int, number of attention heads (from cfg.model.nhead)\n",
    "        dropout: float, dropout rate (from cfg.model.dropout)\n",
    "        use_gate: bool, whether to use gating (from cfg.model.use_gate)\n",
    "        head_hidden: int, hidden dimension for head (from cfg.model.head_hidden)\n",
    "        nonneg: bool, enforce non-negative output (from cfg.model.nonneg)\n",
    "    Input: dict of embeddings (h_local, h_weather, h_price, h_spatial, h_time, h_static)\n",
    "    Output:\n",
    "        y_hat: (B, H), predictions\n",
    "        attn_w: (B, M), attention weights (M=5: weather, price, spatial, time, static)\n",
    "    \"\"\"\n",
    "    def __init__(self, D: int, H: int, nhead: int, dropout: float, use_gate: bool, head_hidden: int, nonneg: bool):\n",
    "        super().__init__()\n",
    "        self.align_local = AlignProjNorm(D, dropout, use_gate)\n",
    "        self.align_weather = AlignProjNorm(D, dropout, use_gate)\n",
    "        self.align_price = AlignProjNorm(D, dropout, use_gate)\n",
    "        self.align_spatial = AlignProjNorm(D, dropout, use_gate)\n",
    "        self.align_time = AlignProjNorm(D, dropout, use_gate)\n",
    "        self.align_static = AlignProjNorm(D, dropout, use_gate)\n",
    "        self.fusion = CrossAttnFusion(D, nhead, dropout)\n",
    "        self.head = MultiHorizonHead(D, H, head_hidden, dropout, nonneg)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        a_local = self.align_local(feats[\"h_local\"])\n",
    "        a_weather = self.align_weather(feats[\"h_weather\"])\n",
    "        a_price = self.align_price(feats[\"h_price\"])\n",
    "        a_spatial = self.align_spatial(feats[\"h_spatial\"])\n",
    "        a_time = self.align_time(feats[\"h_time\"])\n",
    "        a_static = self.align_static(feats[\"h_static\"])\n",
    "        context_list = [a_weather, a_price, a_spatial, a_time, a_static]\n",
    "        f, attn_w = self.fusion(a_local, context_list)\n",
    "        y_hat = self.head(f)  # (B, H)\n",
    "        return y_hat, attn_w  # (B, H), (B, M)\n",
    "\n",
    "# 2) Initialize device and configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Add model configuration if missing\n",
    "if not hasattr(cfg, \"model\"):\n",
    "    cfg.model = SimpleNamespace(\n",
    "        H=6,\n",
    "        nhead=8,\n",
    "        dropout=0.1,\n",
    "        use_gate=True,\n",
    "        head_hidden=512,\n",
    "        nonneg=True\n",
    "    )\n",
    "\n",
    "# Validate configuration\n",
    "required_embed_keys = [\"weather\", \"price\", \"spatial\", \"time\", \"static\"]\n",
    "for key in required_embed_keys:\n",
    "    if key not in cfg.embed or (\"in_ch\" not in cfg.embed[key] and key != \"static\") or (\"in_dim\" not in cfg.embed[key] and key == \"static\"):\n",
    "        raise ValueError(f\"cfg.embed['{key}'] missing required field\")\n",
    "if \"D\" not in cfg.dims:\n",
    "    raise ValueError(\"cfg.dims['D'] is missing\")\n",
    "if \"L\" not in cfg.data_mode:\n",
    "    raise ValueError(\"cfg.data_mode['L'] is missing\")\n",
    "if \"save_path\" not in cfg.moirai:\n",
    "    raise ValueError(\"cfg.moirai['save_path'] is missing\")\n",
    "\n",
    "# 3) Load Moirai embeddings\n",
    "checkpoint = torch.load(cfg.moirai[\"save_path\"], weights_only=True)\n",
    "h_local = checkpoint[\"h_local\"].to(DEVICE)  # (N_samples, cfg.dims[\"D\"])\n",
    "zone_ids = checkpoint[\"zone_id\"].to(DEVICE)  # (N_samples,)\n",
    "assert h_local.shape[1] == cfg.dims[\"D\"], f\"h_local dim must be {cfg.dims['D']}, got {h_local.shape[1]}\"\n",
    "\n",
    "# 4) Define embedders\n",
    "embed_weather = CNN1dEmbedder(\n",
    "    in_ch=cfg.embed[\"weather\"][\"in_ch\"],\n",
    "    out_dim=cfg.dims[\"D\"],\n",
    "    hidden=cfg.cnn.hidden,\n",
    "    kernel_size=cfg.cnn.kernel_size,\n",
    "    dropout=cfg.cnn.dropout\n",
    ").to(DEVICE)\n",
    "\n",
    "embed_price = CNN1dEmbedder(\n",
    "    in_ch=cfg.embed[\"price\"][\"in_ch\"],\n",
    "    out_dim=cfg.dims[\"D\"],\n",
    "    hidden=cfg.cnn.hidden,\n",
    "    kernel_size=cfg.cnn.kernel_size,\n",
    "    dropout=cfg.cnn.dropout\n",
    ").to(DEVICE)\n",
    "\n",
    "embed_spatial = CNN1dEmbedder(\n",
    "    in_ch=cfg.embed[\"spatial\"][\"in_ch\"],\n",
    "    out_dim=cfg.dims[\"D\"],\n",
    "    hidden=cfg.cnn.hidden,\n",
    "    kernel_size=cfg.cnn.kernel_size,\n",
    "    dropout=cfg.cnn.dropout\n",
    ").to(DEVICE)\n",
    "\n",
    "embed_time = CNN1dEmbedder(\n",
    "    in_ch=cfg.embed[\"time\"][\"in_ch\"],\n",
    "    out_dim=cfg.dims[\"D\"],\n",
    "    hidden=cfg.cnn.hidden,\n",
    "    kernel_size=cfg.cnn.kernel_size,\n",
    "    dropout=cfg.cnn.dropout\n",
    ").to(DEVICE)\n",
    "\n",
    "embed_static = StaticMLP(\n",
    "    in_dim=cfg.embed[\"static\"][\"in_dim\"],\n",
    "    out_dim=cfg.dims[\"D\"],\n",
    "    hidden=cfg.mlp.hidden,\n",
    "    dropout=cfg.mlp.dropout\n",
    ").to(DEVICE)\n",
    "\n",
    "# Set eval mode\n",
    "embed_weather.eval()\n",
    "embed_price.eval()\n",
    "embed_spatial.eval()\n",
    "embed_time.eval()\n",
    "embed_static.eval()\n",
    "\n",
    "# 5) Process all batches and save predictions\n",
    "model = EVForecastModel(\n",
    "    D=cfg.dims[\"D\"],\n",
    "    H=cfg.model.H,\n",
    "    nhead=cfg.model.nhead,\n",
    "    dropout=cfg.model.dropout,\n",
    "    use_gate=cfg.model.use_gate,\n",
    "    head_hidden=cfg.model.head_hidden,\n",
    "    nonneg=cfg.model.nonneg\n",
    ").to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "all_preds = {\n",
    "    \"y_pred\": [],\n",
    "    \"attn_w\": [],\n",
    "    \"zone_id\": [],\n",
    "    \"window\": []\n",
    "}\n",
    "modalities = [\"weather\", \"price\", \"spatial\", \"time\", \"static\"]\n",
    "\n",
    "for batch in tqdm(loader, desc=\"Forecasting\"):\n",
    "    # Extract batch data\n",
    "    x_weather = batch[\"x_weather\"].to(DEVICE)  # (B, 9, L)\n",
    "    x_price = batch[\"x_price\"].to(DEVICE)      # (B, 2, L)\n",
    "    x_spatial = batch[\"x_spatial\"].to(DEVICE)  # (B, 3|9, L)\n",
    "    x_time = batch[\"x_time\"].to(DEVICE)        # (B, 5, L)\n",
    "    x_static = batch[\"x_static\"].to(DEVICE)    # (B, F_static)\n",
    "    batch_zone_ids = batch[\"zone_id\"].to(DEVICE)  # (B,)\n",
    "    batch_windows = batch[\"window\"].to(DEVICE)    # (B,)\n",
    "    B = batch_zone_ids.size(0)\n",
    "\n",
    "    # Validate input shapes\n",
    "    assert x_weather.shape[1] == cfg.embed[\"weather\"][\"in_ch\"], f\"x_weather channels must be {cfg.embed['weather']['in_ch']}\"\n",
    "    assert x_price.shape[1] == cfg.embed[\"price\"][\"in_ch\"], f\"x_price channels must be {cfg.embed['price']['in_ch']}\"\n",
    "    assert x_spatial.shape[1] == cfg.embed[\"spatial\"][\"in_ch\"], f\"x_spatial channels must be {cfg.embed['spatial']['in_ch']}\"\n",
    "    assert x_time.shape[1] == cfg.embed[\"time\"][\"in_ch\"], f\"x_time channels must be {cfg.embed['time']['in_ch']}\"\n",
    "    assert x_static.shape[1] == cfg.embed[\"static\"][\"in_dim\"], f\"x_static dim must be {cfg.embed['static']['in_dim']}\"\n",
    "    assert x_weather.shape[2] == cfg.data_mode[\"L\"], f\"Input length must be {cfg.data_mode['L']}\"\n",
    "\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        h_weather = embed_weather(x_weather)  # (B, D)\n",
    "        h_price = embed_price(x_price)       # (B, D)\n",
    "        h_spatial = embed_spatial(x_spatial) # (B, D)\n",
    "        h_time = embed_time(x_time)          # (B, D)\n",
    "        h_static = embed_static(x_static)    # (B, D)\n",
    "\n",
    "        # Select h_local for the batch\n",
    "        selected_indices = torch.zeros(B, dtype=torch.long, device=DEVICE)\n",
    "        used_indices = torch.zeros(zone_ids.size(0), dtype=torch.bool, device=DEVICE)\n",
    "        for i, (bzid, w) in enumerate(zip(batch_zone_ids, batch_windows)):\n",
    "            matching_indices = torch.where(zone_ids == bzid)[0]\n",
    "            if matching_indices.numel() == 0:\n",
    "                raise RuntimeError(f\"No matching index found for batch_zone_id {bzid.item()}\")\n",
    "            for idx in matching_indices:\n",
    "                if not used_indices[idx]:\n",
    "                    selected_indices[i] = idx\n",
    "                    used_indices[idx] = True\n",
    "                    break\n",
    "            else:\n",
    "                raise RuntimeError(f\"No unused index found for batch_zone_id {bzid.item()}, window {w.item()}\")\n",
    "        \n",
    "        h_local_batch = h_local[selected_indices]  # (B, D)\n",
    "\n",
    "        # Validate embedding shapes\n",
    "        assert h_local_batch.shape[1] == cfg.dims[\"D\"], f\"h_local_batch dim must be {cfg.dims['D']}\"\n",
    "        for h, name in [(h_weather, \"h_weather\"), (h_price, \"h_price\"), (h_spatial, \"h_spatial\"), \n",
    "                        (h_time, \"h_time\"), (h_static, \"h_static\")]:\n",
    "            assert h.shape[1] == cfg.dims[\"D\"], f\"{name} dim must be {cfg.dims['D']}\"\n",
    "\n",
    "        # Create feats dictionary\n",
    "        feats = {\n",
    "            \"h_local\": h_local_batch,\n",
    "            \"h_weather\": h_weather,\n",
    "            \"h_price\": h_price,\n",
    "            \"h_spatial\": h_spatial,\n",
    "            \"h_time\": h_time,\n",
    "            \"h_static\": h_static\n",
    "        }\n",
    "\n",
    "        # Run model\n",
    "        y_pred, attn_w = model(feats)  # y_pred: (B, H), attn_w: (B, M)\n",
    "        all_preds[\"y_pred\"].append(y_pred.cpu())\n",
    "        all_preds[\"attn_w\"].append(attn_w.cpu())\n",
    "        all_preds[\"zone_id\"].append(batch_zone_ids.cpu())\n",
    "        all_preds[\"window\"].append(batch_windows.cpu())\n",
    "\n",
    "# 6) Concatenate and save predictions\n",
    "for k in all_preds:\n",
    "    all_preds[k] = torch.cat(all_preds[k], dim=0)\n",
    "\n",
    "# Analyze attention weights\n",
    "attn_w_mean = all_preds[\"attn_w\"].mean(dim=0)  # (M,)\n",
    "for i, w in enumerate(attn_w_mean):\n",
    "    print(f\"{modalities[i]} attention weight: {w.item():.4f}\")\n",
    "\n",
    "# Save predictions\n",
    "save_path = cfg.moirai[\"save_path\"].replace(\".pt\", \"_predictions.pt\")\n",
    "torch.save(all_preds, save_path)\n",
    "print(f\"Saved predictions to: {save_path}\")\n",
    "\n",
    "# 7) Print shapes for verification\n",
    "print(f\"y_pred shape: {tuple(all_preds['y_pred'].shape)} (expected: (N_samples, {cfg.model.H}))\")\n",
    "print(f\"attn_w shape: {tuple(all_preds['attn_w'].shape)} (expected: (N_samples, {len(modalities)}))\")\n",
    "print(f\"zone_id shape: {tuple(all_preds['zone_id'].shape)}\")\n",
    "print(f\"window shape: {tuple(all_preds['window'].shape)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
