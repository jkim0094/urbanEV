{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import random, math, json, hashlib\n",
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from contextlib import nullcontext\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import amp\n",
    "from tqdm.auto import tqdm\n",
    "import chinese_calendar as cc\n",
    "import warnings\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "from uni2ts.model.moirai import MoiraiModule\n",
    "\n",
    "\n",
    "# Device / Seed\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "SEED = 0\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "# Í≤ΩÎ°ú & ÏÑ§Ï†ï\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "DATA_DIR = ROOT / \"UrbanEV\" / \"data\"\n",
    "PATHS = {\n",
    "    \"occ\": str(DATA_DIR / \"occupancy.csv\"),\n",
    "    \"dur\": str(DATA_DIR / \"duration.csv\"),\n",
    "    \"vol\": str(DATA_DIR / \"volume.csv\"),\n",
    "    \"e_price\": str(DATA_DIR / \"e_price.csv\"),\n",
    "    \"s_price\": str(DATA_DIR / \"s_price.csv\"),\n",
    "    \"weather\": str(DATA_DIR / \"weather_central.csv\"),\n",
    "    \"inf\": str(DATA_DIR / \"inf.csv\"),\n",
    "    \"adj\": str(DATA_DIR / \"adj.csv\"),\n",
    "    \"dist\": str(DATA_DIR / \"distance.csv\"),\n",
    "    \"poi\": str(DATA_DIR / \"poi.csv\"),\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def build_clock(train_range, test_range):\n",
    "    start = pd.to_datetime(train_range[0])\n",
    "    end   = pd.to_datetime(test_range[1]) + pd.Timedelta(hours=23)\n",
    "    return pd.date_range(start=start, end=end, freq=\"h\")\n",
    "\n",
    "def read_ts_csv(path):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    return df.sort_index().apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "def reindex_local(df, clock):  return df.reindex(clock).fillna(0)\n",
    "def reindex_price(df, clock):  return df.reindex(clock).ffill().bfill()\n",
    "\n",
    "def reindex_weather(df, clock):\n",
    "    out = df.reindex(clock)\n",
    "    cont_cols = [c for c in out.columns if c.lower() not in (\"nrain\",\"rain\",\"n_rain\")]\n",
    "    rain_cols = [c for c in out.columns if c.lower() in (\"nrain\",\"rain\",\"n_rain\")]\n",
    "    if cont_cols: out[cont_cols] = out[cont_cols].interpolate(\"time\").ffill().bfill()\n",
    "    if rain_cols: out[rain_cols] = out[rain_cols].ffill().bfill().clip(lower=0).astype(int)\n",
    "    return out\n",
    "\n",
    "def load_inf(path):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    cols = [\"longitude\",\"latitude\",\"charge_count\",\"area\",\"perimeter\"]\n",
    "    for c in cols: df[c] = pd.to_numeric(df.get(c, np.nan), errors=\"coerce\")\n",
    "    df.index = df.index.astype(str)\n",
    "    return df[cols]\n",
    "\n",
    "def load_matrix(path, as_float=False):\n",
    "    df = pd.read_csv(path)\n",
    "    df.index = df.columns.astype(str)\n",
    "    df.columns = df.columns.astype(str)\n",
    "    if df.index.duplicated().any():\n",
    "        print(f\"[warn] duplicated index in {path}, keeping first\")\n",
    "        df = df.loc[~df.index.duplicated(keep='first')]\n",
    "    if df.columns.duplicated().any():\n",
    "        print(f\"[warn] duplicated columns in {path}, keeping first\")\n",
    "        df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
    "    df = df.astype(float if as_float else int)\n",
    "    np.fill_diagonal(df.values, 0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def precompute_time(clock):\n",
    "    hrs  = clock.hour; dows = clock.dayofweek\n",
    "    hour_sin = np.sin(2*np.pi*hrs/24).astype(np.float32)\n",
    "    hour_cos = np.cos(2*np.pi*hrs/24).astype(np.float32)\n",
    "    dow_sin  = np.sin(2*np.pi*dows/7).astype(np.float32)\n",
    "    dow_cos  = np.cos(2*np.pi*dows/7).astype(np.float32)\n",
    "\n",
    "    # Ï£ºÎßê ÎòêÎäî Ï§ëÍµ≠ Í≥µÌú¥ÏùºÏù¥Î©¥ 1\n",
    "    is_offday = np.array([\n",
    "        float((d.weekday() in [5,6]) or cc.is_holiday(d.date()))\n",
    "        for d in clock\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"hour_sin\":hour_sin,\"hour_cos\":hour_cos,\n",
    "        \"dow_sin\":dow_sin,\"dow_cos\":dow_cos,\n",
    "        \"is_offday\":is_offday\n",
    "    }, index=clock)\n",
    "\n",
    "\n",
    "def prepare_data(cfg):\n",
    "    clock = build_clock(cfg.data.train_range, cfg.data.test_range)\n",
    "    occ = reindex_local(read_ts_csv(cfg.data.paths[\"occ\"]), clock)\n",
    "    dur = reindex_local(read_ts_csv(cfg.data.paths[\"dur\"]), clock)\n",
    "    vol = reindex_local(read_ts_csv(cfg.data.paths[\"vol\"]), clock)\n",
    "    epr = reindex_price(read_ts_csv(cfg.data.paths[\"e_price\"]), clock)\n",
    "    spr = reindex_price(read_ts_csv(cfg.data.paths[\"s_price\"]), clock)\n",
    "    wth = reindex_weather(read_ts_csv(cfg.data.paths[\"weather\"]), clock)\n",
    "    inf = load_inf(cfg.data.paths[\"inf\"])\n",
    "    adj = load_matrix(cfg.data.paths[\"adj\"])\n",
    "    dist= load_matrix(cfg.data.paths[\"dist\"], as_float=True)\n",
    "\n",
    "    tables = {\"occ\":occ,\"dur\":dur,\"vol\":vol,\"e_price\":epr,\"s_price\":spr,\n",
    "              \"weather\":wth,\"inf\":inf,\"adj\":adj,\"distance\":dist,\"time\":precompute_time(clock)}\n",
    "\n",
    "    if cfg.data.use_poi and os.path.exists(cfg.data.paths[\"poi\"]):\n",
    "        tables[\"poi_raw\"] = pd.read_csv(cfg.data.paths[\"poi\"])\n",
    "\n",
    "    zone_ids = sorted(list(set(occ.columns) & set(epr.columns) & set(spr.columns)))\n",
    "    if len(zone_ids) != len(set(zone_ids)): raise ValueError(\"zone_ids duplicated\")\n",
    "\n",
    "    for k in [\"occ\",\"dur\",\"vol\",\"e_price\",\"s_price\"]:\n",
    "        tables[k] = tables[k][zone_ids]\n",
    "    tables[\"adj\"]      = tables[\"adj\"].reindex(index=zone_ids, columns=zone_ids, fill_value=0)\n",
    "    tables[\"distance\"] = tables[\"distance\"].reindex(index=zone_ids, columns=zone_ids, fill_value=float('inf'))\n",
    "    tables[\"inf\"]      = tables[\"inf\"].reindex(index=zone_ids)\n",
    "\n",
    "    datahash  = hashlib.sha256(str(cfg.data.paths).encode()).hexdigest()[:16]\n",
    "    clockhash = hashlib.sha256(str(clock).encode()).hexdigest()[:16]\n",
    "    hashes = SimpleNamespace(datahash=datahash, clockhash=clockhash)\n",
    "    return clock, tables, zone_ids, hashes\n",
    "\n",
    "def compute_fit_stats(tables, train_times, zone_ids):\n",
    "    \"\"\"\n",
    "    ÌïôÏäµ(train) Íµ¨Í∞ÑÏóêÏÑú ÌëúÏ§ÄÌôîÏóê Ïì∏ ÌÜµÍ≥ÑÏπò Í≥ÑÏÇ∞.\n",
    "    - z-score ÎåÄÏÉÅ: occ, dur, e_price, s_price, weather(T,P0,P,U,Td)\n",
    "    - log1p+z ÎåÄÏÉÅ(ÌÜµÍ≥ÑÎßå): vol, inf(charge_count/area/perimeter), poi_counts[*]\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "\n",
    "    # 1) ÏãúÍ≥ÑÏó¥: z-score\n",
    "    for key in [\"occ\", \"dur\", \"e_price\", \"s_price\"]:\n",
    "        if key in tables:\n",
    "            df = tables[key].loc[train_times, zone_ids]\n",
    "            mu = float(np.nanmean(df.values))\n",
    "            sd = float(np.nanstd(df.values)) or 1.0\n",
    "            stats[key] = {\"mean\": mu, \"std\": sd}\n",
    "\n",
    "    # 2) ÏãúÍ≥ÑÏó¥: log1p + z-score (vol)\n",
    "    if \"vol\" in tables:\n",
    "        df = tables[\"vol\"].loc[train_times, zone_ids]\n",
    "        vals = np.log1p(df.values.astype(float))\n",
    "        mu = float(np.nanmean(vals))\n",
    "        sd = float(np.nanstd(vals)) or 1.0\n",
    "        stats[\"log_vol\"] = {\"mean\": mu, \"std\": sd}\n",
    "\n",
    "    # 3) ÎÇ†Ïî® Ïó∞ÏÜçÍ∞í: z-score  (rain Í≥ÑÏó¥ Ï†úÏô∏)\n",
    "    if \"weather\" in tables:\n",
    "        wdf = tables[\"weather\"].loc[train_times]\n",
    "        for c in [\"T\", \"P0\", \"P\", \"U\", \"Td\"]:\n",
    "            if c in wdf.columns:\n",
    "                col = pd.to_numeric(wdf[c], errors=\"coerce\").values\n",
    "                mu = float(np.nanmean(col))\n",
    "                sd = float(np.nanstd(col)) or 1.0\n",
    "                stats[c] = {\"mean\": mu, \"std\": sd}\n",
    "            else:\n",
    "                stats[c] = {\"mean\": 0.0, \"std\": 1.0}\n",
    "\n",
    "    # 4) Ï†ïÏ†Å: log1p + z-score (ÏõêÎ≥∏ÏùÄ Ïú†ÏßÄÌï† Í≤ÉÏù¥ÎØÄÎ°ú ÌÜµÍ≥ÑÎßå)\n",
    "    if \"inf\" in tables:\n",
    "        inf = tables[\"inf\"]\n",
    "        for c in [\"charge_count\", \"area\", \"perimeter\"]:\n",
    "            if c in inf.columns:\n",
    "                vals = np.log1p(pd.to_numeric(inf[c], errors=\"coerce\").values.astype(float))\n",
    "                mu = float(np.nanmean(vals))\n",
    "                sd = float(np.nanstd(vals)) or 1.0\n",
    "                stats[f\"log_{c}\"] = {\"mean\": mu, \"std\": sd}\n",
    "\n",
    "    # 5) POI Ïπ¥Ïö¥Ìä∏: log1p + z-score (ÏûàÏùÑ Îïå)\n",
    "    if \"poi_counts\" in tables:\n",
    "        poi = tables[\"poi_counts\"]\n",
    "        for c in poi.columns:\n",
    "            vals = np.log1p(pd.to_numeric(poi[c], errors=\"coerce\").values.astype(float))\n",
    "            mu = float(np.nanmean(vals))\n",
    "            sd = float(np.nanstd(vals)) or 1.0\n",
    "            stats[f\"log_{c}\"] = {\"mean\": mu, \"std\": sd}\n",
    "\n",
    "    # Î≤ÑÏ†Ñ ÌÉúÍπÖ\n",
    "    stats_json = json.dumps(stats, sort_keys=True)\n",
    "    stats[\"norm_version\"] = hashlib.sha256(stats_json.encode()).hexdigest()[:16]\n",
    "    return stats\n",
    "\n",
    "\n",
    "def standardize_tables(tables, stats):\n",
    "    \"\"\"\n",
    "    tablesÎ•º Ï†úÏûêÎ¶¨(in-place)ÏóêÏÑú ÌëúÏ§ÄÌôî.\n",
    "    - z-score: occ, dur, e_price, s_price, weather(T,P0,P,U,Td)\n",
    "    - log1p+z: vol, poi_counts[*]\n",
    "    - infÎäî **ÏõêÎ≥∏ Ïú†ÏßÄ**(area/perimeter ÏõêÍ∞í ÌïÑÏöî). ÌïÑÏöî Ïãú fb_static Îì±ÏóêÏÑú on-the-flyÎ°ú Î≥ÄÌôòÌïòÏÑ∏Ïöî.\n",
    "    \"\"\"\n",
    "    eps = 1e-8\n",
    "\n",
    "    # 1) ÏãúÍ≥ÑÏó¥: z-score\n",
    "    for key in [\"occ\", \"dur\", \"e_price\", \"s_price\"]:\n",
    "        if key in tables and key in stats:\n",
    "            mu = stats[key][\"mean\"]; sd = max(stats[key][\"std\"], eps)\n",
    "            tables[key] = ((tables[key] - mu) / sd).astype(np.float32)\n",
    "\n",
    "    # 2) ÏãúÍ≥ÑÏó¥: log1p + z-score (vol)\n",
    "    if \"vol\" in tables and \"log_vol\" in stats:\n",
    "        mu = stats[\"log_vol\"][\"mean\"]; sd = max(stats[\"log_vol\"][\"std\"], eps)\n",
    "        df = tables[\"vol\"].astype(float)\n",
    "        tables[\"vol\"] = ((np.log1p(df) - mu) / sd).astype(np.float32)\n",
    "\n",
    "    # 3) ÎÇ†Ïî® Ïó∞ÏÜçÍ∞í: z-score (rain Í≥ÑÏó¥ÏùÄ ÏÜêÎåÄÏßÄ ÏïäÏùå)\n",
    "    if \"weather\" in tables:\n",
    "        w = tables[\"weather\"].copy()\n",
    "        for c in [\"T\", \"P0\", \"P\", \"U\", \"Td\"]:\n",
    "            if c in w.columns and c in stats:\n",
    "                mu = stats[c][\"mean\"]; sd = max(stats[c][\"std\"], eps)\n",
    "                w[c] = ((pd.to_numeric(w[c], errors=\"coerce\") - mu) / sd).astype(np.float32)\n",
    "        tables[\"weather\"] = w\n",
    "\n",
    "    # 4) POI Ïπ¥Ïö¥Ìä∏: log1p + z-score (ÏûàÏùÑ Îïå, Î™®Îç∏ ÏûÖÎ†•Îßå Ïì∏ Í±∞ÎùºÎ©¥ ÎçÆÏñ¥Ïç®ÎèÑ OK)\n",
    "    if \"poi_counts\" in tables:\n",
    "        poi = tables[\"poi_counts\"].copy()\n",
    "        for c in poi.columns:\n",
    "            key = f\"log_{c}\"\n",
    "            if key in stats:\n",
    "                mu = stats[key][\"mean\"]; sd = max(stats[key][\"std\"], eps)\n",
    "                col = np.log1p(pd.to_numeric(poi[c], errors=\"coerce\").astype(float))\n",
    "                poi[c] = ((col - mu) / sd).astype(np.float32)\n",
    "        tables[\"poi_counts\"] = poi\n",
    "\n",
    "\n",
    "\n",
    "def build_sample_indices(tables, clock, zone_ids, cfg):\n",
    "    # ---- ÌååÎùºÎØ∏ÌÑ∞ ----\n",
    "    L = int(cfg.data.L)\n",
    "    H = int(cfg.data.H)\n",
    "    O = int(getattr(cfg.data, \"pred_offset\", 1))\n",
    "    zone_ids = list(zone_ids)\n",
    "\n",
    "    # ---- split ÎùºÎ≤®ÎßÅ (ÎÅù-of-day Î≥¥Ï†ï Ìè¨Ìï®) ----\n",
    "    split_sr = pd.Series(index=clock, data=\"none\")\n",
    "\n",
    "    def _eod(ts):\n",
    "        ts = pd.to_datetime(ts)\n",
    "        return ts + pd.Timedelta(hours=23) if (ts.hour==0 and ts.minute==0 and ts.second==0) else ts\n",
    "\n",
    "    tr_s, tr_e = pd.to_datetime(cfg.data.train_range[0]), _eod(cfg.data.train_range[1])\n",
    "    va_s, va_e = pd.to_datetime(cfg.data.val_range[0]),   _eod(cfg.data.val_range[1])\n",
    "    te_s, te_e = pd.to_datetime(cfg.data.test_range[0]),  _eod(cfg.data.test_range[1])\n",
    "\n",
    "    split_sr.loc[(split_sr.index>=tr_s)&(split_sr.index<=tr_e)] = \"train\"\n",
    "    split_sr.loc[(split_sr.index>=va_s)&(split_sr.index<=va_e)] = \"val\"\n",
    "    split_sr.loc[(split_sr.index>=te_s)&(split_sr.index<=te_e)] = \"test\"\n",
    "\n",
    "    # ---- ÌÜµÍ≥Ñ/ÌëúÏ§ÄÌôî ----\n",
    "    train_times = split_sr[split_sr==\"train\"].index\n",
    "    stats = compute_fit_stats(tables, train_times, zone_ids)\n",
    "    standardize_tables(tables, stats)\n",
    "\n",
    "    # ====== Ïú†Ìö®ÏÑ± ÎßàÏä§ÌÅ¨ ======\n",
    "    occ_fin = pd.DataFrame(np.isfinite(tables[\"occ\"].values), index=clock, columns=zone_ids)\n",
    "    epr_fin = pd.DataFrame(np.isfinite(tables[\"e_price\"].values), index=clock, columns=zone_ids)\n",
    "    spr_fin = pd.DataFrame(np.isfinite(tables[\"s_price\"].values), index=clock, columns=zone_ids)\n",
    "\n",
    "    masks = [occ_fin, epr_fin, spr_fin]\n",
    "\n",
    "    # dur/volÏùÑ cfgÏóê Îî∞Îùº Ï∂îÍ∞Ä\n",
    "    if \"dur\" in getattr(cfg.data, \"moirai_channels\", []):\n",
    "        dur_fin = pd.DataFrame(np.isfinite(tables[\"dur\"].values), index=clock, columns=zone_ids)\n",
    "        masks.append(dur_fin)\n",
    "\n",
    "    if \"vol\" in getattr(cfg.data, \"moirai_channels\", []):\n",
    "        vol_fin = pd.DataFrame(np.isfinite(tables[\"vol\"].values), index=clock, columns=zone_ids)\n",
    "        masks.append(vol_fin)\n",
    "\n",
    "    # ÏûÖÎ†•Ï∞Ω L Ïó∞ÏÜç Ï°∞Í±¥\n",
    "    input_ok = True\n",
    "    for m in masks:\n",
    "        input_ok = input_ok & (m.rolling(L, min_periods=L).sum().eq(L).shift(-(L-1), fill_value=False))\n",
    "\n",
    "    # ÎÇ†Ïî®Îäî zone Ï∂ïÏù¥ ÏóÜÏúºÎãà Ïª®Ìã∞Îâ¥Ïñ¥Ïä§ 5Í∞ú Î™®Îëê finite ‚Üí L Ïó∞ÏÜç ÌõÑ Î∏åÎ°úÎìúÏ∫êÏä§Ìä∏\n",
    "    w5 = tables[\"weather\"][[\"T\",\"P0\",\"P\",\"U\",\"Td\"]]\n",
    "    w5_fin = pd.Series(np.isfinite(w5.values).all(axis=1), index=clock)\n",
    "    w_ok_sr = w5_fin.rolling(L, min_periods=L).sum().eq(L).shift(-(L-1), fill_value=False)\n",
    "    w_ok = pd.DataFrame(np.repeat(w_ok_sr.values[:,None], len(zone_ids), axis=1), index=clock, columns=zone_ids)\n",
    "\n",
    "    # ÌÉÄÍπÉÏ∞Ω (t0+O Î∂ÄÌÑ∞ HÍ∞ú Ïó∞ÏÜç finite)\n",
    "    if H == 1:\n",
    "        tgt_ok = occ_fin.shift(-O, fill_value=False)\n",
    "    else:\n",
    "        tgt_ok = (occ_fin.shift(-O, fill_value=False)\n",
    "                          .rolling(H, min_periods=H).sum().eq(H)\n",
    "                          .shift(-(H-1), fill_value=False))\n",
    "\n",
    "    # Ï¢ÖÌï© Ïú†Ìö®ÏÑ±\n",
    "    combined_ok = input_ok & w_ok & tgt_ok\n",
    "\n",
    "    # ====== Í≤ΩÍ≥Ñ(ÏúàÎèÑÏö∞) Ìè¨Ìï®ÏùÑ ÏàòÏãùÏúºÎ°ú Í∞ïÏ†ú ======\n",
    "    # ÌóàÏö© t0 Î≤îÏúÑ: [S, E - max(L-1, O+H-1)]\n",
    "    Lm1 = pd.Timedelta(hours=L-1)\n",
    "    Hm1 = pd.Timedelta(hours=H-1)\n",
    "    Off = pd.Timedelta(hours=O)\n",
    "\n",
    "    out = {}\n",
    "    for split, (S, E) in {\n",
    "        \"train\": (tr_s, tr_e),\n",
    "        \"val\"  : (va_s, va_e),\n",
    "        \"test\" : (te_s, te_e),\n",
    "    }.items():\n",
    "        upper = E - max(Lm1, Off + Hm1)  # t0 ÏÉÅÌïú\n",
    "        # ÏãúÍ∞Ñ Ïù∏Îç±Ïä§ Í∏∞Î∞ò Î∂ÄÏö∏ ÏãúÎ¶¨Ï¶à ‚Üí DataFrameÏúºÎ°ú Î∏åÎ°úÎìúÏ∫êÏä§Ìä∏\n",
    "        t0_ok_sr = (clock >= S) & (clock <= upper)                  # ndarray(bool)\n",
    "        t0_ok = pd.DataFrame(np.repeat(t0_ok_sr[:, None], len(zone_ids), axis=1),\n",
    "                            index=clock, columns=zone_ids)\n",
    "\n",
    "        split_ok_sr = (split_sr == split).to_numpy()  # shape (n,)\n",
    "        split_ok = pd.DataFrame(np.repeat(split_ok_sr[:, None], len(zone_ids), axis=1),\n",
    "                                index=clock, columns=zone_ids)\n",
    "\n",
    "        valid = combined_ok & t0_ok & split_ok\n",
    "        # ÌèâÌÉÑÌôî\n",
    "        flat = valid.stack().reset_index()\n",
    "        flat = flat[flat[0]].drop(columns=0)\n",
    "        flat.columns = [\"t_start\", \"zone_id\"]\n",
    "\n",
    "        # Î©îÌÉÄ\n",
    "        if flat.empty:\n",
    "            out[split] = flat.reset_index(drop=True)\n",
    "            continue\n",
    "\n",
    "        flat[\"zone_idx\"] = flat[\"zone_id\"].map({z:i for i,z in enumerate(zone_ids)})\n",
    "        flat[\"L\"] = L\n",
    "        flat[\"H\"] = H\n",
    "        flat[\"occ_only\"] = int(getattr(cfg.data, \"baseline_occ_only\", True))\n",
    "        flat[\"split\"] = split\n",
    "        out[split] = flat.reset_index(drop=True)\n",
    "\n",
    "    return out, stats\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POI utilities \n",
    "def compute_zone_radius(inf, beta=0.7, r_min=300, r_max=2000):\n",
    "    A = inf[\"area\"].to_numpy(); P = inf[\"perimeter\"].to_numpy()\n",
    "    r_area  = np.sqrt(np.clip(A,0,None)/np.pi)\n",
    "    r_perim = P/(2*np.pi)\n",
    "    r = beta*r_area + (1-beta)*r_perim\n",
    "    return pd.Series(np.clip(r, r_min, r_max), index=inf.index)\n",
    "\n",
    "\n",
    "def build_poi_counts_fast(inf, poi, radius_m):\n",
    "    # radians\n",
    "    z = np.deg2rad(np.c_[inf[\"latitude\"].values, inf[\"longitude\"].values])\n",
    "    p = np.deg2rad(np.c_[poi[\"latitude\"].values, poi[\"longitude\"].values])\n",
    "\n",
    "    # BallTree on POI\n",
    "    tree = BallTree(p, metric=\"haversine\")\n",
    "\n",
    "    # radius vector in radians\n",
    "    r_vec = (radius_m.values / 6371000.0).astype(np.float64)\n",
    "\n",
    "    # single batched query\n",
    "    idx_lists = tree.query_radius(z, r=r_vec)  # list of index arrays, len = n_zones\n",
    "\n",
    "    types = poi[\"primary_types\"].str.lower().values\n",
    "    is_life = np.char.find(types.astype(str), \"lifestyle services\") >= 0\n",
    "    is_bres = np.char.find(types.astype(str), \"business and residential\") >= 0\n",
    "    is_food = np.char.find(types.astype(str), \"food and beverage services\") >= 0\n",
    "\n",
    "    out = np.zeros((len(inf), 3), dtype=np.int32)\n",
    "    for i, idxs in enumerate(idx_lists):\n",
    "        if idxs.size == 0: \n",
    "            continue\n",
    "        out[i, 0] = int(is_life[idxs].sum())\n",
    "        out[i, 1] = int(is_bres[idxs].sum())\n",
    "        out[i, 2] = int(is_food[idxs].sum())\n",
    "\n",
    "    return pd.DataFrame(out, index=inf.index,\n",
    "        columns=[\"poi_lifestyle\",\"poi_business_residential\",\"poi_food_beverage\"])\n",
    "\n",
    "\n",
    "def ensure_poi_counts(tables, cfg):\n",
    "    \"\"\"\n",
    "    - Ï∫êÏãúÎêú poi_counts ÏûàÏúºÎ©¥ Î°úÎìú\n",
    "    - ÏóÜÏúºÎ©¥ compute_zone_radius + build_poi_counts ÎèåÎ†§ Í≥ÑÏÇ∞ ÌõÑ Ï†ÄÏû•\n",
    "    \"\"\"\n",
    "    # Ï∫êÏãú Ìè¥Îçî\n",
    "    cache_dir = Path(getattr(cfg.exec, \"poi_shared_dir\", \"poi_cache_global\"))\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Ï∫êÏãú ÌååÏùº Ïù¥Î¶Ñ ÎßåÎì§Í∏∞ (Îç∞Ïù¥ÌÑ∞/ÌååÎùºÎØ∏ÌÑ∞ Í∏∞Î∞ò Ìï¥Ïãú)\n",
    "    meta = {\n",
    "        \"beta\": cfg.data.poi_radius_beta,\n",
    "        \"rmin\": cfg.data.poi_rmin,\n",
    "        \"rmax\": cfg.data.poi_rmax,\n",
    "        # inf/poi ÎÇ¥Ïö©ÎèÑ Ìè¨Ìï® (Ïó¨Í∏∞ÏÑúÎäî Í∞ÑÎã®Ìûà Í∏∏Ïù¥ÏôÄ Ìï¥ÏãúÎ°ú)\n",
    "        \"n_zones\": len(tables[\"inf\"]),\n",
    "        \"n_poi\": len(tables[\"poi_raw\"])\n",
    "    }\n",
    "    h = hashlib.sha256(json.dumps(meta, sort_keys=True).encode()).hexdigest()[:16]\n",
    "    cache_file = cache_dir / f\"poi_counts_{h}.parquet\"\n",
    "\n",
    "    # Ïù¥ÎØ∏ ÏûàÏúºÎ©¥ Î°úÎìú\n",
    "    if cache_file.exists():\n",
    "        poi_counts = pd.read_parquet(cache_file)\n",
    "        tables[\"poi_counts\"] = poi_counts\n",
    "        return poi_counts\n",
    "\n",
    "    # ÏóÜÏúºÎ©¥ ÏÉàÎ°ú Í≥ÑÏÇ∞\n",
    "    radius = compute_zone_radius(\n",
    "        tables[\"inf\"],\n",
    "        beta=cfg.data.poi_radius_beta,\n",
    "        r_min=cfg.data.poi_rmin,\n",
    "        r_max=cfg.data.poi_rmax\n",
    "    )\n",
    "    poi_counts = build_poi_counts(tables[\"inf\"], tables[\"poi_raw\"], radius)\n",
    "\n",
    "    # Ï†ÄÏû•\n",
    "    poi_counts.to_parquet(cache_file)\n",
    "    tables[\"poi_counts\"] = poi_counts\n",
    "    return poi_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moirai cache (keying fixed)\n",
    "KEY_COLS = [\"zone_id\",\"t_start_iso\",\"L\",\"occ_only\",\"c_sig\",\"model_id\",\"psig\",\"pool\",\"clock_hash\",\"norm_version\",\"datahash\"]\n",
    "\n",
    "\n",
    "def make_moirai_keys(si, hashes, stats, cfg):\n",
    "    df = si.copy()\n",
    "    df[\"t_start_iso\"] = df[\"t_start\"].dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    df[\"L\"] = df[\"L\"].astype(int)\n",
    "    df[\"occ_only\"] = df[\"occ_only\"].astype(int)\n",
    "    df[\"model_id\"] = cfg.moirai.model_id\n",
    "    df[\"psig\"] = \"-\".join(map(str, cfg.moirai.patch_sizes))\n",
    "    df[\"pool\"] = cfg.moirai.pool\n",
    "    df[\"clock_hash\"] = hashes.clockhash\n",
    "    df[\"norm_version\"] = stats[\"norm_version\"]\n",
    "    df[\"datahash\"] = hashes.datahash\n",
    "    chan = getattr(cfg.data, \"moirai_channels\", [\"occ\"])\n",
    "    df[\"c_sig\"] = \"+\".join(chan)  # Ïòà: \"occ\" or \"occ+e_price+s_price\"\n",
    "    return df[KEY_COLS]\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_batch(x_np, backbone, patch_sizes, pool):\n",
    "    x = torch.from_numpy(x_np).to(DEVICE).float()  # (B, C, L) ; Ïó¨Í∏∞ÏÑ† C=1\n",
    "    B, C, L = x.shape\n",
    "    target = x.transpose(1,2)  # (B, L, C)\n",
    "    observed_mask = torch.ones_like(target, dtype=torch.bool)\n",
    "    prediction_mask = torch.zeros(B, L, dtype=torch.bool, device=DEVICE)\n",
    "    sample_id = torch.arange(B, device=DEVICE).unsqueeze(1).expand(B, L)\n",
    "    time_id   = torch.arange(L, device=DEVICE).unsqueeze(0).expand(B, L)\n",
    "    variate_id= torch.zeros(B, L, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    hs_list = []\n",
    "    for ps in patch_sizes:\n",
    "        patch_size = torch.full((B, L), ps, dtype=torch.long, device=DEVICE)\n",
    "        try:\n",
    "            hs = backbone.encode(target=target, observed_mask=observed_mask, patch_size=patch_size)\n",
    "        except AttributeError:\n",
    "            output = backbone(target=target, observed_mask=observed_mask,\n",
    "                              prediction_mask=prediction_mask, sample_id=sample_id,\n",
    "                              time_id=time_id, variate_id=variate_id, patch_size=patch_size)\n",
    "            hs = output if isinstance(output, torch.Tensor) else getattr(output, \"mean\", output.sample())\n",
    "        if hs.dim()==3: hs = hs.mean(dim=1) if pool==\"mean\" else hs.max(dim=1)[0]\n",
    "        elif hs.dim()!=2: raise ValueError(f\"Unexpected hs shape: {hs.shape}\")\n",
    "        hs_list.append(hs)\n",
    "    emb = torch.stack(hs_list).mean(dim=0)  # (B, D_emb)\n",
    "    return emb.cpu().numpy()\n",
    "\n",
    "def _sanitize(s:str)->str:\n",
    "    return \"\".join(ch if (str(ch).isalnum() or ch in \"-._\") else \"_\" for ch in str(s))\n",
    "\n",
    "def _make_cache_path_with_dim(cfg, model_id, patch_sig, pool, emb_dim, hashes, stats):\n",
    "    c_sig = \"+\".join(getattr(cfg.data, \"moirai_channels\", [\"occ\"]))\n",
    "    sig = {\"model\":model_id,\"psig\":patch_sig,\"pool\":pool,\"edim\":str(emb_dim),\n",
    "           \"c\":c_sig, \n",
    "           \"clock\":hashes.clockhash[:8],\"norm\":stats[\"norm_version\"][:8],\"data\":hashes.datahash[:8]}\n",
    "    fname = \"moirai_\" + \"_\".join(f\"{k}-{_sanitize(v)}\" for k,v in sig.items()) + \".pt\"\n",
    "    base_dir = str(Path(getattr(cfg.exec, \"moirai_shared_dir\", \"\")))\n",
    "    return os.path.join(base_dir, fname)\n",
    "\n",
    "\n",
    "def ensure_moirai_cache(keys_df, tables, cfg, hashes, stats):\n",
    "    # Îπà ÌÇ§ Í∞ÄÎìú\n",
    "    if len(keys_df) == 0:\n",
    "        empty_df = pd.DataFrame(columns=keys_df.columns)\n",
    "        edim_default = int(getattr(cfg.moirai, \"emb_dim\", 0))\n",
    "        empty_tensor = torch.zeros(0, edim_default, dtype=torch.float32)\n",
    "        return empty_df, empty_tensor, edim_default\n",
    "\n",
    "    # ÏÇ¨Ïö©Ìï† Ï±ÑÎÑê Í≤∞Ï†ï\n",
    "    chan = getattr(cfg.data, \"moirai_channels\", None)\n",
    "    if chan is None:\n",
    "        chan = [\"occ\"] if bool(getattr(cfg.data, \"baseline_occ_only\", True)) else [\"occ\",\"e_price\",\"s_price\"]\n",
    "\n",
    "    # Í∞ÑÎã®Ìïú Ìó¨Ìçº: (C,L) Ïä§ÌÉù\n",
    "    def stack_channels(tables, z, t0, L, chan_names):\n",
    "        win = slice(t0, t0 + pd.Timedelta(hours=L-1))\n",
    "        arrs = []\n",
    "        for name in chan_names:\n",
    "            # Í∞Å Ï±ÑÎÑêÏùÄ ÏãúÍ∞Ñ√ózone ÌÖåÏù¥Î∏îÏù¥Ïñ¥Ïïº Ìï®(Ïòà: occ/e_price/s_price)\n",
    "            series = tables[name].loc[win, z].values.astype(np.float32)\n",
    "            arrs.append(series)  # (L,)\n",
    "        return np.stack(arrs, axis=0)  # (C, L)\n",
    "\n",
    "    backbone = MoiraiModule.from_pretrained(cfg.moirai.model_id).to(DEVICE).eval()\n",
    "    patch_sig = \"-\".join(map(str, cfg.moirai.patch_sizes))\n",
    "    pool = cfg.moirai.pool\n",
    "\n",
    "    # ---- ÏûÑÎ≤†Îî© Ï∞®Ïõê ÌîÑÎ°úÎπô (1Í∞ú ÏÉòÌîåÎ°ú Ïã§Ìñâ) ----\n",
    "    first_row = keys_df.iloc[0]\n",
    "    z, t0 = first_row[\"zone_id\"], pd.to_datetime(first_row[\"t_start_iso\"])\n",
    "    x_probe = stack_channels(tables, z, t0, cfg.data.L, chan)     # (C,L)\n",
    "    x_probe = x_probe[None, ...]                                  # (1,C,L)\n",
    "    probe_np = encode_batch(x_probe, backbone, cfg.moirai.patch_sizes, pool)  # (1,E)\n",
    "    emb_dim = int(probe_np.shape[1])\n",
    "\n",
    "    # ---- Ï∫êÏãú Í≤ΩÎ°ú(ÌååÏùºÎ™ÖÏóê c_sig Ìè¨Ìï®ÎêòÎèÑÎ°ù ÏïûÏÑú Íµ¨ÌòÑ ÏôÑÎ£åÎùºÍ≥† Í∞ÄÏ†ï) ----\n",
    "    cache_file = _make_cache_path_with_dim(\n",
    "        cfg, cfg.moirai.model_id, patch_sig, pool, emb_dim, hashes, stats\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(cache_file), exist_ok=True)\n",
    "\n",
    "    # ---- Î°úÎìú or Ï¥àÍ∏∞Ìôî ----\n",
    "    if os.path.exists(cache_file):\n",
    "        moirai_df, moirai_emb = torch.load(cache_file, map_location=\"cpu\")\n",
    "        if moirai_emb.ndim != 2 or moirai_emb.shape[1] != emb_dim:\n",
    "            print(f\"[MoiraiCache] dim mismatch in file: {moirai_emb.shape} vs emb_dim={emb_dim} ‚Üí reset\")\n",
    "            moirai_df = pd.DataFrame(columns=keys_df.columns)\n",
    "            moirai_emb = torch.zeros(0, emb_dim, dtype=torch.float32)\n",
    "    else:\n",
    "        moirai_df = pd.DataFrame(columns=keys_df.columns)\n",
    "        moirai_emb = torch.zeros(0, emb_dim, dtype=torch.float32)\n",
    "\n",
    "    # ---- ÎàÑÎùΩ ÌÇ§Îßå Ïù∏ÏΩîÎî© ----\n",
    "    key2row = {tuple(row[KEY_COLS]): i for i, (_, row) in enumerate(moirai_df.iterrows())}\n",
    "    todo_rows = [r for _, r in keys_df.iterrows() if tuple(r[KEY_COLS]) not in key2row]\n",
    "\n",
    "    if len(todo_rows) > 0:\n",
    "        B = cfg.moirai.batch_size\n",
    "        for i in tqdm(range(0, len(todo_rows), B), desc=\"[MoiraiCache] encode\"):\n",
    "            batch_rows = todo_rows[i:i+B]\n",
    "            xs = []\n",
    "            for r in batch_rows:\n",
    "                z = r[\"zone_id\"]\n",
    "                t0 = pd.to_datetime(r[\"t_start_iso\"])\n",
    "                x_cl = stack_channels(tables, z, t0, cfg.data.L, chan)  # (C,L)\n",
    "                xs.append(x_cl)\n",
    "            x_np = np.stack(xs).astype(np.float32)  # (b, C, L)\n",
    "\n",
    "            y_np = encode_batch(x_np, backbone, cfg.moirai.patch_sizes, pool)  # (b, E)\n",
    "            new_emb = torch.from_numpy(y_np).float()   # CPU\n",
    "\n",
    "            if new_emb.shape[1] != moirai_emb.shape[1]:\n",
    "                print(f\"[MoiraiCache] runtime dim mismatch: cache={moirai_emb.shape[1]} vs new={new_emb.shape[1]} ‚Üí reset\")\n",
    "                moirai_df = pd.DataFrame(columns=keys_df.columns)\n",
    "                moirai_emb = torch.zeros(0, new_emb.shape[1], dtype=torch.float32)\n",
    "\n",
    "            batch_df = pd.DataFrame(batch_rows, columns=keys_df.columns)\n",
    "            moirai_df = pd.concat([moirai_df, batch_df], ignore_index=True)\n",
    "            moirai_emb = torch.cat([moirai_emb, new_emb], dim=0)\n",
    "\n",
    "            torch.save((moirai_df, moirai_emb), cache_file + \".tmp\")\n",
    "        os.replace(cache_file + \".tmp\", cache_file)\n",
    "\n",
    "    # Ï∞∏Í≥†: Í≤ΩÎ°ú/ÌñâÏàò/Ï∞®Ïõê Ï∂úÎ†•(ÏõêÌïòÎ©¥ Ïú†ÏßÄ)\n",
    "    # print(f\"[MoiraiCache] path={cache_file} | rows={len(moirai_df)} | dim={emb_dim}\")\n",
    "\n",
    "    return moirai_df, moirai_emb, emb_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature builders\n",
    "def fb_local(tables, z, t0, L, occ_only=True):\n",
    "    win = slice(t0, t0 + pd.Timedelta(hours=L-1))\n",
    "    occ = tables[\"occ\"].loc[win, z].values.astype(np.float32)\n",
    "    return occ[None, :]  # (1, L)\n",
    "\n",
    "def fb_price(tables, z, t0, L):\n",
    "    win = slice(t0, t0 + pd.Timedelta(hours=L-1))\n",
    "    epr = tables[\"e_price\"].loc[win, z].values.astype(np.float32)\n",
    "    spr = tables[\"s_price\"].loc[win, z].values.astype(np.float32)\n",
    "    return np.stack([epr, spr]).astype(np.float32)\n",
    "\n",
    "def fb_weather(tables, t0, L):\n",
    "    win = slice(t0, t0 + pd.Timedelta(hours=L-1))\n",
    "    w = tables[\"weather\"].loc[win]\n",
    "    cont = w[[\"T\",\"P0\",\"P\",\"U\",\"Td\"]].values.T.astype(np.float32)\n",
    "    rain = w.get(\"nRAIN\", pd.Series(0, index=w.index)).clip(0,3).values\n",
    "    rain_oh = np.zeros((4, L), dtype=np.float32); rain_oh[rain, np.arange(L)] = 1.0\n",
    "    return np.concatenate([cont, rain_oh], axis=0).astype(np.float32)\n",
    "\n",
    "def fb_spatial(tables, z, t0, L, zone_ids, spatial_usekeys=(\"occ\", \"dur\", \"vol\"), use_adj=True):\n",
    "    \"\"\"\n",
    "    Í≥µÍ∞Ñ ÌîºÏ≤ò (mean/gap/ratio) √ó len(use_keys)\n",
    "    use_keys: Í≥†Î†§Ìï† ÏãúÍ≥ÑÏó¥ ÌÇ§ Î™©Î°ù (\"occ\",\"dur\",\"vol\")\n",
    "    use_adj : TrueÎ©¥ adj ÌñâÎ†¨ Í≥±Ìï¥ÏÑú Ïù∏Ï†ë zoneÎßå Í≥†Î†§\n",
    "    \"\"\"\n",
    "    win = slice(t0, t0 + pd.Timedelta(hours=L-1))\n",
    "    zi = zone_ids.index(z)\n",
    "    D = tables[\"distance\"].values.astype(np.float32)\n",
    "    W = 1.0 / (D + 1e-6)\n",
    "    np.fill_diagonal(W, 0.0)\n",
    "\n",
    "    if use_adj and \"adj\" in tables:\n",
    "        A = tables[\"adj\"].values.astype(np.float32)\n",
    "        np.fill_diagonal(A, 0.0)\n",
    "        W *= A   # adj=0Ïù¥Î©¥ Ï†úÏô∏\n",
    "\n",
    "    # Ìñâ Ï†ïÍ∑úÌôî\n",
    "    W = W / (W.sum(axis=1, keepdims=True) + 1e-6)\n",
    "    w_i = W[zi]\n",
    "\n",
    "    feats = []\n",
    "    for key in spatial_usekeys:\n",
    "        if key not in tables:\n",
    "            continue\n",
    "        mat = tables[key].loc[win].values.astype(np.float32)  # (L, Z)\n",
    "        mean_val = mat @ w_i\n",
    "        self_val = mat[:, zi]\n",
    "        gap   = self_val - mean_val\n",
    "        ratio = np.clip(self_val / (mean_val + 1e-6), 0, 5)\n",
    "        feats.append(np.stack([mean_val, gap, ratio]))\n",
    "\n",
    "    return np.concatenate(feats, axis=0).astype(np.float32)  # (3*len(use_keys), L)\n",
    "\n",
    "\n",
    "\n",
    "def fb_static(tables, z, stats=None):\n",
    "    row = tables[\"inf\"].loc[z]\n",
    "\n",
    "    def _safe_standardize_log1p(val, stat_key):\n",
    "        v = pd.to_numeric(val, errors=\"coerce\")\n",
    "        if not np.isfinite(v):\n",
    "            return np.float32(0.0)  # mean-impute in standardized space\n",
    "\n",
    "        # üîß ÏùåÏàò Î∞©Ïñ¥: log1p Ï†ÑÏóê 0ÏúºÎ°ú ÌÅ¥Î¶Ω\n",
    "        v = float(v)\n",
    "        if v < 0:\n",
    "            v = 0.0\n",
    "\n",
    "        l = math.log1p(v)\n",
    "        s = stats.get(stat_key, {\"mean\": 0.0, \"std\": 1.0})\n",
    "        mu, sd = float(s.get(\"mean\", 0.0)), max(float(s.get(\"std\", 1.0)), 1e-8)\n",
    "        return np.float32((l - mu) / sd)\n",
    "\n",
    "\n",
    "    # meta: 3Í∞ú Î™®Îëê log1p + z-score (NaN‚Üí0.0)\n",
    "    cc  = _safe_standardize_log1p(row.get(\"charge_count\", np.nan), \"log_charge_count\")\n",
    "    ar  = _safe_standardize_log1p(row.get(\"area\", np.nan),          \"log_area\")\n",
    "    pe  = _safe_standardize_log1p(row.get(\"perimeter\", np.nan),     \"log_perimeter\")\n",
    "    meta = np.array([cc, ar, pe], dtype=np.float32)\n",
    "\n",
    "    # poi: ÏûàÏúºÎ©¥ ÎèôÏùºÌïòÍ≤å Ï≤òÎ¶¨, ÏóÜÏúºÎ©¥ 0\n",
    "    if \"poi_counts\" in tables and isinstance(tables[\"poi_counts\"], pd.DataFrame):\n",
    "        poi_row = tables[\"poi_counts\"].loc[z]\n",
    "        vals = []\n",
    "        for c in poi_row.index:\n",
    "            stat_key = f\"log_{c}\"\n",
    "            if stats is not None and stat_key in stats:\n",
    "                vals.append(_safe_standardize_log1p(poi_row[c], stat_key))\n",
    "            else:\n",
    "                # ÌÜµÍ≥ÑÍ∞Ä ÏóÜÏúºÎ©¥ 0.0ÏúºÎ°ú\n",
    "                vals.append(np.float32(0.0))\n",
    "        poi = np.array(vals, dtype=np.float32)\n",
    "    else:\n",
    "        poi = np.zeros(3, dtype=np.float32)\n",
    "\n",
    "    return np.concatenate([meta, poi]).astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "def fb_time(tables, t0, L):\n",
    "    win = slice(t0, t0 + pd.Timedelta(hours=L-1))\n",
    "    return tables[\"time\"].loc[win].values.T.astype(np.float32)\n",
    "\n",
    "def fb_target(tables, z, t0, H, offset=None):\n",
    "    if offset is None:\n",
    "        offset = getattr(cfg.data, \"pred_offset\", 1)  # Í∏∞Î≥∏ 1\n",
    "    start = t0 + pd.Timedelta(hours=offset)\n",
    "    end   = start + pd.Timedelta(hours=H-1)\n",
    "    if H == 1:\n",
    "        y = tables[\"occ\"].loc[start, z].astype(np.float32)\n",
    "        y = np.array([y], dtype=np.float32)\n",
    "        mask = np.isfinite(y)\n",
    "        return y, mask\n",
    "    else:\n",
    "        y_idx = slice(start, end)\n",
    "        y = tables[\"occ\"].loc[y_idx, z].values.astype(np.float32)\n",
    "        mask = np.isfinite(y)\n",
    "        return y, mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ÏúÑÏóêÍ∫º ÎîîÎ≤ÑÍ∑∏\n",
    "# ÏÉàÎ°ú/ÏàòÏ†ï: EVDataset\n",
    "# ------------------------------\n",
    "# EVDataset (ÏàòÏ†ï Î≤ÑÏ†Ñ)\n",
    "# ------------------------------\n",
    "class EVDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, si_df: pd.DataFrame, tables, zone_ids, cfg, stats=None):\n",
    "        self.si = si_df.reset_index(drop=True)\n",
    "        self.tables = tables\n",
    "        self.zone_ids = list(zone_ids)\n",
    "        self.cfg = cfg\n",
    "        self.stats = stats  # ‚òÖ fb_staticÏóêÏÑú ÏÇ¨Ïö©\n",
    "\n",
    "        # spatial ÏòµÏÖò (cfg.data.spatial ÎÑ§ÏûÑÏä§ÌéòÏù¥Ïä§)\n",
    "        self.spatial_keys = tuple(getattr(cfg.data, \"spatial_usekeys\", (\"occ\",)))\n",
    "        self.spatial_use_adj = bool(getattr(cfg.data, \"spatial_use_adj\", True))\n",
    "        self.spatial_dist_thresh = getattr(getattr(cfg.data, \"spatial\", SimpleNamespace()), \"dist_thresh\", None)\n",
    "        self.spatial_fallback_k = getattr(getattr(cfg.data, \"spatial\", SimpleNamespace()), \"fallback_k\", 5)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.si)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.si.iloc[idx]\n",
    "        z   = r[\"zone_id\"]\n",
    "        t0  = pd.to_datetime(r[\"t_start\"])\n",
    "        L   = int(r[\"L\"])\n",
    "        H   = int(self.cfg.data.H)\n",
    "\n",
    "        # ------ build inputs ------\n",
    "        x_local   = fb_local  (self.tables, z, t0, L)                         \n",
    "        x_weather = fb_weather(self.tables, t0, L)                            \n",
    "        x_price   = fb_price  (self.tables, z, t0, L)                        \n",
    "\n",
    "        # ‚òÖ fb_spatial ÌôïÏû• Î≤ÑÏ†Ñ Ìò∏Ï∂ú\n",
    "        x_spatial = fb_spatial(\n",
    "            self.tables, z, t0, L, self.zone_ids,\n",
    "            spatial_usekeys=self.spatial_keys,\n",
    "            use_adj=self.spatial_use_adj\n",
    "        )\n",
    "\n",
    "        x_time    = fb_time   (self.tables, t0, L)                            \n",
    "        # ‚òÖ fb_staticÏóê stats Ï†ÑÎã¨\n",
    "        x_static  = fb_static (self.tables, z, stats=self.stats)              \n",
    "\n",
    "        # ------ target ------\n",
    "        y, mask = fb_target(self.tables, z, t0, H, offset=getattr(self.cfg.data, \"pred_offset\", 1))\n",
    "\n",
    "        # ÎîîÎ≤ÑÍ∑∏ Ïú†Ìö®ÏÑ± Ï≤¥ÌÅ¨ (ÏõêÎûò ÏΩîÎìú Í∑∏ÎåÄÎ°ú ÎëêÏñ¥ÎèÑ OK)\n",
    "        def _chk(name, arr, expect_ndim=None):\n",
    "            if not np.isfinite(arr).all():\n",
    "                bad = np.sum(~np.isfinite(arr))\n",
    "                raise ValueError(f\"[EVDataset] non-finite in {name} (bad={bad}) | idx={idx} z={z} t0={t0}\")\n",
    "            if expect_ndim is not None and arr.ndim != expect_ndim:\n",
    "                raise ValueError(f\"[EVDataset] ndim mismatch for {name}: got {arr.ndim}, expect {expect_ndim}\")\n",
    "        _chk(\"x_local\",   x_local,   expect_ndim=2)\n",
    "        _chk(\"x_weather\", x_weather, expect_ndim=2)\n",
    "        _chk(\"x_price\",   x_price,   expect_ndim=2)\n",
    "        _chk(\"x_spatial\", x_spatial, expect_ndim=2)\n",
    "        _chk(\"x_time\",    x_time,    expect_ndim=2)\n",
    "        _chk(\"x_static\",  x_static,  expect_ndim=1)\n",
    "        _chk(\"y\", y, expect_ndim=1)\n",
    "\n",
    "        has_target = bool(np.isfinite(y[mask]).any()) if mask.any() else False\n",
    "\n",
    "        moirai_key = {\n",
    "            \"zone_id\": z,\n",
    "            \"t_start_iso\": t0.strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "            \"L\": L,\n",
    "            \"occ_only\": int(getattr(self.cfg.data, \"baseline_occ_only\", True)),\n",
    "            \"c_sig\": \"+\".join(getattr(self.cfg.data, \"moirai_channels\", [\"occ\"])),\n",
    "            \"model_id\": self.cfg.moirai.model_id,\n",
    "            \"psig\": \"-\".join(map(str, self.cfg.moirai.patch_sizes)),\n",
    "            \"pool\": self.cfg.moirai.pool,\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"x_local\":  x_local,\n",
    "            \"x_weather\":x_weather,\n",
    "            \"x_price\":  x_price,\n",
    "            \"x_spatial\":x_spatial,\n",
    "            \"x_time\":   x_time,\n",
    "            \"x_static\": x_static,\n",
    "            \"y\":        y.astype(np.float32),\n",
    "            \"mask\":     mask.astype(bool),\n",
    "            \"has_target\": has_target,\n",
    "            \"moirai_key\": moirai_key,\n",
    "        }\n",
    "\n",
    "def _to_f32_on_device(xs):\n",
    "    return torch.from_numpy(np.stack(xs)).float().to(DEVICE)\n",
    "\n",
    "def _stack_bool_on_device(xs):\n",
    "    return torch.from_numpy(np.stack(xs)).to(DEVICE)\n",
    "\n",
    "def collate_fn_builder_baseline(moirai_emb, key2row, hashes, stats, *, debug_print=False):\n",
    "    def collate_fn(batch):\n",
    "        # --- 1) ÏÉòÌîå Î†àÎ≤® ÌïÑÌÑ∞: Ïú†Ìö® ÌÉÄÍπÉ ÏûàÎäî ÏÉòÌîåÎßå\n",
    "        valid_batch = [b for b in batch if b.get(\"has_target\", True) and b[\"mask\"].any()]\n",
    "        if len(valid_batch) == 0:\n",
    "            # ÌïôÏäµ Î£®ÌîÑÏóêÏÑú skip Ï≤òÎ¶¨Ìï† Ïàò ÏûàÍ≤å ÌîåÎûòÍ∑∏ Î∞òÌôò\n",
    "            if debug_print:\n",
    "                print(\"[collate/baseline] skip: no valid samples in this batch\")\n",
    "            return {\"skip\": True}\n",
    "\n",
    "        batch = valid_batch\n",
    "\n",
    "        # --- 2) Moirai ÌÇ§ Íµ¨ÏÑ±\n",
    "        rows = []\n",
    "        for b in batch:\n",
    "            mk = dict(b[\"moirai_key\"])\n",
    "            mk[\"clock_hash\"] = hashes.clockhash\n",
    "            mk[\"norm_version\"] = stats[\"norm_version\"]\n",
    "            mk[\"datahash\"] = hashes.datahash\n",
    "            rows.append([mk[k] for k in KEY_COLS])\n",
    "        keys_df = pd.DataFrame(rows, columns=KEY_COLS)\n",
    "        idxs = [key2row[tuple(r)] for _, r in keys_df.iterrows()]\n",
    "\n",
    "        # --- 3) Ïä§ÌÉù\n",
    "        out = {\n",
    "            \"x_local\":   _to_f32_on_device([b[\"x_local\"]   for b in batch]),  # (B,1,L)\n",
    "            \"x_weather\": _to_f32_on_device([b[\"x_weather\"] for b in batch]),\n",
    "            \"x_price\":   _to_f32_on_device([b[\"x_price\"]   for b in batch]),\n",
    "            \"x_spatial\": _to_f32_on_device([b[\"x_spatial\"] for b in batch]),\n",
    "            \"x_static\":  _to_f32_on_device([b[\"x_static\"]  for b in batch]),\n",
    "            \"x_time\":    _to_f32_on_device([b[\"x_time\"]    for b in batch]),\n",
    "            \"y\":         _to_f32_on_device([b[\"y\"]         for b in batch]),  # (B,H)\n",
    "            \"mask\":      _stack_bool_on_device([b[\"mask\"]  for b in batch]),  # (B,H)\n",
    "            \"h_local_moirai\": moirai_emb[idxs].to(DEVICE).float(),            # (B, E)\n",
    "            \"skip\": False,\n",
    "        }\n",
    "\n",
    "        # --- 4) Î∞∞Ïπò Ïú†Ìö®ÏÑ± ÎîîÎ≤ÑÍ∑∏\n",
    "        sel = out[\"mask\"].bool()\n",
    "        if debug_print:\n",
    "            B, H = out[\"y\"].shape\n",
    "            print(f\"[collate/baseline] B={B} H={H} | valid_targets={int(sel.sum().item())}\")\n",
    "        if sel.sum().item() == 0:\n",
    "            if debug_print:\n",
    "                print(\"[collate/baseline] skip: mask has no True\")\n",
    "            out[\"skip\"] = True\n",
    "            return out\n",
    "\n",
    "        # NaN/Inf Ï≤¥ÌÅ¨ (yÎäî mask Í∏∞Ï§ÄÏúºÎ°úÎßå Í≤ÄÏÇ¨)\n",
    "        if not torch.isfinite(out[\"y\"][sel]).all():\n",
    "            raise ValueError(\"[collate/baseline] non-finite in y (masked selection)\")\n",
    "        # ÏûÖÎ†•Îì§ Í∞ÑÎã® Í≤ÄÏÇ¨(ÏõêÌïòÎ©¥ Îçî Ï∂îÍ∞Ä)\n",
    "        for k in [\"x_local\",\"x_weather\",\"x_price\",\"x_spatial\",\"x_time\",\"x_static\",\"h_local_moirai\"]:\n",
    "            if not torch.isfinite(out[k]).all():\n",
    "                raise ValueError(f\"[collate/baseline] non-finite in {k}\")\n",
    "\n",
    "        return out\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "def collate_fn_builder_finetune(*, debug_print=False):\n",
    "    def collate_fn(batch):\n",
    "        # --- 1) ÏÉòÌîå Î†àÎ≤® ÌïÑÌÑ∞\n",
    "        valid_batch = [b for b in batch if b.get(\"has_target\", True) and b[\"mask\"].any()]\n",
    "        if len(valid_batch) == 0:\n",
    "            if debug_print:\n",
    "                print(\"[collate/finetune] skip: no valid samples in this batch\")\n",
    "            return {\"skip\": True}\n",
    "\n",
    "        batch = valid_batch\n",
    "\n",
    "        # --- 2) Ïä§ÌÉù\n",
    "        out = {\n",
    "            \"x_local\":   _to_f32_on_device([b[\"x_local\"]   for b in batch]),\n",
    "            \"x_weather\": _to_f32_on_device([b[\"x_weather\"] for b in batch]),\n",
    "            \"x_price\":   _to_f32_on_device([b[\"x_price\"]   for b in batch]),\n",
    "            \"x_spatial\": _to_f32_on_device([b[\"x_spatial\"] for b in batch]),\n",
    "            \"x_static\":  _to_f32_on_device([b[\"x_static\"]  for b in batch]),\n",
    "            \"x_time\":    _to_f32_on_device([b[\"x_time\"]    for b in batch]),\n",
    "            \"y\":         _to_f32_on_device([b[\"y\"]         for b in batch]),\n",
    "            \"mask\":      _stack_bool_on_device([b[\"mask\"]  for b in batch]),\n",
    "            \"skip\": False,\n",
    "        }\n",
    "\n",
    "        # --- 3) Î∞∞Ïπò Ïú†Ìö®ÏÑ± ÎîîÎ≤ÑÍ∑∏\n",
    "        sel = out[\"mask\"].bool()\n",
    "        if debug_print:\n",
    "            B, H = out[\"y\"].shape\n",
    "            print(f\"[collate/finetune] B={B} H={H} | valid_targets={int(sel.sum().item())}\")\n",
    "        if sel.sum().item() == 0:\n",
    "            if debug_print:\n",
    "                print(\"[collate/finetune] skip: mask has no True\")\n",
    "            out[\"skip\"] = True\n",
    "            return out\n",
    "\n",
    "        if not torch.isfinite(out[\"y\"][sel]).all():\n",
    "            raise ValueError(\"[collate/finetune] non-finite in y (masked selection)\")\n",
    "        for k in [\"x_local\",\"x_weather\",\"x_price\",\"x_spatial\",\"x_time\",\"x_static\"]:\n",
    "            if not torch.isfinite(out[k]).all():\n",
    "                raise ValueError(f\"[collate/finetune] non-finite in {k}\")\n",
    "\n",
    "        return out\n",
    "    return collate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVForecastModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        D   = cfg.model.D_model\n",
    "        p   = cfg.model.dropout\n",
    "\n",
    "        # ----- local branch (modeÎ≥Ñ) -----\n",
    "        if cfg.model.mode == \"baseline\":\n",
    "            # emb_dimÏùÄ ensure_moirai_cache() Ïù¥ÌõÑ cfg.model.D_moiraiÍ∞Ä ÏÑ∏ÌåÖÎêòÏñ¥ ÏûàÏùå\n",
    "            D_m = cfg.model.D_moirai\n",
    "            self.adapter = nn.Sequential(\n",
    "                nn.Linear(D_m, D), nn.GELU(), nn.Dropout(p), nn.Linear(D, D)\n",
    "            )\n",
    "            self.moirai_backbone = None\n",
    "            self.local_proj = None  # baselineÏóêÏÑ† ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏùå\n",
    "        else:  # finetune\n",
    "            # ÌïôÏäµ Í∞ÄÎä•Ìïú Moirai\n",
    "            self.moirai_backbone = MoiraiModule.from_pretrained(cfg.moirai.model_id)\n",
    "            for prm in self.moirai_backbone.parameters():\n",
    "                prm.requires_grad = True\n",
    "            # D_mÏùÄ Î™®Îìà ÎÇ¥Î∂Ä ÏÑ§Ï†ïÏóê Îî∞Îùº Îã¨ÎùºÏßà Ïàò ÏûàÏñ¥ ‚Üí Ï≤´ forwardÏóêÏÑú lazy init\n",
    "            self.adapter = None\n",
    "            self.local_proj = None  # Ï≤´ forwardÏóêÏÑú Moirai Ï∂úÎ†• Ï∞®Ïõê Î≥¥Í≥† ÏÉùÏÑ±\n",
    "\n",
    "        # ----- ÎèôÏ†Å Ï±ÑÎÑê Í≥ÑÏÇ∞ -----\n",
    "        # weather: Ïó∞ÏÜç 5(T,P0,P,U,Td) + rain ÏõêÌï´ 4 = 9\n",
    "        cin_weather = 9\n",
    "        # price: e_price, s_price = 2\n",
    "        cin_price   = 2\n",
    "        # time: hour_sin, hour_cos, dow_sin, dow_cos, is_offday = 5\n",
    "        cin_time    = 5\n",
    "\n",
    "        # spatial: (mean, gap, ratio) √ó len(use_keys)\n",
    "        # Ïö∞ÏÑ†ÏàúÏúÑ: cfg.data.spatial.use_keys -> cfg.data.spatial_usekeys -> Í∏∞Î≥∏ (\"occ\",)\n",
    "        spatial_use_keys = None\n",
    "    \n",
    "        if hasattr(cfg.data, \"spatial_usekeys\"):\n",
    "            spatial_use_keys = tuple(cfg.data.spatial_usekeys)\n",
    "        else:\n",
    "            spatial_use_keys = (\"occ\",)  # fallback\n",
    "\n",
    "        cin_spatial = 3 * max(1, len(spatial_use_keys))\n",
    "\n",
    "        # ----- other encoders -----\n",
    "        ks = cfg.model.KERNEL_SIZE; pad = ks//2\n",
    "        def conv_block(cin):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(cin, 64, ks, padding=pad), nn.GELU(),\n",
    "                nn.Conv1d(64, 64, ks, padding=pad), nn.GELU()\n",
    "            )\n",
    "\n",
    "        self.enc_weather = conv_block(cin_weather)\n",
    "        self.enc_price   = conv_block(cin_price)\n",
    "        self.enc_spatial = conv_block(cin_spatial)\n",
    "        self.enc_time    = conv_block(cin_time)\n",
    "\n",
    "        # static: charge_count/area/perimeter + poi 3Í∞ú = 6\n",
    "        self.enc_static  = nn.Sequential(nn.Linear(6, 128), nn.GELU(), nn.Dropout(p), nn.Linear(128, D))\n",
    "\n",
    "        self.proj_weather= nn.Linear(64, D)\n",
    "        self.proj_price  = nn.Linear(64, D)\n",
    "        self.proj_spatial= nn.Linear(64, D)\n",
    "        self.proj_time   = nn.Linear(64, D)\n",
    "\n",
    "        def align(): return nn.Sequential(nn.LayerNorm(D), nn.Dropout(p), nn.Linear(D, D))\n",
    "        self.align_local = align(); self.align_weather = align(); self.align_price = align()\n",
    "        self.align_spatial = align(); self.align_static = align(); self.align_time = align()\n",
    "\n",
    "        self.mha  = nn.MultiheadAttention(D, cfg.model.nhead, dropout=p, batch_first=True)\n",
    "        self.head = nn.Linear(D, self.cfg.data.H)\n",
    "        self.use_softplus = bool(getattr(cfg.model, \"nonneg_head\", False))\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # ===== 1) Local (modeÎ≥Ñ Í≤ΩÎ°ú) =====\n",
    "        if self.cfg.model.mode == \"baseline\":\n",
    "            # Ï∫êÏãúÏóêÏÑú Ïò® h_local_moirai ‚Üí adapter\n",
    "            h_local = self.adapter(batch[\"h_local_moirai\"])\n",
    "        else:\n",
    "            # x_local: (B,1,L) ‚Üí Moirai -> (B,T,Dm or B,Dm)\n",
    "            x_local = batch[\"x_local\"]  # (B,1,L)\n",
    "            B, C, L = x_local.shape\n",
    "            target = x_local.transpose(1, 2)  # (B,L,1)\n",
    "            observed_mask = torch.ones_like(target, dtype=torch.bool)\n",
    "            prediction_mask = torch.zeros(B, L, dtype=torch.bool, device=target.device)\n",
    "            sample_id = torch.arange(B, device=target.device).unsqueeze(1).expand(B, L)\n",
    "            time_id   = torch.arange(L, device=target.device).unsqueeze(0).expand(B, L)\n",
    "            variate_id= torch.zeros(B, L, dtype=torch.long, device=target.device)\n",
    "            patch_size = torch.full((B, L), self.cfg.moirai.patch_sizes[0], dtype=torch.long, device=target.device)\n",
    "\n",
    "            # ‚îÄ‚îÄ lazy init: local_projÍ∞Ä ÏïÑÏßÅ ÏóÜÏúºÎ©¥ Î™®ÏñëÎßå Ìïú Î≤à ÌôïÏù∏\n",
    "            if self.local_proj is None:\n",
    "                with torch.no_grad():\n",
    "                    out_probe = self.moirai_backbone(\n",
    "                        target=target, observed_mask=observed_mask,\n",
    "                        prediction_mask=prediction_mask, sample_id=sample_id,\n",
    "                        time_id=time_id, variate_id=variate_id, patch_size=patch_size\n",
    "                    )\n",
    "                    hs_probe = out_probe if isinstance(out_probe, torch.Tensor) else getattr(out_probe, \"mean\", out_probe.sample())\n",
    "                    if hs_probe.dim() == 3:  # (B,T,Dm)\n",
    "                        hs_probe = hs_probe.mean(dim=1)  # cfg.moirai.pool Î∞òÏòÅÌïòÎ†§Î©¥ Ïó¨Í∏∞ÏÑú Î∂ÑÍ∏∞\n",
    "                    D_m = int(hs_probe.shape[1])\n",
    "                # Ïù¥Ï†ú Ìà¨ÏÇ¨Í∏∞ ÏÉùÏÑ±(D_m‚ÜíD)\n",
    "                D = self.cfg.model.D_model; p = self.cfg.model.dropout\n",
    "                self.local_proj = nn.Sequential(nn.Linear(D_m, D), nn.GELU(), nn.Dropout(p), nn.Linear(D, D)).to(target.device)\n",
    "\n",
    "            # Ïã§Ï†ú forwardÎäî Í∑∏ÎûòÌîÑÎ•º Ïú†ÏßÄÌï¥Ïïº ÌïòÎØÄÎ°ú Ïû¨Í≥ÑÏÇ∞(ÌîÑÎ°úÎ∏åÎäî no_gradÎ°úÎßå Î™®Ïñë ÌôïÏù∏Ïö©)\n",
    "            out = self.moirai_backbone(\n",
    "                target=target, observed_mask=observed_mask,\n",
    "                prediction_mask=prediction_mask, sample_id=sample_id,\n",
    "                time_id=time_id, variate_id=variate_id, patch_size=patch_size\n",
    "            )\n",
    "            hs = out if isinstance(out, torch.Tensor) else getattr(out, \"mean\", out.sample())\n",
    "            if hs.dim() == 3:\n",
    "                hs = hs.mean(dim=1)  # pool=\"mean\" Í∏∞Î≥∏\n",
    "            h_local = self.local_proj(hs)\n",
    "\n",
    "        # ===== 2) Other modalities =====\n",
    "        h_weather = self.proj_weather(self.enc_weather(batch[\"x_weather\"]).mean(-1))\n",
    "        h_price   = self.proj_price  (self.enc_price  (batch[\"x_price\"  ]).mean(-1))\n",
    "        h_spatial = self.proj_spatial(self.enc_spatial(batch[\"x_spatial\"]).mean(-1))\n",
    "        h_time    = self.proj_time   (self.enc_time   (batch[\"x_time\"   ]).mean(-1))\n",
    "        h_static  = self.enc_static(batch[\"x_static\"])\n",
    "\n",
    "        # ===== 3) Align + fusion + head =====\n",
    "        def gate(align, h): return torch.sigmoid(align(h)) * h\n",
    "        h_local_a   = gate(self.align_local,   h_local)\n",
    "        h_weather_a = gate(self.align_weather, h_weather)\n",
    "        h_price_a   = gate(self.align_price,   h_price)\n",
    "        h_spatial_a = gate(self.align_spatial, h_spatial)\n",
    "        h_static_a  = gate(self.align_static,  h_static)\n",
    "        h_time_a    = gate(self.align_time,    h_time)\n",
    "\n",
    "        tokens = torch.stack([h_weather_a, h_price_a, h_spatial_a, h_static_a, h_time_a], dim=1)\n",
    "        q = h_local_a.unsqueeze(1)\n",
    "        h, _ = self.mha(q, tokens, tokens)\n",
    "        y_hat = self.head(h.squeeze(1))\n",
    "        return self.softplus(y_hat) if self.use_softplus else y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def loss_epoch(model, dataloader, criterion=F.mse_loss, optimizer=None, scheduler=None, sched_per_batch=False, cfg=None, desc=None):\n",
    "    \"\"\"\n",
    "    Ìïú epoch ÌïôÏäµ/Í≤ÄÏ¶ù (optimizer Ïú†Î¨¥Î°ú train/eval).\n",
    "    - lossÎäî mask=True ÏúÑÏπòÎßå ÌèâÍ∑†(reduction='mean').\n",
    "    - epoch ÌèâÍ∑†ÏùÄ Ïú†Ìö®ÌÉÄÍπÉÏàò Í∞ÄÏ§ë ÌèâÍ∑†.\n",
    "    - clip_gradÎäî cfg.train.clip_grad ÏÇ¨Ïö©(ÏóÜÏúºÎ©¥ ÎØ∏Ï†ÅÏö©).\n",
    "    - schedulerÎäî ÏòµÏÖò: per-batch(step_per_batch=True) ÎòêÎäî per-epoch(train() Î∞îÍπ•).\n",
    "    \"\"\"\n",
    "    is_train = optimizer is not None\n",
    "    model.train() if is_train else model.eval()\n",
    "\n",
    "    clip_grad = None\n",
    "    if cfg is not None and hasattr(cfg, \"train\") and hasattr(cfg.train, \"clip_grad\"):\n",
    "        clip_grad = cfg.train.clip_grad\n",
    "        # 0 ÎòêÎäî FalseÎ©¥ ÎπÑÏ†ÅÏö©\n",
    "\n",
    "    total_loss_w = 0.0\n",
    "    total_valid  = 0\n",
    "    batch_losses = []\n",
    "\n",
    "    loop = tqdm(dataloader, desc=desc or (\"train\" if is_train else \"valid\"))\n",
    "    for batch in loop:\n",
    "        if batch.get(\"skip\", False):\n",
    "            continue\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # forward & masked loss\n",
    "        y_hat = model(batch)             # (B,H)\n",
    "        sel   = batch[\"mask\"].bool()     # (B,H)\n",
    "        n     = int(sel.sum().item())\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        loss = criterion(y_hat[sel], batch[\"y\"][sel])  # ÌèâÍ∑† ÏÜêÏã§\n",
    "\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            if clip_grad and clip_grad > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            optimizer.step()\n",
    "            if scheduler is not None and sched_per_batch:\n",
    "                scheduler.step()\n",
    "\n",
    "        total_loss_w += float(loss.item()) * n\n",
    "        total_valid  += n\n",
    "        batch_losses.append(float(loss.item()))\n",
    "        loop.set_postfix(loss=f\"{loss.item():.4f}\", n_valid=n)\n",
    "\n",
    "    epoch_loss = total_loss_w / max(1, total_valid)\n",
    "    return epoch_loss, batch_losses\n",
    "\n",
    "import os, re, time, torch\n",
    "\n",
    "def _find_latest_epoch_ckpt(ckpt_dir):\n",
    "    \"\"\"\n",
    "    ckpt_dirÏóêÏÑú epoch_####.ckpt / .pt Ï§ë Í∞ÄÏû• Î≤àÌò∏Í∞Ä ÌÅ∞ ÌååÏùº Í≤ΩÎ°ú Î∞òÌôò. ÏóÜÏúºÎ©¥ None.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(ckpt_dir):\n",
    "        return None\n",
    "    files = os.listdir(ckpt_dir)\n",
    "    cand = []\n",
    "    for fn in files:\n",
    "        m = re.match(r\"epoch_(\\d+)\\.(ckpt|pt)$\", fn)\n",
    "        if m:\n",
    "            cand.append((int(m.group(1)), os.path.join(ckpt_dir, fn)))\n",
    "    if not cand:\n",
    "        return None\n",
    "    cand.sort(key=lambda x: x[0])\n",
    "    return cand[-1][1]  # latest path\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion,\n",
    "          optimizer, save_best_path, config=None, resume_path=None, scheduler=None, sched_per_batch=False):\n",
    "    \"\"\"\n",
    "    - best Í∏∞Ï§Ä: validation loss (criterion)\n",
    "    - metrics: cfg.train.metrics ÏÇ¨Ïö© (ÏóÜÏúºÎ©¥ Í∏∞Î≥∏ 4Ï¢Ö)\n",
    "    - scheduler: Ï£ºÏñ¥ÏßÄÎ©¥ ÏÇ¨Ïö©(NoneÏù¥Î©¥ Î¨¥Ïãú)\n",
    "    - resume: cfg.exec.new_model_train==FalseÎ©¥ run_dir/checkpointsÏóêÏÑú ÏµúÏã† epoch_* Î°úÎìú(Î™®Îç∏+ÏòµÌã∞ÎßàÏù¥Ï†Ä(+Ïä§ÏºÄÏ§ÑÎü¨)).\n",
    "              resume_pathÍ∞Ä Îî∞Î°ú Ï£ºÏñ¥ÏßÄÎ©¥ Í∑∏Í±∏ Ïö∞ÏÑ† ÏÇ¨Ïö©.\n",
    "    \"\"\"\n",
    "    cfg = config or {}\n",
    "    EPOCHS   = getattr(cfg.train, \"epochs\", 5)\n",
    "    METRICS  = tuple(getattr(cfg.train, \"metrics\", [\"RMSE\",\"MAPE\",\"RAE\",\"MAE\"]))\n",
    "    RUN_DIR  = cfg.out.run_dir\n",
    "    CKPT_DIR = cfg.out.ckpt_dir\n",
    "    NEW_TRAIN = bool(getattr(cfg.exec, \"new_model_train\", True))\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # ---- FULL RESUME (A)\n",
    "    start_epoch = 0\n",
    "    if not NEW_TRAIN:\n",
    "        # Ïö∞ÏÑ†ÏàúÏúÑ: Î™ÖÏãú resume_path > ÏµúÏã† epoch_* ÏûêÎèôÌÉêÏÉâ\n",
    "        latest = resume_path if resume_path and os.path.exists(resume_path) else _find_latest_epoch_ckpt(CKPT_DIR)\n",
    "        if latest:\n",
    "            print(f\"‚ñ∂Ô∏è Full resume from: {latest}\")\n",
    "            ckpt = torch.load(latest, map_location=DEVICE)\n",
    "            model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "            if \"optimizer_state_dict\" in ckpt:\n",
    "                optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "            if scheduler and ckpt.get(\"scheduler_state_dict\") is not None:\n",
    "                try:\n",
    "                    scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
    "                except Exception:\n",
    "                    print(\"[warn] scheduler state incompatible; ignoring\")\n",
    "            start_epoch = int(ckpt.get(\"epoch\", 0))\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è new_model_train=False Ïù¥ÏßÄÎßå Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Í∞Ä ÏóÜÏñ¥ ÏÉàÎ°ú ÏãúÏûëÌï©ÎãàÎã§.\")\n",
    "\n",
    "    history = {\"train_epoch\": [], \"train_iter\": [], \"val_epoch\": [], \"val_metrics\": []}\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_epoch = 0\n",
    "\n",
    "    t0 = time.time()\n",
    "    for ep in range(start_epoch, EPOCHS):\n",
    "        eidx = ep + 1\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f\"\\n[Epoch {eidx}/{EPOCHS}] LR={lr:.3e}\")\n",
    "\n",
    "        # train\n",
    "        train_loss, train_batches = loss_epoch(\n",
    "            model, train_loader, criterion=criterion,\n",
    "            optimizer=optimizer, scheduler=scheduler, sched_per_batch=sched_per_batch, cfg=cfg, desc=\"train\"\n",
    "        )\n",
    "        history[\"train_epoch\"].append(train_loss)\n",
    "        history[\"train_iter\"].append(train_batches)\n",
    "        print(f\" Train Loss: {train_loss:.4f} | iters: {len(train_batches)}\")\n",
    "\n",
    "        # valid\n",
    "        val_loss, _ = loss_epoch(model, val_loader, criterion=criterion, optimizer=None, cfg=cfg, desc=\"valid\")\n",
    "        history[\"val_epoch\"].append(val_loss)\n",
    "        metrics = compute_metrics(model, val_loader, metrics=METRICS)\n",
    "        history[\"val_metrics\"].append(metrics)\n",
    "        print(\" Val  Loss: {:.4f} | {}\".format(val_loss, \" | \".join([f\"{k}:{v:.4f}\" for k,v in metrics.items()])))\n",
    "\n",
    "        # best by val loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = eidx\n",
    "            os.makedirs(os.path.dirname(save_best_path) or \".\", exist_ok=True)\n",
    "            torch.save({\n",
    "                \"epoch\": best_epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": best_val_loss,\n",
    "                \"val_metrics\": metrics\n",
    "            }, save_best_path)\n",
    "            print(f\" ‚úÖ Best model updated @ epoch {best_epoch} (ValLoss {best_val_loss:.4f})\")\n",
    "\n",
    "        # per-epoch ckpt\n",
    "        ep_ckpt = cfg.out.epoch_ckpt_tmpl.format(eidx)  # Ïòà: \".../epoch_0001.ckpt\"\n",
    "        torch.save({\n",
    "            \"epoch\": eidx,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_metrics\": metrics\n",
    "        }, ep_ckpt)\n",
    "\n",
    "        # scheduler per-epoch step (per-batchÎ©¥ loss_epochÏóêÏÑú Ïù¥ÎØ∏ Ï≤òÎ¶¨)\n",
    "        if scheduler is not None and not sched_per_batch:\n",
    "            scheduler.step()\n",
    "\n",
    "    print(f\"\\nDone. Best epoch={best_epoch} | best val loss={best_val_loss:.4f} | time={time.time()-t0:.1f}s\")\n",
    "    return history\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics(model, dataloader, metrics=(\"RMSE\",\"MAPE\",\"RAE\",\"MAE\"), eps=1e-8):\n",
    "    model.eval()\n",
    "    agg = {m: 0.0 for m in metrics}\n",
    "    total_valid = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        if batch.get(\"skip\", False):\n",
    "            continue\n",
    "        y_hat = model(batch)\n",
    "        y     = batch[\"y\"]\n",
    "        sel   = batch[\"mask\"].bool()\n",
    "        n     = int(sel.sum().item())\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        diff   = (y_hat - y)[sel]\n",
    "        absdif = diff.abs()\n",
    "        sqdif  = diff.pow(2)\n",
    "        y_sel  = y[sel]\n",
    "\n",
    "        rmse = torch.sqrt(sqdif.mean()).item()\n",
    "        mae  = absdif.mean().item()\n",
    "        mape = (absdif / (y_sel.abs() + eps)).mean().item() * 100.0\n",
    "        rae  = (absdif.sum().item()) / ((y_sel - y_sel.mean()).abs().sum().item() + eps)\n",
    "\n",
    "        if \"RMSE\" in metrics: agg[\"RMSE\"] += rmse * n\n",
    "        if \"MAE\"  in metrics: agg[\"MAE\"]  += mae  * n\n",
    "        if \"MAPE\" in metrics: agg[\"MAPE\"] += mape * n\n",
    "        if \"RAE\"  in metrics: agg[\"RAE\"]  += rae  * n\n",
    "        total_valid += n\n",
    "\n",
    "    return {m: (agg[m] / max(1, total_valid)) for m in metrics}\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, test_loader, criterion=F.mse_loss, metrics=(\"RMSE\",\"MAPE\",\"RAE\",\"MAE\")):\n",
    "    model.eval()\n",
    "    total_loss_w = 0.0\n",
    "    total_valid  = 0\n",
    "    for batch in tqdm(test_loader, desc=\"test\"):\n",
    "        if batch.get(\"skip\", False):\n",
    "            continue\n",
    "        out = model(batch)\n",
    "        sel = batch[\"mask\"].bool()\n",
    "        n   = int(sel.sum().item())\n",
    "        if n == 0:\n",
    "            continue\n",
    "        loss = criterion(out[sel], batch[\"y\"][sel])\n",
    "        total_loss_w += float(loss.item()) * n\n",
    "        total_valid  += n\n",
    "    test_loss = total_loss_w / max(1, total_valid)\n",
    "\n",
    "    m = compute_metrics(model, test_loader, metrics=metrics)\n",
    "    print(\"Test:\", \" | \".join([f\"{k}:{v:.4f}\" for k,v in m.items()]), f\"| Loss:{test_loss:.4f}\")\n",
    "    return {\"loss\": test_loss, **m}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(cfg):\n",
    "    # 1) Load & align data\n",
    "    clock, tables, zone_ids, hashes = prepare_data(cfg)\n",
    "\n",
    "    # 2) (Optional) POI counts via cache (fast, idempotent)\n",
    "    if getattr(cfg.data, \"use_poi\", False) and os.path.exists(cfg.data.paths.get(\"poi\", \"\")):\n",
    "        try:\n",
    "            ensure_poi_counts(tables, cfg)  # you implemented (uses build_poi_counts_fast inside)\n",
    "        except ModuleNotFoundError:\n",
    "            print(\"[warn] scikit-learn not installed; skip POI\")\n",
    "            cfg.data.use_poi = False\n",
    "\n",
    "    # 3) Sample indices + standardization (tables standardized in-place)\n",
    "    si, stats = build_sample_indices(tables, clock, zone_ids, cfg)\n",
    "    print(f\"[norm_version] {stats['norm_version']}\")\n",
    "\n",
    "    # 4) Collate function & (optional) Moirai cache for baseline mode\n",
    "    mode = str(getattr(cfg.model, \"mode\", \"baseline\")).lower()\n",
    "    if mode == \"baseline\":\n",
    "        # create Moirai cache for train+val to avoid misses during training\n",
    "        si_train = si.get(\"train\", pd.DataFrame())\n",
    "        si_val   = si.get(\"val\",   pd.DataFrame())\n",
    "        si_tv    = pd.concat([si_train, si_val], ignore_index=True) if not si_train.empty or not si_val.empty else pd.DataFrame(columns=[\"t_start\",\"zone_id\",\"L\",\"H\",\"occ_only\",\"split\"])\n",
    "        keys_tv  = make_moirai_keys(si_tv, hashes, stats, cfg) if not si_tv.empty else pd.DataFrame(columns=KEY_COLS)\n",
    "\n",
    "        moirai_df, moirai_emb, emb_dim = ensure_moirai_cache(keys_tv, tables, cfg, hashes, stats)\n",
    "        cfg.model.D_moirai = int(emb_dim)\n",
    "\n",
    "        # build lookup index for cached embeddings\n",
    "        key2row = {}\n",
    "        for i, (_, r) in enumerate(moirai_df.iterrows()):\n",
    "            key2row[tuple(r[KEY_COLS])] = i\n",
    "\n",
    "        collate_fn = collate_fn_builder_baseline(moirai_emb, key2row, hashes, stats)\n",
    "    else:\n",
    "        collate_fn = collate_fn_builder_finetune()\n",
    "\n",
    "    # 5) Datasets & Loaders (pass stats to keep fb_static normalization consistent)\n",
    "    train_df = si.get(\"train\", pd.DataFrame())\n",
    "    val_df   = si.get(\"val\",   pd.DataFrame())\n",
    "    train_ds = EVDataset(train_df, tables, zone_ids, cfg, stats=stats)\n",
    "    val_ds   = EVDataset(val_df,   tables, zone_ids, cfg, stats=stats)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=cfg.train.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=cfg.train.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # 6) Model & optimizer\n",
    "    model = EVForecastModel(cfg).to(DEVICE)\n",
    "    opt_name = str(getattr(cfg.train, \"optimizer\", \"adamw\")).lower()\n",
    "    if opt_name == \"adamw\":\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.train.lr, weight_decay=cfg.train.weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=cfg.train.lr, weight_decay=cfg.train.weight_decay)\n",
    "\n",
    "    # 7) Scheduler (inline)\n",
    "    scheduler = None\n",
    "    sched_per_batch = False\n",
    "    sched_name = str(getattr(cfg.train, \"scheduler\", \"none\") or \"none\").lower()\n",
    "    if sched_name == \"onecycle\":\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=cfg.train.lr,\n",
    "            epochs=cfg.train.epochs,\n",
    "            steps_per_epoch=max(1, len(train_loader))\n",
    "        )\n",
    "        sched_per_batch = True\n",
    "    elif sched_name in (\"steplr\", \"step\"):\n",
    "        step  = getattr(cfg.train, \"lr_step\", None)\n",
    "        gamma = getattr(cfg.train, \"lr_gamma\", None)\n",
    "        if step and gamma:\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step, gamma=gamma)\n",
    "        else:\n",
    "            print(\"[warn] StepLR needs cfg.train.lr_step & lr_gamma ‚Äî skipping scheduler.\")\n",
    "\n",
    "    # 8) Full resume when exec.new_model_train == False\n",
    "    resume_path = None\n",
    "    if not bool(getattr(cfg.exec, \"new_model_train\", True)):\n",
    "        latest_num, latest_path = -1, None\n",
    "        if os.path.isdir(cfg.out.ckpt_dir):\n",
    "            for fn in os.listdir(cfg.out.ckpt_dir):\n",
    "                if fn.startswith(\"epoch_\") and (fn.endswith(\".ckpt\") or fn.endswith(\".pt\")):\n",
    "                    try:\n",
    "                        n = int(fn.split(\"_\")[1].split(\".\")[0])\n",
    "                        if n > latest_num:\n",
    "                            latest_num, latest_path = n, os.path.join(cfg.out.ckpt_dir, fn)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        if latest_path:\n",
    "            resume_path = latest_path\n",
    "            print(f\"‚ñ∂Ô∏è Full resume from: {resume_path}\")\n",
    "            ckpt = torch.load(resume_path, map_location=DEVICE)\n",
    "            model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "            if \"optimizer_state_dict\" in ckpt:\n",
    "                optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "            if scheduler is not None and ckpt.get(\"scheduler_state_dict\") is not None:\n",
    "                try:\n",
    "                    scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
    "                except Exception:\n",
    "                    print(\"[warn] scheduler state incompatible; ignoring\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è new_model_train=False Ïù¥ÏßÄÎßå Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Í∞Ä ÏóÜÏñ¥ ÏÉàÎ°ú ÏãúÏûëÌï©ÎãàÎã§.\")\n",
    "\n",
    "    # 9) Train (best by val loss)\n",
    "    history = train(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=torch.nn.functional.mse_loss,\n",
    "        optimizer=optimizer,\n",
    "        save_best_path=cfg.out.best_ckpt,\n",
    "        config=cfg,\n",
    "        resume_path=resume_path,\n",
    "        scheduler=scheduler,\n",
    "        sched_per_batch=sched_per_batch\n",
    "    )\n",
    "\n",
    "    # 10) Return prepared bundle for reuse\n",
    "    return {\n",
    "        \"history\": history,\n",
    "        \"clock\": clock,\n",
    "        \"tables\": tables,\n",
    "        \"zone_ids\": zone_ids,\n",
    "        \"hashes\": hashes,\n",
    "        \"stats\": stats,\n",
    "        \"si\": si\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def run_test(cfg, prepared=None, mode=\"best\", ckpt_path=None, batch_size=None, drop_last=False, num_workers=0):\n",
    "    \"\"\"\n",
    "    mode:\n",
    "      - \"best\":  cfg.out.best_ckpt Î°úÎìú\n",
    "      - \"last\":  cfg.out.last_ckpt Î°úÎìú(ÏóÜÏúºÎ©¥ ckpt_dirÏùò ÏµúÎåÄ epoch_* ÏûêÎèôÌÉêÏÉâ)\n",
    "      - \"path\":  Ïù∏ÏûêÎ°ú Ï£ºÎäî ckpt_path Î°úÎìú\n",
    "    \"\"\"\n",
    "    # ---------- 0) sanity ----------\n",
    "    mode = str(mode).lower()\n",
    "    assert mode in (\"best\",\"last\",\"path\"), \"mode must be one of {'best','last','path'}\"\n",
    "    if mode == \"path\":\n",
    "        assert ckpt_path and os.path.exists(ckpt_path), f\"ckpt_path not found: {ckpt_path}\"\n",
    "\n",
    "    # ---------- 1) Îç∞Ïù¥ÌÑ∞/Ï†ÑÏ≤òÎ¶¨ Ï§ÄÎπÑ ----------\n",
    "    if prepared is not None:\n",
    "        clock  = prepared[\"clock\"]\n",
    "        tables = prepared[\"tables\"]\n",
    "        zone_ids = prepared[\"zone_ids\"]\n",
    "        hashes = prepared[\"hashes\"]\n",
    "        stats  = prepared[\"stats\"]\n",
    "        si     = prepared[\"si\"]\n",
    "    else:\n",
    "        clock, tables, zone_ids, hashes = prepare_data(cfg)\n",
    "\n",
    "        # (ÏòµÏÖò) POI Ï∫êÏãú/Í≥ÑÏÇ∞\n",
    "        if getattr(cfg.data, \"use_poi\", False) and os.path.exists(cfg.data.paths.get(\"poi\",\"\")):\n",
    "            try:\n",
    "                ensure_poi_counts(tables, cfg)\n",
    "            except ModuleNotFoundError:\n",
    "                print(\"[warn] scikit-learn not installed; skip POI\"); cfg.data.use_poi = False\n",
    "\n",
    "        # ÏÉòÌîå Ïù∏Îç±Ïä§ + ÌëúÏ§ÄÌôî\n",
    "        si, stats = build_sample_indices(tables, clock, zone_ids, cfg)\n",
    "\n",
    "    # ÌÖåÏä§Ìä∏ Î∂ÑÌï† Ï≤¥ÌÅ¨\n",
    "    if \"test\" not in si or si[\"test\"].empty:\n",
    "        raise RuntimeError(\"[run_test] no test samples found in si['test'].\")\n",
    "\n",
    "    # ---------- 2) Collate & (baselineÏùº Îïå) Moirai Ï∫êÏãú ----------\n",
    "    model_mode = str(getattr(cfg.model, \"mode\", \"baseline\")).lower()\n",
    "    if model_mode == \"baseline\":\n",
    "        keys_te = make_moirai_keys(si[\"test\"], hashes, stats, cfg)\n",
    "        moirai_df, moirai_emb, emb_dim = ensure_moirai_cache(keys_te, tables, cfg, hashes, stats)\n",
    "        cfg.model.D_moirai = int(emb_dim)\n",
    "\n",
    "        key2row = {}\n",
    "        for i, (_, r) in enumerate(moirai_df.iterrows()):\n",
    "            key2row[tuple(r[KEY_COLS])] = i\n",
    "\n",
    "        collate_fn = collate_fn_builder_baseline(moirai_emb, key2row, hashes, stats)\n",
    "    else:\n",
    "        collate_fn = collate_fn_builder_finetune()\n",
    "\n",
    "    # ---------- 3) Test Loader ----------\n",
    "    bs = int(batch_size or cfg.train.batch_size)\n",
    "    test_ds = EVDataset(si[\"test\"], tables, zone_ids, cfg, stats=stats)\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=bs,\n",
    "        shuffle=False,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # ---------- 4) Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏÑ†ÌÉù ----------\n",
    "    load_path = None\n",
    "    if mode == \"best\":\n",
    "        load_path = cfg.out.best_ckpt\n",
    "        if not os.path.exists(load_path):\n",
    "            raise FileNotFoundError(f\"[run_test] best_ckpt not found: {load_path}\")\n",
    "    elif mode == \"last\":\n",
    "        lp = cfg.out.last_ckpt\n",
    "        if os.path.exists(lp):\n",
    "            load_path = lp\n",
    "        else:\n",
    "            # ckpt_dirÏóêÏÑú Í∞ÄÏû• ÌÅ∞ epoch_* ÌÉêÏÉâ\n",
    "            ckpt_dir = cfg.out.ckpt_dir\n",
    "            best_n, best_path = -1, None\n",
    "            if os.path.isdir(ckpt_dir):\n",
    "                for fn in os.listdir(ckpt_dir):\n",
    "                    if fn.startswith(\"epoch_\") and (fn.endswith(\".ckpt\") or fn.endswith(\".pt\")):\n",
    "                        try:\n",
    "                            n = int(fn.split(\"_\")[1].split(\".\")[0])\n",
    "                            if n > best_n:\n",
    "                                best_n, best_path = n, os.path.join(ckpt_dir, fn)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "            if best_path is None:\n",
    "                raise FileNotFoundError(\"[run_test] no last checkpoint found (last_ckpt missing and no epoch_*.ckpt).\")\n",
    "            load_path = best_path\n",
    "    else:  # mode == \"path\"\n",
    "        load_path = ckpt_path\n",
    "\n",
    "    # ---------- 5) Î™®Îç∏ Î°úÎìú ----------\n",
    "    model = EVForecastModel(cfg).to(DEVICE)\n",
    "    ckpt = torch.load(load_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    # ---------- 6) ÌèâÍ∞Ä ----------\n",
    "    # test()Îäî masked MSEÎ°ú loss Í≥ÑÏÇ∞ + compute_metricsÎ°ú 4Ï¢Ö ÏßÄÌëú Î¶¨ÌÑ¥\n",
    "    result = test(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        criterion=torch.nn.functional.mse_loss,\n",
    "        metrics=tuple(getattr(cfg.train, \"metrics\", [\"RMSE\",\"MAPE\",\"RAE\",\"MAE\"]))\n",
    "    )\n",
    "\n",
    "    # Î©îÌÉÄÏ†ïÎ≥¥ Ï∂îÍ∞Ä\n",
    "    out = {\n",
    "        \"mode\": mode,\n",
    "        \"ckpt_path\": load_path,\n",
    "        \"n_test_samples\": len(test_ds),\n",
    "        \"batch_size\": bs,\n",
    "        **result,\n",
    "    }\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = SimpleNamespace(\n",
    "    exec = SimpleNamespace(\n",
    "        new_model_train = True,\n",
    "        poi_shared_dir = \"poi_cache_global\",\n",
    "        moirai_shared_dir = \"moirai_cache_global\",\n",
    "    ),\n",
    "    data = SimpleNamespace(\n",
    "        L = 24, H = 1, pred_offset=3,\n",
    "        baseline_occ_only = True,\n",
    "        moirai_channel = [\"occ\"], # occ, dur, etc...\n",
    "        train_range = (\"2022-09-01\", \"2022-12-31\"),\n",
    "        val_range   = (\"2023-01-01\", \"2023-01-31\"),\n",
    "        test_range  = (\"2023-02-01\", \"2023-02-28\"),\n",
    "        paths = PATHS,\n",
    "        use_poi = True,\n",
    "        poi_radius_beta = 0.7, poi_rmin = 300.0, poi_rmax = 2000.0,\n",
    "        sample_stride = 1, min_spatial_neighbors = 1,\n",
    "        spatial_usekeys = ('occ','dur','vol')\n",
    "    ),\n",
    "    moirai = SimpleNamespace(\n",
    "        model_id = \"Salesforce/moirai-1.0-R-base\",\n",
    "        patch_sizes = [1],\n",
    "        pool = \"mean\",\n",
    "        emb_dim = 768,\n",
    "        batch_size = 64,\n",
    "        cache_file = \"moirai_cache.pt\",   # Ïã§Ï†ú Ï†ÄÏû•ÏùÄ out.embed_dir ÏïÑÎûò\n",
    "    ),\n",
    "    model = SimpleNamespace(\n",
    "        D_moirai = 768,     # ‚Üê Ïã§Ï†ú ÏûÑÎ≤†Îî© Ï∞®ÏõêÏúºÎ°ú Îü∞ÌÉÄÏûÑÏóê Í∞±Ïã†Îê®\n",
    "        D_model  = 256,\n",
    "        dropout  = 0.1,\n",
    "        nhead    = 8,\n",
    "        head_hidden = 512,\n",
    "        nonneg_head = False,  # ÌëúÏ§ÄÌôî ÌÉÄÍπÉ ‚Üí ÏùåÏàò ÌóàÏö©\n",
    "        mode = \"baseline\",\n",
    "        HIDDEN = 64, KERNEL_SIZE = 5,\n",
    "        fusion_layers = 1,\n",
    "        head_kind = \"linear\",\n",
    "    ),\n",
    "    train = SimpleNamespace(\n",
    "        epochs = 2, batch_size = 64,\n",
    "        lr = 1e-3, weight_decay = 1e-4,\n",
    "        optimizer = \"adamw\",\n",
    "        scheduler = None ,#\"onecycle\",\n",
    "        clip_grad = 1.0,\n",
    "        early_stop_patience = 5,\n",
    "        metrics = [\"RMSE\",\"MAPE\",\"RAE\", \"MAE\", ],\n",
    "    ),\n",
    "    out = SimpleNamespace(\n",
    "        run_id = \"baseline_occOnly\",\n",
    "        run_dir = \"runs/baseline_occOnly\",\n",
    "        history_dir = \"runs/baseline_occOnly/history\",\n",
    "        ckpt_dir = \"runs/baseline_occOnly/checkpoints\",\n",
    "        embed_dir = \"runs/baseline_occOnly/embeddings\",\n",
    "        artifacts_dir = \"runs/baseline_occOnly/artifacts\",\n",
    "        config_dump = \"runs/baseline_occOnly/config.yaml\",\n",
    "        train_log_csv = \"runs/baseline_occOnly/history/train_log.csv\",\n",
    "        summary_json = \"runs/baseline_occOnly/history/summary.json\",\n",
    "        best_ckpt = \"runs/baseline_occOnly/checkpoints/best.ckpt\",\n",
    "        last_ckpt = \"runs/baseline_occOnly/checkpoints/last.ckpt\",\n",
    "        epoch_ckpt_tmpl = \"runs/baseline_occOnly/checkpoints/epoch_{:04d}.ckpt\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "for d in [cfg.out.run_dir, cfg.out.history_dir, cfg.out.ckpt_dir, cfg.out.embed_dir, cfg.out.artifacts_dir]:\n",
    "    os.makedirs(d, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[norm_version] 2635e1472fea4ea7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MoiraiCache] encode:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 8662/15581 [2:23:45<3:38:34,  1.90s/it]"
     ]
    }
   ],
   "source": [
    "prepared = run_train(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3831592/2375273881.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  moirai_df, moirai_emb = torch.load(cache_file, map_location=\"cpu\")\n",
      "[MoiraiCache] encode: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 108/108 [00:16<00:00,  6.54it/s]\n",
      "/tmp/ipykernel_3831592/3144376219.py:101: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(load_path, map_location=DEVICE)\n",
      "test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 108/108 [00:17<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: RMSE:0.3231 | MAPE:111.6814 | RAE:0.3321 | MAE:0.2017 | Loss:0.1117\n"
     ]
    }
   ],
   "source": [
    "# 1) Î≤†Ïä§Ìä∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î°ú ÌÖåÏä§Ìä∏\n",
    "res_best = run_test(cfg, prepared=prepared, mode=\"best\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) ÎùºÏä§Ìä∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î°ú ÌÖåÏä§Ìä∏\n",
    "res_last = run_test(cfg, prepared=prepared_bundle, mode=\"last\")\n",
    "\n",
    "# 3) ÌäπÏ†ï ÏóêÌè¨ÌÅ¨ Í≤ΩÎ°úÎ°ú ÌÖåÏä§Ìä∏\n",
    "res_ep2 = run_test(cfg, prepared=prepared_bundle, mode=\"path\",\n",
    "                   ckpt_path=\"runs/baseline_occOnly/checkpoints/epoch_0002.ckpt\")\n",
    "\n",
    "# 1) Best Î™®Îç∏Î°ú ÌÖåÏä§Ìä∏\n",
    "res_best = run_eval(cfg,\n",
    "                    prepared[\"si\"], prepared[\"tables\"], prepared[\"zone_ids\"],\n",
    "                    prepared[\"hashes\"], prepared[\"stats\"],\n",
    "                    which=\"best\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Last Î™®Îç∏Î°ú ÌÖåÏä§Ìä∏\n",
    "res_last = run_eval(cfg,\n",
    "                    prepared[\"si\"], prepared[\"tables\"], prepared[\"zone_ids\"],\n",
    "                    prepared[\"hashes\"], prepared[\"stats\"],\n",
    "                    which=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) ÏûÑÏùò ÏóêÌè¨ÌÅ¨ Í≤ΩÎ°úÎ°ú ÌÖåÏä§Ìä∏ (Ïòà: epoch_0002)\n",
    "some_ckpt = cfg.out.epoch_ckpt_tmpl.format(2)\n",
    "if os.path.exists(some_ckpt):\n",
    "    res_path = run_eval(cfg,\n",
    "                        prepared[\"si\"], prepared[\"tables\"], prepared[\"zone_ids\"],\n",
    "                        prepared[\"hashes\"], prepared[\"stats\"],\n",
    "                        which=\"path\", path_override=some_ckpt)\n",
    "else:\n",
    "    print(f\"[Info] {some_ckpt} not found; skip path-based eval.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
