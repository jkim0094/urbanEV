{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import random, math, json, hashlib\n",
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from contextlib import nullcontext\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import amp\n",
    "from tqdm.auto import tqdm\n",
    "import chinese_calendar as cc\n",
    "import warnings\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "from uni2ts.model.moirai import MoiraiModule\n",
    "\n",
    "\n",
    "# Device / Seed\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "SEED = 0\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "# ê²½ë¡œ & ì„¤ì •\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "DATA_DIR = ROOT / \"UrbanEV\" / \"data\"\n",
    "PATHS = {\n",
    "    \"occ\": str(DATA_DIR / \"occupancy.csv\"),\n",
    "    \"dur\": str(DATA_DIR / \"duration.csv\"),\n",
    "    \"vol\": str(DATA_DIR / \"volume.csv\"),\n",
    "    \"e_price\": str(DATA_DIR / \"e_price.csv\"),\n",
    "    \"s_price\": str(DATA_DIR / \"s_price.csv\"),\n",
    "    \"weather\": str(DATA_DIR / \"weather_central.csv\"),\n",
    "    \"inf\": str(DATA_DIR / \"inf.csv\"),\n",
    "    \"adj\": str(DATA_DIR / \"adj.csv\"),\n",
    "    \"dist\": str(DATA_DIR / \"distance.csv\"),\n",
    "    \"poi\": str(DATA_DIR / \"poi.csv\"),\n",
    "}\n",
    "\n",
    "# Helpers\n",
    "def build_clock(train_range, test_range):\n",
    "    start = pd.to_datetime(train_range[0])\n",
    "    end   = pd.to_datetime(test_range[1]) + pd.Timedelta(hours=23)\n",
    "    return pd.date_range(start=start, end=end, freq=\"h\")\n",
    "\n",
    "def read_ts_csv(path):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    return df.sort_index().apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "def reindex_local(df, clock):  return df.reindex(clock).fillna(0)\n",
    "def reindex_price(df, clock):  return df.reindex(clock).ffill().bfill()\n",
    "\n",
    "def reindex_weather(df, clock):\n",
    "    out = df.reindex(clock)\n",
    "    cont_cols = [c for c in out.columns if c.lower() not in (\"nrain\",\"rain\",\"n_rain\")]\n",
    "    rain_cols = [c for c in out.columns if c.lower() in (\"nrain\",\"rain\",\"n_rain\")]\n",
    "    if cont_cols: out[cont_cols] = out[cont_cols].interpolate(\"time\").ffill().bfill()\n",
    "    if rain_cols: out[rain_cols] = out[rain_cols].ffill().bfill().clip(lower=0).astype(int)\n",
    "    return out\n",
    "\n",
    "def load_inf(path):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    cols = [\"longitude\",\"latitude\",\"charge_count\",\"area\",\"perimeter\"]\n",
    "    for c in cols: df[c] = pd.to_numeric(df.get(c, np.nan), errors=\"coerce\")\n",
    "    df.index = df.index.astype(str)\n",
    "    return df[cols]\n",
    "\n",
    "def load_matrix(path, as_float=False):\n",
    "    df = pd.read_csv(path)\n",
    "    df.index = df.columns.astype(str)\n",
    "    df.columns = df.columns.astype(str)\n",
    "    if df.index.duplicated().any():\n",
    "        print(f\"[warn] duplicated index in {path}, keeping first\")\n",
    "        df = df.loc[~df.index.duplicated(keep='first')]\n",
    "    if df.columns.duplicated().any():\n",
    "        print(f\"[warn] duplicated columns in {path}, keeping first\")\n",
    "        df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
    "    df = df.astype(float if as_float else int)\n",
    "    np.fill_diagonal(df.values, 0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def precompute_time(clock):\n",
    "    hrs  = clock.hour; dows = clock.dayofweek\n",
    "    hour_sin = np.sin(2*np.pi*hrs/24).astype(np.float32)\n",
    "    hour_cos = np.cos(2*np.pi*hrs/24).astype(np.float32)\n",
    "    dow_sin  = np.sin(2*np.pi*dows/7).astype(np.float32)\n",
    "    dow_cos  = np.cos(2*np.pi*dows/7).astype(np.float32)\n",
    "\n",
    "    # ì£¼ë§ ë˜ëŠ” ì¤‘êµ­ ê³µíœ´ì¼ì´ë©´ 1\n",
    "    is_offday = np.array([\n",
    "        float((d.weekday() in [5,6]) or cc.is_holiday(d.date()))\n",
    "        for d in clock\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"hour_sin\":hour_sin,\"hour_cos\":hour_cos,\n",
    "        \"dow_sin\":dow_sin,\"dow_cos\":dow_cos,\n",
    "        \"is_offday\":is_offday\n",
    "    }, index=clock)\n",
    "\n",
    "\n",
    "def prepare_data(cfg):\n",
    "    clock = build_clock(cfg.data.train_range, cfg.data.test_range)\n",
    "    occ = reindex_local(read_ts_csv(cfg.data.paths[\"occ\"]), clock)\n",
    "    dur = reindex_local(read_ts_csv(cfg.data.paths[\"dur\"]), clock)\n",
    "    vol = reindex_local(read_ts_csv(cfg.data.paths[\"vol\"]), clock)\n",
    "    epr = reindex_price(read_ts_csv(cfg.data.paths[\"e_price\"]), clock)\n",
    "    spr = reindex_price(read_ts_csv(cfg.data.paths[\"s_price\"]), clock)\n",
    "    wth = reindex_weather(read_ts_csv(cfg.data.paths[\"weather\"]), clock)\n",
    "    inf = load_inf(cfg.data.paths[\"inf\"])\n",
    "    adj = load_matrix(cfg.data.paths[\"adj\"])\n",
    "    dist= load_matrix(cfg.data.paths[\"dist\"], as_float=True)\n",
    "\n",
    "    tables = {\"occ\":occ,\"dur\":dur,\"vol\":vol,\"e_price\":epr,\"s_price\":spr,\n",
    "              \"weather\":wth,\"inf\":inf,\"adj\":adj,\"distance\":dist,\"time\":precompute_time(clock)}\n",
    "\n",
    "    if cfg.data.use_poi and os.path.exists(cfg.data.paths[\"poi\"]):\n",
    "        tables[\"poi_raw\"] = pd.read_csv(cfg.data.paths[\"poi\"])\n",
    "\n",
    "    zone_ids = sorted(list(set(occ.columns) & set(epr.columns) & set(spr.columns)))\n",
    "    if len(zone_ids) != len(set(zone_ids)): raise ValueError(\"zone_ids duplicated\")\n",
    "\n",
    "    for k in [\"occ\",\"dur\",\"vol\",\"e_price\",\"s_price\"]:\n",
    "        tables[k] = tables[k][zone_ids]\n",
    "    tables[\"adj\"]      = tables[\"adj\"].reindex(index=zone_ids, columns=zone_ids, fill_value=0)\n",
    "    tables[\"distance\"] = tables[\"distance\"].reindex(index=zone_ids, columns=zone_ids, fill_value=float('inf'))\n",
    "    tables[\"inf\"]      = tables[\"inf\"].reindex(index=zone_ids)\n",
    "\n",
    "    datahash  = hashlib.sha256(str(cfg.data.paths).encode()).hexdigest()[:16]\n",
    "    clockhash = hashlib.sha256(str(clock).encode()).hexdigest()[:16]\n",
    "    hashes = SimpleNamespace(datahash=datahash, clockhash=clockhash)\n",
    "    return clock, tables, zone_ids, hashes\n",
    "\n",
    "def compute_fit_stats(tables, train_times, zone_ids):\n",
    "    \"\"\"\n",
    "    í•™ìŠµ(train) êµ¬ê°„ì—ì„œ í‘œì¤€í™”ì— ì“¸ í†µê³„ì¹˜ ê³„ì‚°.\n",
    "    - z-score ëŒ€ìƒ: occ, dur, e_price, s_price, weather(T,P0,P,U,Td)\n",
    "    - log1p+z ëŒ€ìƒ(í†µê³„ë§Œ): vol, inf(charge_count/area/perimeter), poi_counts[*]\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "\n",
    "    # 1) ì‹œê³„ì—´: z-score\n",
    "    for key in [\"occ\", \"dur\", \"e_price\", \"s_price\"]:\n",
    "        if key in tables:\n",
    "            df = tables[key].loc[train_times, zone_ids]\n",
    "            mu = float(np.nanmean(df.values))\n",
    "            sd = float(np.nanstd(df.values)) or 1.0\n",
    "            stats[key] = {\"mean\": mu, \"std\": sd}\n",
    "\n",
    "    # 2) ì‹œê³„ì—´: log1p + z-score (vol)\n",
    "    if \"vol\" in tables:\n",
    "        df = tables[\"vol\"].loc[train_times, zone_ids]\n",
    "        vals = np.log1p(df.values.astype(float))\n",
    "        mu = float(np.nanmean(vals))\n",
    "        sd = float(np.nanstd(vals)) or 1.0\n",
    "        stats[\"log_vol\"] = {\"mean\": mu, \"std\": sd}\n",
    "\n",
    "    # 3) ë‚ ì”¨ ì—°ì†ê°’: z-score  (rain ê³„ì—´ ì œì™¸)\n",
    "    if \"weather\" in tables:\n",
    "        wdf = tables[\"weather\"].loc[train_times]\n",
    "        for c in [\"T\", \"P0\", \"P\", \"U\", \"Td\"]:\n",
    "            if c in wdf.columns:\n",
    "                col = pd.to_numeric(wdf[c], errors=\"coerce\").values\n",
    "                mu = float(np.nanmean(col))\n",
    "                sd = float(np.nanstd(col)) or 1.0\n",
    "                stats[c] = {\"mean\": mu, \"std\": sd}\n",
    "            else:\n",
    "                stats[c] = {\"mean\": 0.0, \"std\": 1.0}\n",
    "\n",
    "    # 4) ì •ì : log1p + z-score (ì›ë³¸ì€ ìœ ì§€í•  ê²ƒì´ë¯€ë¡œ í†µê³„ë§Œ)\n",
    "    if \"inf\" in tables:\n",
    "        inf = tables[\"inf\"]\n",
    "        for c in [\"charge_count\", \"area\", \"perimeter\"]:\n",
    "            if c in inf.columns:\n",
    "                vals = np.log1p(pd.to_numeric(inf[c], errors=\"coerce\").values.astype(float))\n",
    "                mu = float(np.nanmean(vals))\n",
    "                sd = float(np.nanstd(vals)) or 1.0\n",
    "                stats[f\"log_{c}\"] = {\"mean\": mu, \"std\": sd}\n",
    "\n",
    "    # 5) POI ì¹´ìš´íŠ¸: log1p + z-score (ìˆì„ ë•Œ)\n",
    "    if \"poi_counts\" in tables:\n",
    "        poi = tables[\"poi_counts\"]\n",
    "        for c in poi.columns:\n",
    "            vals = np.log1p(pd.to_numeric(poi[c], errors=\"coerce\").values.astype(float))\n",
    "            mu = float(np.nanmean(vals))\n",
    "            sd = float(np.nanstd(vals)) or 1.0\n",
    "            stats[f\"log_{c}\"] = {\"mean\": mu, \"std\": sd}\n",
    "\n",
    "    # ë²„ì „ íƒœê¹…\n",
    "    stats_json = json.dumps(stats, sort_keys=True)\n",
    "    stats[\"norm_version\"] = hashlib.sha256(stats_json.encode()).hexdigest()[:16]\n",
    "    return stats\n",
    "\n",
    "\n",
    "def standardize_tables(tables, stats):\n",
    "    \"\"\"\n",
    "    tablesë¥¼ ì œìë¦¬(in-place)ì—ì„œ í‘œì¤€í™”.\n",
    "    - z-score: occ, dur, e_price, s_price, weather(T,P0,P,U,Td)\n",
    "    - log1p+z: vol, poi_counts[*]\n",
    "    - infëŠ” **ì›ë³¸ ìœ ì§€**(area/perimeter ì›ê°’ í•„ìš”). í•„ìš” ì‹œ fb_static ë“±ì—ì„œ on-the-flyë¡œ ë³€í™˜í•˜ì„¸ìš”.\n",
    "    \"\"\"\n",
    "    eps = 1e-8\n",
    "\n",
    "    # 1) ì‹œê³„ì—´: z-score\n",
    "    for key in [\"occ\", \"dur\", \"e_price\", \"s_price\"]:\n",
    "        if key in tables and key in stats:\n",
    "            mu = stats[key][\"mean\"]; sd = max(stats[key][\"std\"], eps)\n",
    "            tables[key] = ((tables[key] - mu) / sd).astype(np.float32)\n",
    "\n",
    "    # 2) ì‹œê³„ì—´: log1p + z-score (vol)\n",
    "    if \"vol\" in tables and \"log_vol\" in stats:\n",
    "        mu = stats[\"log_vol\"][\"mean\"]; sd = max(stats[\"log_vol\"][\"std\"], eps)\n",
    "        df = tables[\"vol\"].astype(float)\n",
    "        tables[\"vol\"] = ((np.log1p(df) - mu) / sd).astype(np.float32)\n",
    "\n",
    "    # 3) ë‚ ì”¨ ì—°ì†ê°’: z-score (rain ê³„ì—´ì€ ì†ëŒ€ì§€ ì•ŠìŒ)\n",
    "    if \"weather\" in tables:\n",
    "        w = tables[\"weather\"].copy()\n",
    "        for c in [\"T\", \"P0\", \"P\", \"U\", \"Td\"]:\n",
    "            if c in w.columns and c in stats:\n",
    "                mu = stats[c][\"mean\"]; sd = max(stats[c][\"std\"], eps)\n",
    "                w[c] = ((pd.to_numeric(w[c], errors=\"coerce\") - mu) / sd).astype(np.float32)\n",
    "        tables[\"weather\"] = w\n",
    "\n",
    "    # 4) POI ì¹´ìš´íŠ¸: log1p + z-score (ìˆì„ ë•Œ, ëª¨ë¸ ì…ë ¥ë§Œ ì“¸ ê±°ë¼ë©´ ë®ì–´ì¨ë„ OK)\n",
    "    if \"poi_counts\" in tables:\n",
    "        poi = tables[\"poi_counts\"].copy()\n",
    "        for c in poi.columns:\n",
    "            key = f\"log_{c}\"\n",
    "            if key in stats:\n",
    "                mu = stats[key][\"mean\"]; sd = max(stats[key][\"std\"], eps)\n",
    "                col = np.log1p(pd.to_numeric(poi[c], errors=\"coerce\").astype(float))\n",
    "                poi[c] = ((col - mu) / sd).astype(np.float32)\n",
    "        tables[\"poi_counts\"] = poi\n",
    "\n",
    "\n",
    "\n",
    "def build_sample_indices(tables, clock, zone_ids, cfg):\n",
    "    # ---- íŒŒë¼ë¯¸í„° ----\n",
    "    L = int(cfg.data.L)\n",
    "    H = int(cfg.data.H)\n",
    "    O = int(getattr(cfg.data, \"pred_offset\", 1))\n",
    "    zone_ids = list(zone_ids)\n",
    "\n",
    "    # ---- split ë¼ë²¨ë§ (ë-of-day ë³´ì • í¬í•¨) ----\n",
    "    split_sr = pd.Series(index=clock, data=\"none\")\n",
    "\n",
    "    def _eod(ts):\n",
    "        ts = pd.to_datetime(ts)\n",
    "        return ts + pd.Timedelta(hours=23) if (ts.hour==0 and ts.minute==0 and ts.second==0) else ts\n",
    "\n",
    "    tr_s, tr_e = pd.to_datetime(cfg.data.train_range[0]), _eod(cfg.data.train_range[1])\n",
    "    va_s, va_e = pd.to_datetime(cfg.data.val_range[0]),   _eod(cfg.data.val_range[1])\n",
    "    te_s, te_e = pd.to_datetime(cfg.data.test_range[0]),  _eod(cfg.data.test_range[1])\n",
    "\n",
    "    split_sr.loc[(split_sr.index>=tr_s)&(split_sr.index<=tr_e)] = \"train\"\n",
    "    split_sr.loc[(split_sr.index>=va_s)&(split_sr.index<=va_e)] = \"val\"\n",
    "    split_sr.loc[(split_sr.index>=te_s)&(split_sr.index<=te_e)] = \"test\"\n",
    "\n",
    "    # ---- í†µê³„/í‘œì¤€í™” ----\n",
    "    train_times = split_sr[split_sr==\"train\"].index\n",
    "    stats = compute_fit_stats(tables, train_times, zone_ids)\n",
    "    standardize_tables(tables, stats)\n",
    "\n",
    "    # ====== ìœ íš¨ì„± ë§ˆìŠ¤í¬ ======\n",
    "    occ_fin = pd.DataFrame(np.isfinite(tables[\"occ\"].values), index=clock, columns=zone_ids)\n",
    "    epr_fin = pd.DataFrame(np.isfinite(tables[\"e_price\"].values), index=clock, columns=zone_ids)\n",
    "    spr_fin = pd.DataFrame(np.isfinite(tables[\"s_price\"].values), index=clock, columns=zone_ids)\n",
    "\n",
    "    masks = [occ_fin, epr_fin, spr_fin]\n",
    "\n",
    "    # dur/volì„ cfgì— ë”°ë¼ ì¶”ê°€\n",
    "    if \"dur\" in getattr(cfg.data, \"moirai_channels\", []):\n",
    "        dur_fin = pd.DataFrame(np.isfinite(tables[\"dur\"].values), index=clock, columns=zone_ids)\n",
    "        masks.append(dur_fin)\n",
    "\n",
    "    if \"vol\" in getattr(cfg.data, \"moirai_channels\", []):\n",
    "        vol_fin = pd.DataFrame(np.isfinite(tables[\"vol\"].values), index=clock, columns=zone_ids)\n",
    "        masks.append(vol_fin)\n",
    "\n",
    "    # ì…ë ¥ì°½ L ì—°ì† ì¡°ê±´\n",
    "    input_ok = True\n",
    "    for m in masks:\n",
    "        input_ok = input_ok & (m.rolling(L, min_periods=L).sum().eq(L).shift(-(L-1), fill_value=False))\n",
    "\n",
    "    # ë‚ ì”¨ëŠ” zone ì¶•ì´ ì—†ìœ¼ë‹ˆ ì»¨í‹°ë‰´ì–´ìŠ¤ 5ê°œ ëª¨ë‘ finite â†’ L ì—°ì† í›„ ë¸Œë¡œë“œìºìŠ¤íŠ¸\n",
    "    w5 = tables[\"weather\"][[\"T\",\"P0\",\"P\",\"U\",\"Td\"]]\n",
    "    w5_fin = pd.Series(np.isfinite(w5.values).all(axis=1), index=clock)\n",
    "    w_ok_sr = w5_fin.rolling(L, min_periods=L).sum().eq(L).shift(-(L-1), fill_value=False)\n",
    "    w_ok = pd.DataFrame(np.repeat(w_ok_sr.values[:,None], len(zone_ids), axis=1), index=clock, columns=zone_ids)\n",
    "\n",
    "    # íƒ€ê¹ƒì°½ (t0+O ë¶€í„° Hê°œ ì—°ì† finite)\n",
    "    if H == 1:\n",
    "        tgt_ok = occ_fin.shift(-O, fill_value=False)\n",
    "    else:\n",
    "        tgt_ok = (occ_fin.shift(-O, fill_value=False)\n",
    "                          .rolling(H, min_periods=H).sum().eq(H)\n",
    "                          .shift(-(H-1), fill_value=False))\n",
    "\n",
    "    # ì¢…í•© ìœ íš¨ì„±\n",
    "    combined_ok = input_ok & w_ok & tgt_ok\n",
    "\n",
    "    # ====== ê²½ê³„(ìœˆë„ìš°) í¬í•¨ì„ ìˆ˜ì‹ìœ¼ë¡œ ê°•ì œ ======\n",
    "    # í—ˆìš© t0 ë²”ìœ„: [S, E - max(L-1, O+H-1)]\n",
    "    Lm1 = pd.Timedelta(hours=L-1)\n",
    "    Hm1 = pd.Timedelta(hours=H-1)\n",
    "    Off = pd.Timedelta(hours=O)\n",
    "\n",
    "    out = {}\n",
    "    for split, (S, E) in {\n",
    "        \"train\": (tr_s, tr_e),\n",
    "        \"val\"  : (va_s, va_e),\n",
    "        \"test\" : (te_s, te_e),\n",
    "    }.items():\n",
    "        upper = E - max(Lm1, Off + Hm1)  # t0 ìƒí•œ\n",
    "        # ì‹œê°„ ì¸ë±ìŠ¤ ê¸°ë°˜ ë¶€ìš¸ ì‹œë¦¬ì¦ˆ â†’ DataFrameìœ¼ë¡œ ë¸Œë¡œë“œìºìŠ¤íŠ¸\n",
    "        t0_ok_sr = (clock >= S) & (clock <= upper)                  # ndarray(bool)\n",
    "        t0_ok = pd.DataFrame(np.repeat(t0_ok_sr[:, None], len(zone_ids), axis=1),\n",
    "                            index=clock, columns=zone_ids)\n",
    "\n",
    "        split_ok_sr = (split_sr == split).to_numpy()  # shape (n,)\n",
    "        split_ok = pd.DataFrame(np.repeat(split_ok_sr[:, None], len(zone_ids), axis=1),\n",
    "                                index=clock, columns=zone_ids)\n",
    "\n",
    "        valid = combined_ok & t0_ok & split_ok\n",
    "        # í‰íƒ„í™”\n",
    "        flat = valid.stack().reset_index()\n",
    "        flat = flat[flat[0]].drop(columns=0)\n",
    "        flat.columns = [\"t_start\", \"zone_id\"]\n",
    "\n",
    "        # ë©”íƒ€\n",
    "        if flat.empty:\n",
    "            out[split] = flat.reset_index(drop=True)\n",
    "            continue\n",
    "\n",
    "        flat[\"zone_idx\"] = flat[\"zone_id\"].map({z:i for i,z in enumerate(zone_ids)})\n",
    "        flat[\"L\"] = L\n",
    "        flat[\"H\"] = H\n",
    "        flat[\"occ_only\"] = int(getattr(cfg.data, \"baseline_occ_only\", True))\n",
    "        flat[\"split\"] = split\n",
    "        out[split] = flat.reset_index(drop=True)\n",
    "\n",
    "    return out, stats\n",
    "\n",
    "\n",
    "# POI utilities \n",
    "def compute_zone_radius(inf, beta=0.7, r_min=300, r_max=2000):\n",
    "    A = inf[\"area\"].to_numpy(); P = inf[\"perimeter\"].to_numpy()\n",
    "    r_area  = np.sqrt(np.clip(A,0,None)/np.pi)\n",
    "    r_perim = P/(2*np.pi)\n",
    "    r = beta*r_area + (1-beta)*r_perim\n",
    "    return pd.Series(np.clip(r, r_min, r_max), index=inf.index)\n",
    "\n",
    "\n",
    "def build_poi_counts(inf, poi, radius_m):\n",
    "    # radians\n",
    "    z = np.deg2rad(np.c_[inf[\"latitude\"].values, inf[\"longitude\"].values])\n",
    "    p = np.deg2rad(np.c_[poi[\"latitude\"].values, poi[\"longitude\"].values])\n",
    "\n",
    "    # BallTree on POI\n",
    "    tree = BallTree(p, metric=\"haversine\")\n",
    "\n",
    "    # radius vector in radians\n",
    "    r_vec = (radius_m.values / 6371000.0).astype(np.float64)\n",
    "\n",
    "    # single batched query\n",
    "    idx_lists = tree.query_radius(z, r=r_vec)  # list of index arrays, len = n_zones\n",
    "\n",
    "    types = poi[\"primary_types\"].str.lower().values\n",
    "    is_life = np.char.find(types.astype(str), \"lifestyle services\") >= 0\n",
    "    is_bres = np.char.find(types.astype(str), \"business and residential\") >= 0\n",
    "    is_food = np.char.find(types.astype(str), \"food and beverage services\") >= 0\n",
    "\n",
    "    out = np.zeros((len(inf), 3), dtype=np.int32)\n",
    "    for i, idxs in enumerate(idx_lists):\n",
    "        if idxs.size == 0: \n",
    "            continue\n",
    "        out[i, 0] = int(is_life[idxs].sum())\n",
    "        out[i, 1] = int(is_bres[idxs].sum())\n",
    "        out[i, 2] = int(is_food[idxs].sum())\n",
    "\n",
    "    return pd.DataFrame(out, index=inf.index,\n",
    "        columns=[\"poi_lifestyle\",\"poi_business_residential\",\"poi_food_beverage\"])\n",
    "\n",
    "\n",
    "def ensure_poi_counts(tables, cfg):\n",
    "    \"\"\"\n",
    "    - ìºì‹œëœ poi_counts ìˆìœ¼ë©´ ë¡œë“œ\n",
    "    - ì—†ìœ¼ë©´ compute_zone_radius + build_poi_counts ëŒë ¤ ê³„ì‚° í›„ ì €ì¥\n",
    "    \"\"\"\n",
    "    # ìºì‹œ í´ë”\n",
    "    cache_dir = Path(getattr(cfg.exec, \"poi_shared_dir\", \"poi_cache_global\"))\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ìºì‹œ íŒŒì¼ ì´ë¦„ ë§Œë“¤ê¸° (ë°ì´í„°/íŒŒë¼ë¯¸í„° ê¸°ë°˜ í•´ì‹œ)\n",
    "    meta = {\n",
    "        \"beta\": cfg.data.poi_radius_beta,\n",
    "        \"rmin\": cfg.data.poi_rmin,\n",
    "        \"rmax\": cfg.data.poi_rmax,\n",
    "        # inf/poi ë‚´ìš©ë„ í¬í•¨ (ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ê¸¸ì´ì™€ í•´ì‹œë¡œ)\n",
    "        \"n_zones\": len(tables[\"inf\"]),\n",
    "        \"n_poi\": len(tables[\"poi_raw\"])\n",
    "    }\n",
    "    h = hashlib.sha256(json.dumps(meta, sort_keys=True).encode()).hexdigest()[:16]\n",
    "    cache_file = cache_dir / f\"poi_counts_{h}.parquet\"\n",
    "\n",
    "    # ì´ë¯¸ ìˆìœ¼ë©´ ë¡œë“œ\n",
    "    if cache_file.exists():\n",
    "        poi_counts = pd.read_parquet(cache_file)\n",
    "        tables[\"poi_counts\"] = poi_counts\n",
    "        return poi_counts\n",
    "\n",
    "    # ì—†ìœ¼ë©´ ìƒˆë¡œ ê³„ì‚°\n",
    "    radius = compute_zone_radius(\n",
    "        tables[\"inf\"],\n",
    "        beta=cfg.data.poi_radius_beta,\n",
    "        r_min=cfg.data.poi_rmin,\n",
    "        r_max=cfg.data.poi_rmax\n",
    "    )\n",
    "    poi_counts = build_poi_counts(tables[\"inf\"], tables[\"poi_raw\"], radius)\n",
    "\n",
    "    # ì €ì¥\n",
    "    poi_counts.to_parquet(cache_file)\n",
    "    tables[\"poi_counts\"] = poi_counts\n",
    "    return poi_counts\n",
    "\n",
    "\n",
    "# Moirai cache (keying fixed)\n",
    "KEY_COLS = [\"zone_id\",\"t_start_iso\",\"L\",\"occ_only\",\"c_sig\",\"model_id\",\"psig\",\"pool\",\"clock_hash\",\"norm_version\",\"datahash\"]\n",
    "\n",
    "\n",
    "def make_moirai_keys(si, hashes, stats, cfg):\n",
    "    df = si.copy()\n",
    "    df[\"t_start_iso\"] = df[\"t_start\"].dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    df[\"L\"] = df[\"L\"].astype(int)\n",
    "    df[\"occ_only\"] = df[\"occ_only\"].astype(int)\n",
    "    df[\"model_id\"] = cfg.moirai.model_id\n",
    "    df[\"psig\"] = \"-\".join(map(str, cfg.moirai.patch_sizes))\n",
    "    df[\"pool\"] = cfg.moirai.pool\n",
    "    df[\"clock_hash\"] = hashes.clockhash\n",
    "    df[\"norm_version\"] = stats[\"norm_version\"]\n",
    "    df[\"datahash\"] = hashes.datahash\n",
    "    chan = getattr(cfg.data, \"moirai_channels\", [\"occ\"])\n",
    "    df[\"c_sig\"] = \"+\".join(chan)  # ì˜ˆ: \"occ\" or \"occ+e_price+s_price\"\n",
    "    return df[KEY_COLS]\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_batch(x_np, backbone, patch_sizes, pool):\n",
    "    x = torch.from_numpy(x_np).to(DEVICE).float()  # (B, C, L)\n",
    "    B, C, L = x.shape\n",
    "    target = x.transpose(1,2)  # (B, L, C)\n",
    "    observed_mask = torch.ones_like(target, dtype=torch.bool)\n",
    "    prediction_mask = torch.zeros(B, L, dtype=torch.bool, device=DEVICE)\n",
    "    sample_id = torch.arange(B, device=DEVICE).unsqueeze(1).expand(B, L)\n",
    "    time_id   = torch.arange(L, device=DEVICE).unsqueeze(0).expand(B, L)\n",
    "    variate_id= torch.zeros(B, L, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    hs_list = []\n",
    "    for ps in patch_sizes:\n",
    "        patch_size = torch.full((B, L), ps, dtype=torch.long, device=DEVICE)\n",
    "        # âœ… CUDAì¼ ë•Œë§Œ ì¼œê¸°\n",
    "        use_amp = (DEVICE == \"cuda\")\n",
    "        with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
    "            try:\n",
    "                hs = backbone.encode(\n",
    "                    target=target, observed_mask=observed_mask, patch_size=patch_size\n",
    "                )\n",
    "            except AttributeError:\n",
    "                out = backbone(\n",
    "                    target=target, observed_mask=observed_mask,\n",
    "                    prediction_mask=prediction_mask, sample_id=sample_id,\n",
    "                    time_id=time_id, variate_id=variate_id, patch_size=patch_size\n",
    "                )\n",
    "                hs = out if isinstance(out, torch.Tensor) else getattr(out, \"mean\", out.sample())\n",
    "        if hs.dim()==3: hs = hs.mean(dim=1) if pool==\"mean\" else hs.max(dim=1)[0]\n",
    "        elif hs.dim()!=2: raise ValueError(f\"Unexpected hs shape: {hs.shape}\")\n",
    "        hs_list.append(hs)\n",
    "    emb = torch.stack(hs_list).mean(dim=0)  # (B, D)\n",
    "    return emb.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def _sanitize(s:str)->str:\n",
    "    return \"\".join(ch if (str(ch).isalnum() or ch in \"-._\") else \"_\" for ch in str(s))\n",
    "\n",
    "def _make_cache_path_with_dim(cfg, model_id, patch_sig, pool, emb_dim, hashes, stats):\n",
    "    c_sig = \"+\".join(getattr(cfg.data, \"moirai_channels\", [\"occ\"]))\n",
    "    sig = {\"model\":model_id,\"psig\":patch_sig,\"pool\":pool,\"edim\":str(emb_dim),\n",
    "           \"c\":c_sig, \n",
    "           \"clock\":hashes.clockhash[:8],\"norm\":stats[\"norm_version\"][:8],\"data\":hashes.datahash[:8]}\n",
    "    fname = \"moirai_\" + \"_\".join(f\"{k}-{_sanitize(v)}\" for k,v in sig.items()) + \".pt\"\n",
    "    base_dir = str(Path(getattr(cfg.exec, \"moirai_shared_dir\", \"\")))\n",
    "    return os.path.join(base_dir, fname)\n",
    "\n",
    "\n",
    "def ensure_moirai_cache(keys_df, tables, cfg, hashes, stats):\n",
    "    # ë¹ˆ í‚¤ ê°€ë“œ\n",
    "    if len(keys_df) == 0:\n",
    "        empty_df = pd.DataFrame(columns=keys_df.columns)\n",
    "        edim_default = int(getattr(cfg.moirai, \"emb_dim\", 0))\n",
    "        empty_tensor = torch.zeros(0, edim_default, dtype=torch.float32)\n",
    "        return empty_df, empty_tensor, edim_default\n",
    "\n",
    "    # ì‚¬ìš©í•  ì±„ë„ ê²°ì •\n",
    "    chan = getattr(cfg.data, \"moirai_channels\", None)\n",
    "    if chan is None:\n",
    "        chan = [\"occ\"] if bool(getattr(cfg.data, \"baseline_occ_only\", True)) else [\"occ\",\"e_price\",\"s_price\"]\n",
    "\n",
    "    # ê°„ë‹¨í•œ í—¬í¼: (C,L) ìŠ¤íƒ\n",
    "    def stack_channels(tables, z, t0, L, chan_names):\n",
    "        win = slice(t0, t0 + pd.Timedelta(hours=L-1))\n",
    "        arrs = []\n",
    "        for name in chan_names:\n",
    "            series = tables[name].loc[win, z].values.astype(np.float32)\n",
    "            arrs.append(series)  # (L,)\n",
    "        return np.stack(arrs, axis=0)  # (C, L)\n",
    "\n",
    "    # ë°±ë³¸ ì¤€ë¹„\n",
    "    backbone = MoiraiModule.from_pretrained(cfg.moirai.model_id).to(DEVICE).eval()\n",
    "    patch_sig = \"-\".join(map(str, cfg.moirai.patch_sizes))\n",
    "    pool = cfg.moirai.pool\n",
    "\n",
    "    # ---- ì„ë² ë”© ì°¨ì› í”„ë¡œë¹™ ----\n",
    "    first_row = keys_df.iloc[0]\n",
    "    z, t0 = first_row[\"zone_id\"], pd.to_datetime(first_row[\"t_start_iso\"])\n",
    "    x_probe = stack_channels(tables, z, t0, cfg.data.L, chan)[None, ...]  # (1,C,L)\n",
    "    probe_np = encode_batch(x_probe, backbone, cfg.moirai.patch_sizes, pool)  # (1,E)\n",
    "    emb_dim = int(probe_np.shape[1])\n",
    "\n",
    "    # ---- ìºì‹œ ê²½ë¡œ (í™•ì¥ì ë¶„ë¦¬: DF=parquet, EMB=.pt) ----\n",
    "    base = _make_cache_path_with_dim(cfg, cfg.moirai.model_id, patch_sig, pool, emb_dim, hashes, stats)\n",
    "    os.makedirs(os.path.dirname(base), exist_ok=True)\n",
    "    df_path  = base + \".parquet\"\n",
    "    emb_path = base + \".pt\"\n",
    "\n",
    "    # ---- ë¡œë“œ or ì´ˆê¸°í™” ----\n",
    "    if os.path.exists(df_path) and os.path.exists(emb_path):\n",
    "        moirai_df  = pd.read_parquet(df_path)\n",
    "        moirai_emb = torch.load(emb_path, map_location=\"cpu\") \n",
    "        if not isinstance(moirai_emb, torch.Tensor) or moirai_emb.ndim != 2 or moirai_emb.shape[1] != emb_dim:\n",
    "            print(f\"[MoiraiCache] dim mismatch in file: {getattr(moirai_emb,'shape',None)} vs emb_dim={emb_dim} â†’ reset\")\n",
    "            moirai_df  = pd.DataFrame(columns=keys_df.columns)\n",
    "            moirai_emb = torch.zeros(0, emb_dim, dtype=torch.float32)\n",
    "    else:\n",
    "        moirai_df  = pd.DataFrame(columns=keys_df.columns)\n",
    "        moirai_emb = torch.zeros(0, emb_dim, dtype=torch.float32)\n",
    "\n",
    "    # ---- ëˆ„ë½ í‚¤ë§Œ ì¸ì½”ë”© ----\n",
    "    key2row = {tuple(row[KEY_COLS]): i for i, (_, row) in enumerate(moirai_df.iterrows())}\n",
    "    todo_rows = [r for _, r in keys_df.iterrows() if tuple(r[KEY_COLS]) not in key2row]\n",
    "\n",
    "    if len(todo_rows) > 0:\n",
    "        B = int(getattr(cfg.moirai, \"batch_size\", 64))\n",
    "        for i in tqdm(range(0, len(todo_rows), B), desc=\"[MoiraiCache] encode\"):\n",
    "            batch_rows = todo_rows[i:i+B]\n",
    "            xs = []\n",
    "            for r in batch_rows:\n",
    "                z = r[\"zone_id\"]\n",
    "                t0 = pd.to_datetime(r[\"t_start_iso\"])\n",
    "                x_cl = stack_channels(tables, z, t0, cfg.data.L, chan)  # (C,L)\n",
    "                xs.append(x_cl)\n",
    "            x_np = np.stack(xs).astype(np.float32)  # (b, C, L)\n",
    "\n",
    "            y_np = encode_batch(x_np, backbone, cfg.moirai.patch_sizes, pool)  # (b, E)\n",
    "            new_emb = torch.from_numpy(y_np).float()   # CPU\n",
    "\n",
    "            # ëŸ°íƒ€ì„ ì°¨ì› ë¶ˆì¼ì¹˜ ëŒ€ì‘\n",
    "            if new_emb.shape[1] != moirai_emb.shape[1]:\n",
    "                print(f\"[MoiraiCache] runtime dim mismatch: cache={moirai_emb.shape[1]} vs new={new_emb.shape[1]} â†’ reset\")\n",
    "                moirai_df  = pd.DataFrame(columns=keys_df.columns)\n",
    "                moirai_emb = torch.zeros(0, new_emb.shape[1], dtype=torch.float32)\n",
    "\n",
    "            batch_df = pd.DataFrame(batch_rows, columns=keys_df.columns)\n",
    "            moirai_df  = pd.concat([moirai_df, batch_df], ignore_index=True)\n",
    "            moirai_emb = torch.cat([moirai_emb, new_emb], dim=0)\n",
    "\n",
    "            # ---- ì›ìì  ì €ì¥ (tmp â†’ replace) ----\n",
    "            tmp_df  = df_path  + \".tmp\"\n",
    "            tmp_pt  = emb_path + \".tmp\"\n",
    "            moirai_df.to_parquet(tmp_df)\n",
    "            torch.save(moirai_emb, tmp_pt)\n",
    "            os.replace(tmp_df, df_path)\n",
    "            os.replace(tmp_pt, emb_path)\n",
    "\n",
    "    return moirai_df, moirai_emb, emb_dim\n",
    "\n",
    "\n",
    "\n",
    "# Feature builders\n",
    "def fb_local(tables, z, t0, L, occ_only=True):\n",
    "    win = slice(t0, t0 + pd.Timedelta(hours=L-1))\n",
    "    occ = tables[\"occ\"].loc[win, z].values.astype(np.float32)\n",
    "    return occ[None, :]  # (1, L)\n",
    "\n",
    "def fb_price(tables, z, t0, L):\n",
    "    win = slice(t0, t0 + pd.Timedelta(hours=L-1))\n",
    "    epr = tables[\"e_price\"].loc[win, z].values.astype(np.float32)\n",
    "    spr = tables[\"s_price\"].loc[win, z].values.astype(np.float32)\n",
    "    return np.stack([epr, spr]).astype(np.float32)\n",
    "\n",
    "def fb_weather(tables, t0, L):\n",
    "    win = slice(t0, t0 + pd.Timedelta(hours=L-1))\n",
    "    w = tables[\"weather\"].loc[win]\n",
    "    cont = w[[\"T\",\"P0\",\"P\",\"U\",\"Td\"]].values.T.astype(np.float32)\n",
    "    rain = w.get(\"nRAIN\", pd.Series(0, index=w.index)).clip(0,3).values\n",
    "    rain_oh = np.zeros((4, L), dtype=np.float32); rain_oh[rain, np.arange(L)] = 1.0\n",
    "    return np.concatenate([cont, rain_oh], axis=0).astype(np.float32)\n",
    "\n",
    "def fb_spatial(tables, z, t0, L, zone_ids, spatial_usekeys=(\"occ\", \"dur\", \"vol\"), use_adj=True):\n",
    "    \"\"\"\n",
    "    ê³µê°„ í”¼ì²˜ (mean/gap/ratio) Ã— len(use_keys)\n",
    "    use_keys: ê³ ë ¤í•  ì‹œê³„ì—´ í‚¤ ëª©ë¡ (\"occ\",\"dur\",\"vol\")\n",
    "    use_adj : Trueë©´ adj í–‰ë ¬ ê³±í•´ì„œ ì¸ì ‘ zoneë§Œ ê³ ë ¤\n",
    "    \"\"\"\n",
    "    win = slice(t0, t0 + pd.Timedelta(hours=L-1))\n",
    "    zi = zone_ids.index(z)\n",
    "    D = tables[\"distance\"].values.astype(np.float32)\n",
    "    W = 1.0 / (D + 1e-6)\n",
    "    np.fill_diagonal(W, 0.0)\n",
    "\n",
    "    if use_adj and \"adj\" in tables:\n",
    "        A = tables[\"adj\"].values.astype(np.float32)\n",
    "        np.fill_diagonal(A, 0.0)\n",
    "        W *= A   # adj=0ì´ë©´ ì œì™¸\n",
    "\n",
    "    # í–‰ ì •ê·œí™”\n",
    "    W = W / (W.sum(axis=1, keepdims=True) + 1e-6)\n",
    "    w_i = W[zi]\n",
    "\n",
    "    feats = []\n",
    "    for key in spatial_usekeys:\n",
    "        if key not in tables:\n",
    "            continue\n",
    "        mat = tables[key].loc[win].values.astype(np.float32)  # (L, Z)\n",
    "        mean_val = mat @ w_i\n",
    "        self_val = mat[:, zi]\n",
    "        gap   = self_val - mean_val\n",
    "        ratio = np.clip(self_val / (mean_val + 1e-6), 0, 5)\n",
    "        feats.append(np.stack([mean_val, gap, ratio]))\n",
    "\n",
    "    return np.concatenate(feats, axis=0).astype(np.float32)  # (3*len(use_keys), L)\n",
    "\n",
    "\n",
    "\n",
    "def fb_static(tables, z, stats=None):\n",
    "    row = tables[\"inf\"].loc[z]\n",
    "\n",
    "    def _safe_standardize_log1p(val, stat_key):\n",
    "        v = pd.to_numeric(val, errors=\"coerce\")\n",
    "        if not np.isfinite(v):\n",
    "            return np.float32(0.0)  # mean-impute in standardized space\n",
    "\n",
    "        # ğŸ”§ ìŒìˆ˜ ë°©ì–´: log1p ì „ì— 0ìœ¼ë¡œ í´ë¦½\n",
    "        v = float(v)\n",
    "        if v < 0:\n",
    "            v = 0.0\n",
    "\n",
    "        l = math.log1p(v)\n",
    "        s = stats.get(stat_key, {\"mean\": 0.0, \"std\": 1.0})\n",
    "        mu, sd = float(s.get(\"mean\", 0.0)), max(float(s.get(\"std\", 1.0)), 1e-8)\n",
    "        return np.float32((l - mu) / sd)\n",
    "\n",
    "    # meta: 3ê°œ ëª¨ë‘ log1p + z-score (NaNâ†’0.0)\n",
    "    cc  = _safe_standardize_log1p(row.get(\"charge_count\", np.nan), \"log_charge_count\")\n",
    "    ar  = _safe_standardize_log1p(row.get(\"area\", np.nan),          \"log_area\")\n",
    "    pe  = _safe_standardize_log1p(row.get(\"perimeter\", np.nan),     \"log_perimeter\")\n",
    "    meta = np.array([cc, ar, pe], dtype=np.float32)\n",
    "\n",
    "    # poi: ìˆìœ¼ë©´ ë™ì¼í•˜ê²Œ ì²˜ë¦¬, ì—†ìœ¼ë©´ 0\n",
    "    if \"poi_counts\" in tables and isinstance(tables[\"poi_counts\"], pd.DataFrame):\n",
    "        poi_row = tables[\"poi_counts\"].loc[z]\n",
    "        vals = []\n",
    "        for c in poi_row.index:\n",
    "            stat_key = f\"log_{c}\"\n",
    "            if stats is not None and stat_key in stats:\n",
    "                vals.append(_safe_standardize_log1p(poi_row[c], stat_key))\n",
    "            else:\n",
    "                # í†µê³„ê°€ ì—†ìœ¼ë©´ 0.0ìœ¼ë¡œ\n",
    "                vals.append(np.float32(0.0))\n",
    "        poi = np.array(vals, dtype=np.float32)\n",
    "    else:\n",
    "        poi = np.zeros(3, dtype=np.float32)\n",
    "\n",
    "    return np.concatenate([meta, poi]).astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "def fb_time(tables, t0, L):\n",
    "    win = slice(t0, t0 + pd.Timedelta(hours=L-1))\n",
    "    return tables[\"time\"].loc[win].values.T.astype(np.float32)\n",
    "\n",
    "def fb_target(tables, z, t0, H, offset=None):\n",
    "    if offset is None:\n",
    "        offset = getattr(cfg.data, \"pred_offset\", 1)  # ê¸°ë³¸ 1\n",
    "    start = t0 + pd.Timedelta(hours=offset)\n",
    "    end   = start + pd.Timedelta(hours=H-1)\n",
    "    if H == 1:\n",
    "        y = tables[\"occ\"].loc[start, z].astype(np.float32)\n",
    "        y = np.array([y], dtype=np.float32)\n",
    "        mask = np.isfinite(y)\n",
    "        return y, mask\n",
    "    else:\n",
    "        y_idx = slice(start, end)\n",
    "        y = tables[\"occ\"].loc[y_idx, z].values.astype(np.float32)\n",
    "        mask = np.isfinite(y)\n",
    "        return y, mask\n",
    "\n",
    "\n",
    "#### ìœ„ì—êº¼ ë””ë²„ê·¸\n",
    "# ìƒˆë¡œ/ìˆ˜ì •: EVDataset\n",
    "# ------------------------------\n",
    "# EVDataset (ìˆ˜ì • ë²„ì „)\n",
    "# ------------------------------\n",
    "class EVDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, si_df: pd.DataFrame, tables, zone_ids, cfg, stats=None):\n",
    "        self.si = si_df.reset_index(drop=True)\n",
    "        self.tables = tables\n",
    "        self.zone_ids = list(zone_ids)\n",
    "        self.cfg = cfg\n",
    "        self.stats = stats  # â˜… fb_staticì—ì„œ ì‚¬ìš©\n",
    "\n",
    "        # spatial ì˜µì…˜ (cfg.data.spatial ë„¤ì„ìŠ¤í˜ì´ìŠ¤)\n",
    "        self.spatial_keys = tuple(getattr(cfg.data, \"spatial_usekeys\", (\"occ\",)))\n",
    "        self.spatial_use_adj = bool(getattr(cfg.data, \"spatial_use_adj\", True))\n",
    "        self.spatial_dist_thresh = getattr(getattr(cfg.data, \"spatial\", SimpleNamespace()), \"dist_thresh\", None)\n",
    "        self.spatial_fallback_k = getattr(getattr(cfg.data, \"spatial\", SimpleNamespace()), \"fallback_k\", 5)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.si)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.si.iloc[idx]\n",
    "        z   = r[\"zone_id\"]\n",
    "        t0  = pd.to_datetime(r[\"t_start\"])\n",
    "        L   = int(r[\"L\"])\n",
    "        H   = int(self.cfg.data.H)\n",
    "\n",
    "        # ------ build inputs ------\n",
    "        x_local   = fb_local  (self.tables, z, t0, L)                         \n",
    "        x_weather = fb_weather(self.tables, t0, L)                            \n",
    "        x_price   = fb_price  (self.tables, z, t0, L)                        \n",
    "\n",
    "        # â˜… fb_spatial í™•ì¥ ë²„ì „ í˜¸ì¶œ\n",
    "        x_spatial = fb_spatial(\n",
    "            self.tables, z, t0, L, self.zone_ids,\n",
    "            spatial_usekeys=self.spatial_keys,\n",
    "            use_adj=self.spatial_use_adj\n",
    "        )\n",
    "\n",
    "        x_time    = fb_time   (self.tables, t0, L)                            \n",
    "        # â˜… fb_staticì— stats ì „ë‹¬\n",
    "        x_static  = fb_static (self.tables, z, stats=self.stats)              \n",
    "\n",
    "        # ------ target ------\n",
    "        y, mask = fb_target(self.tables, z, t0, H, offset=getattr(self.cfg.data, \"pred_offset\", 1))\n",
    "\n",
    "        # ë””ë²„ê·¸ ìœ íš¨ì„± ì²´í¬ (ì›ë˜ ì½”ë“œ ê·¸ëŒ€ë¡œ ë‘ì–´ë„ OK)\n",
    "        def _chk(name, arr, expect_ndim=None):\n",
    "            if not np.isfinite(arr).all():\n",
    "                bad = np.sum(~np.isfinite(arr))\n",
    "                raise ValueError(f\"[EVDataset] non-finite in {name} (bad={bad}) | idx={idx} z={z} t0={t0}\")\n",
    "            if expect_ndim is not None and arr.ndim != expect_ndim:\n",
    "                raise ValueError(f\"[EVDataset] ndim mismatch for {name}: got {arr.ndim}, expect {expect_ndim}\")\n",
    "        _chk(\"x_local\",   x_local,   expect_ndim=2)\n",
    "        _chk(\"x_weather\", x_weather, expect_ndim=2)\n",
    "        _chk(\"x_price\",   x_price,   expect_ndim=2)\n",
    "        _chk(\"x_spatial\", x_spatial, expect_ndim=2)\n",
    "        _chk(\"x_time\",    x_time,    expect_ndim=2)\n",
    "        _chk(\"x_static\",  x_static,  expect_ndim=1)\n",
    "        _chk(\"y\", y, expect_ndim=1)\n",
    "\n",
    "        has_target = bool(np.isfinite(y[mask]).any()) if mask.any() else False\n",
    "\n",
    "        moirai_key = {\n",
    "            \"zone_id\": z,\n",
    "            \"t_start_iso\": t0.strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "            \"L\": L,\n",
    "            \"occ_only\": int(getattr(self.cfg.data, \"baseline_occ_only\", True)),\n",
    "            \"c_sig\": \"+\".join(getattr(self.cfg.data, \"moirai_channels\", [\"occ\"])),\n",
    "            \"model_id\": self.cfg.moirai.model_id,\n",
    "            \"psig\": \"-\".join(map(str, self.cfg.moirai.patch_sizes)),\n",
    "            \"pool\": self.cfg.moirai.pool,\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"x_local\":  x_local,\n",
    "            \"x_weather\":x_weather,\n",
    "            \"x_price\":  x_price,\n",
    "            \"x_spatial\":x_spatial,\n",
    "            \"x_time\":   x_time,\n",
    "            \"x_static\": x_static,\n",
    "            \"y\":        y.astype(np.float32),\n",
    "            \"mask\":     mask.astype(bool),\n",
    "            \"has_target\": has_target,\n",
    "            \"moirai_key\": moirai_key,\n",
    "        }\n",
    "\n",
    "def _to_f32_on_device(xs):\n",
    "    return torch.from_numpy(np.stack(xs)).float().to(DEVICE)\n",
    "\n",
    "def _stack_bool_on_device(xs):\n",
    "    return torch.from_numpy(np.stack(xs)).to(DEVICE)\n",
    "\n",
    "def collate_fn_builder_baseline(moirai_emb, key2row, hashes, stats, *, debug_print=False):\n",
    "    def collate_fn(batch):\n",
    "        # --- 1) ìƒ˜í”Œ ë ˆë²¨ í•„í„°: ìœ íš¨ íƒ€ê¹ƒ ìˆëŠ” ìƒ˜í”Œë§Œ\n",
    "        valid_batch = [b for b in batch if b.get(\"has_target\", True) and b[\"mask\"].any()]\n",
    "        if len(valid_batch) == 0:\n",
    "            # í•™ìŠµ ë£¨í”„ì—ì„œ skip ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ í”Œë˜ê·¸ ë°˜í™˜\n",
    "            if debug_print:\n",
    "                print(\"[collate/baseline] skip: no valid samples in this batch\")\n",
    "            return {\"skip\": True}\n",
    "\n",
    "        batch = valid_batch\n",
    "\n",
    "        # --- 2) Moirai í‚¤ êµ¬ì„±\n",
    "        rows = []\n",
    "        for b in batch:\n",
    "            mk = dict(b[\"moirai_key\"])\n",
    "            mk[\"clock_hash\"] = hashes.clockhash\n",
    "            mk[\"norm_version\"] = stats[\"norm_version\"]\n",
    "            mk[\"datahash\"] = hashes.datahash\n",
    "            rows.append([mk[k] for k in KEY_COLS])\n",
    "        keys_df = pd.DataFrame(rows, columns=KEY_COLS)\n",
    "        idxs = [key2row[tuple(r)] for _, r in keys_df.iterrows()]\n",
    "\n",
    "        # --- 3) ìŠ¤íƒ\n",
    "        out = {\n",
    "            \"x_local\":   _to_f32_on_device([b[\"x_local\"]   for b in batch]),  # (B,1,L)\n",
    "            \"x_weather\": _to_f32_on_device([b[\"x_weather\"] for b in batch]),\n",
    "            \"x_price\":   _to_f32_on_device([b[\"x_price\"]   for b in batch]),\n",
    "            \"x_spatial\": _to_f32_on_device([b[\"x_spatial\"] for b in batch]),\n",
    "            \"x_static\":  _to_f32_on_device([b[\"x_static\"]  for b in batch]),\n",
    "            \"x_time\":    _to_f32_on_device([b[\"x_time\"]    for b in batch]),\n",
    "            \"y\":         _to_f32_on_device([b[\"y\"]         for b in batch]),  # (B,H)\n",
    "            \"mask\":      _stack_bool_on_device([b[\"mask\"]  for b in batch]),  # (B,H)\n",
    "            \"h_local_moirai\": moirai_emb[idxs].to(DEVICE).float(),            # (B, E)\n",
    "            \"skip\": False,\n",
    "        }\n",
    "\n",
    "        # --- 4) ë°°ì¹˜ ìœ íš¨ì„± ë””ë²„ê·¸\n",
    "        sel = out[\"mask\"].bool()\n",
    "        if debug_print:\n",
    "            B, H = out[\"y\"].shape\n",
    "            print(f\"[collate/baseline] B={B} H={H} | valid_targets={int(sel.sum().item())}\")\n",
    "        if sel.sum().item() == 0:\n",
    "            if debug_print:\n",
    "                print(\"[collate/baseline] skip: mask has no True\")\n",
    "            out[\"skip\"] = True\n",
    "            return out\n",
    "\n",
    "        # NaN/Inf ì²´í¬ (yëŠ” mask ê¸°ì¤€ìœ¼ë¡œë§Œ ê²€ì‚¬)\n",
    "        if not torch.isfinite(out[\"y\"][sel]).all():\n",
    "            raise ValueError(\"[collate/baseline] non-finite in y (masked selection)\")\n",
    "        # ì…ë ¥ë“¤ ê°„ë‹¨ ê²€ì‚¬(ì›í•˜ë©´ ë” ì¶”ê°€)\n",
    "        for k in [\"x_local\",\"x_weather\",\"x_price\",\"x_spatial\",\"x_time\",\"x_static\",\"h_local_moirai\"]:\n",
    "            if not torch.isfinite(out[k]).all():\n",
    "                raise ValueError(f\"[collate/baseline] non-finite in {k}\")\n",
    "\n",
    "        return out\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "def collate_fn_builder_finetune(*, debug_print=False):\n",
    "    def collate_fn(batch):\n",
    "        # --- 1) ìƒ˜í”Œ ë ˆë²¨ í•„í„°\n",
    "        valid_batch = [b for b in batch if b.get(\"has_target\", True) and b[\"mask\"].any()]\n",
    "        if len(valid_batch) == 0:\n",
    "            if debug_print:\n",
    "                print(\"[collate/finetune] skip: no valid samples in this batch\")\n",
    "            return {\"skip\": True}\n",
    "\n",
    "        batch = valid_batch\n",
    "\n",
    "        # --- 2) ìŠ¤íƒ\n",
    "        out = {\n",
    "            \"x_local\":   _to_f32_on_device([b[\"x_local\"]   for b in batch]),\n",
    "            \"x_weather\": _to_f32_on_device([b[\"x_weather\"] for b in batch]),\n",
    "            \"x_price\":   _to_f32_on_device([b[\"x_price\"]   for b in batch]),\n",
    "            \"x_spatial\": _to_f32_on_device([b[\"x_spatial\"] for b in batch]),\n",
    "            \"x_static\":  _to_f32_on_device([b[\"x_static\"]  for b in batch]),\n",
    "            \"x_time\":    _to_f32_on_device([b[\"x_time\"]    for b in batch]),\n",
    "            \"y\":         _to_f32_on_device([b[\"y\"]         for b in batch]),\n",
    "            \"mask\":      _stack_bool_on_device([b[\"mask\"]  for b in batch]),\n",
    "            \"skip\": False,\n",
    "        }\n",
    "\n",
    "        # --- 3) ë°°ì¹˜ ìœ íš¨ì„± ë””ë²„ê·¸\n",
    "        sel = out[\"mask\"].bool()\n",
    "        if debug_print:\n",
    "            B, H = out[\"y\"].shape\n",
    "            print(f\"[collate/finetune] B={B} H={H} | valid_targets={int(sel.sum().item())}\")\n",
    "        if sel.sum().item() == 0:\n",
    "            if debug_print:\n",
    "                print(\"[collate/finetune] skip: mask has no True\")\n",
    "            out[\"skip\"] = True\n",
    "            return out\n",
    "\n",
    "        if not torch.isfinite(out[\"y\"][sel]).all():\n",
    "            raise ValueError(\"[collate/finetune] non-finite in y (masked selection)\")\n",
    "        for k in [\"x_local\",\"x_weather\",\"x_price\",\"x_spatial\",\"x_time\",\"x_static\"]:\n",
    "            if not torch.isfinite(out[k]).all():\n",
    "                raise ValueError(f\"[collate/finetune] non-finite in {k}\")\n",
    "\n",
    "        return out\n",
    "    return collate_fn\n",
    "\n",
    "class EVForecastModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        D   = cfg.model.D_model\n",
    "        p   = cfg.model.dropout\n",
    "\n",
    "        # ----- local branch (modeë³„) -----\n",
    "        if cfg.model.mode == \"baseline\":\n",
    "            # emb_dimì€ ensure_moirai_cache() ì´í›„ cfg.model.D_moiraiê°€ ì„¸íŒ…ë˜ì–´ ìˆìŒ\n",
    "            D_m = cfg.model.D_moirai\n",
    "            self.adapter = nn.Sequential(\n",
    "                nn.Linear(D_m, D), nn.GELU(), nn.Dropout(p), nn.Linear(D, D)\n",
    "            )\n",
    "            self.moirai_backbone = None\n",
    "            self.local_proj = None  # baselineì—ì„  ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n",
    "        else:  # finetune\n",
    "            # í•™ìŠµ ê°€ëŠ¥í•œ Moirai\n",
    "            self.moirai_backbone = MoiraiModule.from_pretrained(cfg.moirai.model_id)\n",
    "            for prm in self.moirai_backbone.parameters():\n",
    "                prm.requires_grad = True\n",
    "            # D_mì€ ëª¨ë“ˆ ë‚´ë¶€ ì„¤ì •ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì–´ â†’ ì²« forwardì—ì„œ lazy init\n",
    "            self.adapter = None\n",
    "            self.local_proj = None  # ì²« forwardì—ì„œ Moirai ì¶œë ¥ ì°¨ì› ë³´ê³  ìƒì„±\n",
    "\n",
    "        # ----- ë™ì  ì±„ë„ ê³„ì‚° -----\n",
    "        # weather: ì—°ì† 5(T,P0,P,U,Td) + rain ì›í•« 4 = 9\n",
    "        cin_weather = 9\n",
    "        # price: e_price, s_price = 2\n",
    "        cin_price   = 2\n",
    "        # time: hour_sin, hour_cos, dow_sin, dow_cos, is_offday = 5\n",
    "        cin_time    = 5\n",
    "\n",
    "        # spatial: (mean, gap, ratio) Ã— len(use_keys)\n",
    "        # ìš°ì„ ìˆœìœ„: cfg.data.spatial.use_keys -> cfg.data.spatial_usekeys -> ê¸°ë³¸ (\"occ\",)\n",
    "        spatial_use_keys = None\n",
    "    \n",
    "        if hasattr(cfg.data, \"spatial_usekeys\"):\n",
    "            spatial_use_keys = tuple(cfg.data.spatial_usekeys)\n",
    "        else:\n",
    "            spatial_use_keys = (\"occ\",)  # fallback\n",
    "\n",
    "        cin_spatial = 3 * max(1, len(spatial_use_keys))\n",
    "\n",
    "        # ----- other encoders -----\n",
    "        ks = cfg.model.KERNEL_SIZE; pad = ks//2\n",
    "        def conv_block(cin):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(cin, 64, ks, padding=pad), nn.GELU(),\n",
    "                nn.Conv1d(64, 64, ks, padding=pad), nn.GELU()\n",
    "            )\n",
    "\n",
    "        self.enc_weather = conv_block(cin_weather)\n",
    "        self.enc_price   = conv_block(cin_price)\n",
    "        self.enc_spatial = conv_block(cin_spatial)\n",
    "        self.enc_time    = conv_block(cin_time)\n",
    "\n",
    "        # static: charge_count/area/perimeter + poi 3ê°œ = 6\n",
    "        self.enc_static  = nn.Sequential(nn.Linear(6, 128), nn.GELU(), nn.Dropout(p), nn.Linear(128, D))\n",
    "\n",
    "        self.proj_weather= nn.Linear(64, D)\n",
    "        self.proj_price  = nn.Linear(64, D)\n",
    "        self.proj_spatial= nn.Linear(64, D)\n",
    "        self.proj_time   = nn.Linear(64, D)\n",
    "\n",
    "        def align(): return nn.Sequential(nn.LayerNorm(D), nn.Dropout(p), nn.Linear(D, D))\n",
    "        self.align_local = align(); self.align_weather = align(); self.align_price = align()\n",
    "        self.align_spatial = align(); self.align_static = align(); self.align_time = align()\n",
    "\n",
    "        self.mha  = nn.MultiheadAttention(D, cfg.model.nhead, dropout=p, batch_first=True)\n",
    "        self.head = nn.Linear(D, self.cfg.data.H)\n",
    "        self.use_softplus = bool(getattr(cfg.model, \"nonneg_head\", False))\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # ===== 1) Local (modeë³„ ê²½ë¡œ) =====\n",
    "        if self.cfg.model.mode == \"baseline\":\n",
    "            # ìºì‹œì—ì„œ ì˜¨ h_local_moirai â†’ adapter\n",
    "            h_local = self.adapter(batch[\"h_local_moirai\"])\n",
    "        else:\n",
    "            # x_local: (B,1,L) â†’ Moirai -> (B,T,Dm or B,Dm)\n",
    "            x_local = batch[\"x_local\"]  # (B,1,L)\n",
    "            B, C, L = x_local.shape\n",
    "            target = x_local.transpose(1, 2)  # (B,L,1)\n",
    "            observed_mask = torch.ones_like(target, dtype=torch.bool)\n",
    "            prediction_mask = torch.zeros(B, L, dtype=torch.bool, device=target.device)\n",
    "            sample_id = torch.arange(B, device=target.device).unsqueeze(1).expand(B, L)\n",
    "            time_id   = torch.arange(L, device=target.device).unsqueeze(0).expand(B, L)\n",
    "            variate_id= torch.zeros(B, L, dtype=torch.long, device=target.device)\n",
    "            patch_size = torch.full((B, L), self.cfg.moirai.patch_sizes[0], dtype=torch.long, device=target.device)\n",
    "\n",
    "            # â”€â”€ lazy init: local_projê°€ ì•„ì§ ì—†ìœ¼ë©´ ëª¨ì–‘ë§Œ í•œ ë²ˆ í™•ì¸\n",
    "            if self.local_proj is None:\n",
    "                with torch.no_grad():\n",
    "                    out_probe = self.moirai_backbone(\n",
    "                        target=target, observed_mask=observed_mask,\n",
    "                        prediction_mask=prediction_mask, sample_id=sample_id,\n",
    "                        time_id=time_id, variate_id=variate_id, patch_size=patch_size\n",
    "                    )\n",
    "                    hs_probe = out_probe if isinstance(out_probe, torch.Tensor) else getattr(out_probe, \"mean\", out_probe.sample())\n",
    "                    if hs_probe.dim() == 3:  # (B,T,Dm)\n",
    "                        hs_probe = hs_probe.mean(dim=1)  # cfg.moirai.pool ë°˜ì˜í•˜ë ¤ë©´ ì—¬ê¸°ì„œ ë¶„ê¸°\n",
    "                    D_m = int(hs_probe.shape[1])\n",
    "                # ì´ì œ íˆ¬ì‚¬ê¸° ìƒì„±(D_mâ†’D)\n",
    "                D = self.cfg.model.D_model; p = self.cfg.model.dropout\n",
    "                self.local_proj = nn.Sequential(nn.Linear(D_m, D), nn.GELU(), nn.Dropout(p), nn.Linear(D, D)).to(target.device)\n",
    "\n",
    "            # ì‹¤ì œ forwardëŠ” ê·¸ë˜í”„ë¥¼ ìœ ì§€í•´ì•¼ í•˜ë¯€ë¡œ ì¬ê³„ì‚°(í”„ë¡œë¸ŒëŠ” no_gradë¡œë§Œ ëª¨ì–‘ í™•ì¸ìš©)\n",
    "            out = self.moirai_backbone(\n",
    "                target=target, observed_mask=observed_mask,\n",
    "                prediction_mask=prediction_mask, sample_id=sample_id,\n",
    "                time_id=time_id, variate_id=variate_id, patch_size=patch_size\n",
    "            )\n",
    "            hs = out if isinstance(out, torch.Tensor) else getattr(out, \"mean\", out.sample())\n",
    "            if hs.dim() == 3:\n",
    "                hs = hs.mean(dim=1)  # pool=\"mean\" ê¸°ë³¸\n",
    "            h_local = self.local_proj(hs)\n",
    "\n",
    "        # ===== 2) Other modalities =====\n",
    "        h_weather = self.proj_weather(self.enc_weather(batch[\"x_weather\"]).mean(-1))\n",
    "        h_price   = self.proj_price  (self.enc_price  (batch[\"x_price\"  ]).mean(-1))\n",
    "        h_spatial = self.proj_spatial(self.enc_spatial(batch[\"x_spatial\"]).mean(-1))\n",
    "        h_time    = self.proj_time   (self.enc_time   (batch[\"x_time\"   ]).mean(-1))\n",
    "        h_static  = self.enc_static(batch[\"x_static\"])\n",
    "\n",
    "        # ===== 3) Align + fusion + head =====\n",
    "        def gate(align, h): return torch.sigmoid(align(h)) * h\n",
    "        h_local_a   = gate(self.align_local,   h_local)\n",
    "        h_weather_a = gate(self.align_weather, h_weather)\n",
    "        h_price_a   = gate(self.align_price,   h_price)\n",
    "        h_spatial_a = gate(self.align_spatial, h_spatial)\n",
    "        h_static_a  = gate(self.align_static,  h_static)\n",
    "        h_time_a    = gate(self.align_time,    h_time)\n",
    "\n",
    "        tokens = torch.stack([h_weather_a, h_price_a, h_spatial_a, h_static_a, h_time_a], dim=1)\n",
    "        q = h_local_a.unsqueeze(1)\n",
    "        h, _ = self.mha(q, tokens, tokens)\n",
    "        y_hat = self.head(h.squeeze(1))\n",
    "        return self.softplus(y_hat) if self.use_softplus else y_hat\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def loss_epoch(model, dataloader, criterion=F.mse_loss, optimizer=None, scheduler=None, sched_per_batch=False, cfg=None, desc=None):\n",
    "    \"\"\"\n",
    "    í•œ epoch í•™ìŠµ/ê²€ì¦ (optimizer ìœ ë¬´ë¡œ train/eval).\n",
    "    - lossëŠ” mask=True ìœ„ì¹˜ë§Œ í‰ê· (reduction='mean').\n",
    "    - epoch í‰ê· ì€ ìœ íš¨íƒ€ê¹ƒìˆ˜ ê°€ì¤‘ í‰ê· .\n",
    "    - clip_gradëŠ” cfg.train.clip_grad ì‚¬ìš©(ì—†ìœ¼ë©´ ë¯¸ì ìš©).\n",
    "    - schedulerëŠ” ì˜µì…˜: per-batch(step_per_batch=True) ë˜ëŠ” per-epoch(train() ë°”ê¹¥).\n",
    "    \"\"\"\n",
    "    is_train = optimizer is not None\n",
    "    model.train() if is_train else model.eval()\n",
    "\n",
    "    clip_grad = None\n",
    "    if cfg is not None and hasattr(cfg, \"train\") and hasattr(cfg.train, \"clip_grad\"):\n",
    "        clip_grad = cfg.train.clip_grad\n",
    "        # 0 ë˜ëŠ” Falseë©´ ë¹„ì ìš©\n",
    "\n",
    "    total_loss_w = 0.0\n",
    "    total_valid  = 0\n",
    "    batch_losses = []\n",
    "\n",
    "    loop = tqdm(dataloader, desc=desc or (\"train\" if is_train else \"valid\"))\n",
    "    for batch in loop:\n",
    "        if batch.get(\"skip\", False):\n",
    "            continue\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # forward & masked loss\n",
    "        y_hat = model(batch)             # (B,H)\n",
    "        sel   = batch[\"mask\"].bool()     # (B,H)\n",
    "        n     = int(sel.sum().item())\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        loss = criterion(y_hat[sel], batch[\"y\"][sel])  # í‰ê·  ì†ì‹¤\n",
    "\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            if clip_grad and clip_grad > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            optimizer.step()\n",
    "            if scheduler is not None and sched_per_batch:\n",
    "                scheduler.step()\n",
    "\n",
    "        total_loss_w += float(loss.item()) * n\n",
    "        total_valid  += n\n",
    "        batch_losses.append(float(loss.item()))\n",
    "        loop.set_postfix(loss=f\"{loss.item():.4f}\", n_valid=n)\n",
    "\n",
    "    epoch_loss = total_loss_w / max(1, total_valid)\n",
    "    return epoch_loss, batch_losses\n",
    "\n",
    "import os, re, time, torch\n",
    "\n",
    "def _find_latest_epoch_ckpt(ckpt_dir):\n",
    "    \"\"\"\n",
    "    ckpt_dirì—ì„œ epoch_####.ckpt / .pt ì¤‘ ê°€ì¥ ë²ˆí˜¸ê°€ í° íŒŒì¼ ê²½ë¡œ ë°˜í™˜. ì—†ìœ¼ë©´ None.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(ckpt_dir):\n",
    "        return None\n",
    "    files = os.listdir(ckpt_dir)\n",
    "    cand = []\n",
    "    for fn in files:\n",
    "        m = re.match(r\"epoch_(\\d+)\\.(ckpt|pt)$\", fn)\n",
    "        if m:\n",
    "            cand.append((int(m.group(1)), os.path.join(ckpt_dir, fn)))\n",
    "    if not cand:\n",
    "        return None\n",
    "    cand.sort(key=lambda x: x[0])\n",
    "    return cand[-1][1]  # latest path\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion,\n",
    "          optimizer, save_best_path, config=None, resume_path=None, scheduler=None, sched_per_batch=False):\n",
    "    \"\"\"\n",
    "    - best ê¸°ì¤€: validation loss (criterion)\n",
    "    - metrics: cfg.train.metrics ì‚¬ìš© (ì—†ìœ¼ë©´ ê¸°ë³¸ 4ì¢…)\n",
    "    - scheduler: ì£¼ì–´ì§€ë©´ ì‚¬ìš©(Noneì´ë©´ ë¬´ì‹œ)\n",
    "    - resume: cfg.exec.new_model_train==Falseë©´ run_dir/checkpointsì—ì„œ ìµœì‹  epoch_* ë¡œë“œ(ëª¨ë¸+ì˜µí‹°ë§ˆì´ì €(+ìŠ¤ì¼€ì¤„ëŸ¬)).\n",
    "              resume_pathê°€ ë”°ë¡œ ì£¼ì–´ì§€ë©´ ê·¸ê±¸ ìš°ì„  ì‚¬ìš©.\n",
    "    \"\"\"\n",
    "    cfg = config or {}\n",
    "    EPOCHS   = getattr(cfg.train, \"epochs\", 5)\n",
    "    METRICS  = tuple(getattr(cfg.train, \"metrics\", [\"RMSE\",\"MAPE\",\"RAE\",\"MAE\"]))\n",
    "    RUN_DIR  = cfg.out.run_dir\n",
    "    CKPT_DIR = cfg.out.ckpt_dir\n",
    "    NEW_TRAIN = bool(getattr(cfg.exec, \"new_model_train\", True))\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # ---- FULL RESUME (A)\n",
    "    start_epoch = 0\n",
    "    if not NEW_TRAIN:\n",
    "        # ìš°ì„ ìˆœìœ„: ëª…ì‹œ resume_path > ìµœì‹  epoch_* ìë™íƒìƒ‰\n",
    "        latest = resume_path if resume_path and os.path.exists(resume_path) else _find_latest_epoch_ckpt(CKPT_DIR)\n",
    "        if latest:\n",
    "            print(f\"â–¶ï¸ Full resume from: {latest}\")\n",
    "            ckpt = torch.load(latest, map_location=DEVICE)\n",
    "            model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "            if \"optimizer_state_dict\" in ckpt:\n",
    "                optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "            if scheduler and ckpt.get(\"scheduler_state_dict\") is not None:\n",
    "                try:\n",
    "                    scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
    "                except Exception:\n",
    "                    print(\"[warn] scheduler state incompatible; ignoring\")\n",
    "            start_epoch = int(ckpt.get(\"epoch\", 0))\n",
    "        else:\n",
    "            print(\"â„¹ï¸ new_model_train=False ì´ì§€ë§Œ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ì–´ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    history = {\"train_epoch\": [], \"train_iter\": [], \"val_epoch\": [], \"val_metrics\": []}\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_epoch = 0\n",
    "\n",
    "    t0 = time.time()\n",
    "    for ep in range(start_epoch, EPOCHS):\n",
    "        eidx = ep + 1\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f\"\\n[Epoch {eidx}/{EPOCHS}] LR={lr:.3e}\")\n",
    "\n",
    "        # train\n",
    "        train_loss, train_batches = loss_epoch(\n",
    "            model, train_loader, criterion=criterion,\n",
    "            optimizer=optimizer, scheduler=scheduler, sched_per_batch=sched_per_batch, cfg=cfg, desc=\"train\"\n",
    "        )\n",
    "        history[\"train_epoch\"].append(train_loss)\n",
    "        history[\"train_iter\"].append(train_batches)\n",
    "        print(f\" Train Loss: {train_loss:.4f} | iters: {len(train_batches)}\")\n",
    "\n",
    "        # valid\n",
    "        val_loss, _ = loss_epoch(model, val_loader, criterion=criterion, optimizer=None, cfg=cfg, desc=\"valid\")\n",
    "        history[\"val_epoch\"].append(val_loss)\n",
    "        metrics = compute_metrics(model, val_loader, metrics=METRICS)\n",
    "        history[\"val_metrics\"].append(metrics)\n",
    "        print(\" Val  Loss: {:.4f} | {}\".format(val_loss, \" | \".join([f\"{k}:{v:.4f}\" for k,v in metrics.items()])))\n",
    "\n",
    "        # best by val loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = eidx\n",
    "            os.makedirs(os.path.dirname(save_best_path) or \".\", exist_ok=True)\n",
    "            torch.save({\n",
    "                \"epoch\": best_epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": best_val_loss,\n",
    "                \"val_metrics\": metrics\n",
    "            }, save_best_path)\n",
    "            print(f\" âœ… Best model updated @ epoch {best_epoch} (ValLoss {best_val_loss:.4f})\")\n",
    "\n",
    "        # per-epoch ckpt\n",
    "        ep_ckpt = cfg.out.epoch_ckpt_tmpl.format(eidx)  # ì˜ˆ: \".../epoch_0001.ckpt\"\n",
    "        torch.save({\n",
    "            \"epoch\": eidx,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_metrics\": metrics\n",
    "        }, ep_ckpt)\n",
    "\n",
    "        # scheduler per-epoch step (per-batchë©´ loss_epochì—ì„œ ì´ë¯¸ ì²˜ë¦¬)\n",
    "        if scheduler is not None and not sched_per_batch:\n",
    "            scheduler.step()\n",
    "\n",
    "    print(f\"\\nDone. Best epoch={best_epoch} | best val loss={best_val_loss:.4f} | time={time.time()-t0:.1f}s\")\n",
    "    return history\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics(model, dataloader, metrics=(\"RMSE\",\"MAPE\",\"RAE\",\"MAE\"), eps=1e-8):\n",
    "    model.eval()\n",
    "    agg = {m: 0.0 for m in metrics}\n",
    "    total_valid = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        if batch.get(\"skip\", False):\n",
    "            continue\n",
    "        y_hat = model(batch)\n",
    "        y     = batch[\"y\"]\n",
    "        sel   = batch[\"mask\"].bool()\n",
    "        n     = int(sel.sum().item())\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        diff   = (y_hat - y)[sel]\n",
    "        absdif = diff.abs()\n",
    "        sqdif  = diff.pow(2)\n",
    "        y_sel  = y[sel]\n",
    "\n",
    "        rmse = torch.sqrt(sqdif.mean()).item()\n",
    "        mae  = absdif.mean().item()\n",
    "        mape = (absdif / (y_sel.abs() + eps)).mean().item() * 100.0\n",
    "        rae  = (absdif.sum().item()) / ((y_sel - y_sel.mean()).abs().sum().item() + eps)\n",
    "\n",
    "        if \"RMSE\" in metrics: agg[\"RMSE\"] += rmse * n\n",
    "        if \"MAE\"  in metrics: agg[\"MAE\"]  += mae  * n\n",
    "        if \"MAPE\" in metrics: agg[\"MAPE\"] += mape * n\n",
    "        if \"RAE\"  in metrics: agg[\"RAE\"]  += rae  * n\n",
    "        total_valid += n\n",
    "\n",
    "    return {m: (agg[m] / max(1, total_valid)) for m in metrics}\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, test_loader, criterion=F.mse_loss, metrics=(\"RMSE\",\"MAPE\",\"RAE\",\"MAE\")):\n",
    "    model.eval()\n",
    "    total_loss_w = 0.0\n",
    "    total_valid  = 0\n",
    "    for batch in tqdm(test_loader, desc=\"test\"):\n",
    "        if batch.get(\"skip\", False):\n",
    "            continue\n",
    "        out = model(batch)\n",
    "        sel = batch[\"mask\"].bool()\n",
    "        n   = int(sel.sum().item())\n",
    "        if n == 0:\n",
    "            continue\n",
    "        loss = criterion(out[sel], batch[\"y\"][sel])\n",
    "        total_loss_w += float(loss.item()) * n\n",
    "        total_valid  += n\n",
    "    test_loss = total_loss_w / max(1, total_valid)\n",
    "\n",
    "    m = compute_metrics(model, test_loader, metrics=metrics)\n",
    "    print(\"Test:\", \" | \".join([f\"{k}:{v:.4f}\" for k,v in m.items()]), f\"| Loss:{test_loss:.4f}\")\n",
    "    return {\"loss\": test_loss, **m}\n",
    "def run_train(cfg):\n",
    "    # 1) Load & align data\n",
    "    clock, tables, zone_ids, hashes = prepare_data(cfg)\n",
    "\n",
    "    # 2) (Optional) POI counts via cache (fast, idempotent)\n",
    "    if getattr(cfg.data, \"use_poi\", False) and os.path.exists(cfg.data.paths.get(\"poi\", \"\")):\n",
    "        try:\n",
    "            ensure_poi_counts(tables, cfg)  \n",
    "        except ModuleNotFoundError:\n",
    "            print(\"[warn] scikit-learn not installed; skip POI\")\n",
    "            cfg.data.use_poi = False\n",
    "\n",
    "    # 3) Sample indices + standardization (tables standardized in-place)\n",
    "    si, stats = build_sample_indices(tables, clock, zone_ids, cfg)\n",
    "    print(f\"[norm_version] {stats['norm_version']}\")\n",
    "\n",
    "    # 4) Collate function & (optional) Moirai cache for baseline mode\n",
    "    mode = str(getattr(cfg.model, \"mode\", \"baseline\")).lower()\n",
    "    if mode == \"baseline\":\n",
    "        # create Moirai cache for train+val to avoid misses during training\n",
    "        si_train = si.get(\"train\", pd.DataFrame())\n",
    "        si_val   = si.get(\"val\",   pd.DataFrame())\n",
    "        si_tv    = pd.concat([si_train, si_val], ignore_index=True) if not si_train.empty or not si_val.empty else pd.DataFrame(columns=[\"t_start\",\"zone_id\",\"L\",\"H\",\"occ_only\",\"split\"])\n",
    "        keys_tv  = make_moirai_keys(si_tv, hashes, stats, cfg) if not si_tv.empty else pd.DataFrame(columns=KEY_COLS)\n",
    "\n",
    "        moirai_df, moirai_emb, emb_dim = ensure_moirai_cache(keys_tv, tables, cfg, hashes, stats)\n",
    "        cfg.model.D_moirai = int(emb_dim)\n",
    "\n",
    "        # build lookup index for cached embeddings\n",
    "        key2row = {}\n",
    "        for i, (_, r) in enumerate(moirai_df.iterrows()):\n",
    "            key2row[tuple(r[KEY_COLS])] = i\n",
    "\n",
    "        collate_fn = collate_fn_builder_baseline(moirai_emb, key2row, hashes, stats)\n",
    "    else:\n",
    "        collate_fn = collate_fn_builder_finetune()\n",
    "\n",
    "    # 5) Datasets & Loaders (pass stats to keep fb_static normalization consistent)\n",
    "    train_df = si.get(\"train\", pd.DataFrame())\n",
    "    val_df   = si.get(\"val\",   pd.DataFrame())\n",
    "    train_ds = EVDataset(train_df, tables, zone_ids, cfg, stats=stats)\n",
    "    val_ds   = EVDataset(val_df,   tables, zone_ids, cfg, stats=stats)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=cfg.train.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=cfg.train.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # 6) Model & optimizer\n",
    "    model = EVForecastModel(cfg).to(DEVICE)\n",
    "    opt_name = str(getattr(cfg.train, \"optimizer\", \"adamw\")).lower()\n",
    "    if opt_name == \"adamw\":\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.train.lr, weight_decay=cfg.train.weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=cfg.train.lr, weight_decay=cfg.train.weight_decay)\n",
    "\n",
    "    # 7) Scheduler (inline)\n",
    "    scheduler = None\n",
    "    sched_per_batch = False\n",
    "    sched_name = str(getattr(cfg.train, \"scheduler\", \"none\") or \"none\").lower()\n",
    "    if sched_name == \"onecycle\":\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=cfg.train.lr,\n",
    "            epochs=cfg.train.epochs,\n",
    "            steps_per_epoch=max(1, len(train_loader))\n",
    "        )\n",
    "        sched_per_batch = True\n",
    "    elif sched_name in (\"steplr\", \"step\"):\n",
    "        step  = getattr(cfg.train, \"lr_step\", None)\n",
    "        gamma = getattr(cfg.train, \"lr_gamma\", None)\n",
    "        if step and gamma:\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step, gamma=gamma)\n",
    "        else:\n",
    "            print(\"[warn] StepLR needs cfg.train.lr_step & lr_gamma â€” skipping scheduler.\")\n",
    "\n",
    "    # 8) Full resume when exec.new_model_train == False\n",
    "    resume_path = None\n",
    "    if not bool(getattr(cfg.exec, \"new_model_train\", True)):\n",
    "        latest_num, latest_path = -1, None\n",
    "        if os.path.isdir(cfg.out.ckpt_dir):\n",
    "            for fn in os.listdir(cfg.out.ckpt_dir):\n",
    "                if fn.startswith(\"epoch_\") and (fn.endswith(\".ckpt\") or fn.endswith(\".pt\")):\n",
    "                    try:\n",
    "                        n = int(fn.split(\"_\")[1].split(\".\")[0])\n",
    "                        if n > latest_num:\n",
    "                            latest_num, latest_path = n, os.path.join(cfg.out.ckpt_dir, fn)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        if latest_path:\n",
    "            resume_path = latest_path\n",
    "            print(f\"â–¶ï¸ Full resume from: {resume_path}\")\n",
    "            ckpt = torch.load(resume_path, map_location=DEVICE)\n",
    "            model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "            if \"optimizer_state_dict\" in ckpt:\n",
    "                optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "            if scheduler is not None and ckpt.get(\"scheduler_state_dict\") is not None:\n",
    "                try:\n",
    "                    scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
    "                except Exception:\n",
    "                    print(\"[warn] scheduler state incompatible; ignoring\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸ new_model_train=False ì´ì§€ë§Œ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ì–´ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    # 9) Train (best by val loss)\n",
    "    history = train(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=torch.nn.functional.mse_loss,\n",
    "        optimizer=optimizer,\n",
    "        save_best_path=cfg.out.best_ckpt,\n",
    "        config=cfg,\n",
    "        resume_path=resume_path,\n",
    "        scheduler=scheduler,\n",
    "        sched_per_batch=sched_per_batch\n",
    "    )\n",
    "\n",
    "    # 10) Return prepared bundle for reuse\n",
    "    return {\n",
    "        \"history\": history,\n",
    "        \"clock\": clock,\n",
    "        \"tables\": tables,\n",
    "        \"zone_ids\": zone_ids,\n",
    "        \"hashes\": hashes,\n",
    "        \"stats\": stats,\n",
    "        \"si\": si\n",
    "    }\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def run_test(cfg, prepared=None, mode=\"best\", ckpt_path=None, batch_size=None, drop_last=False, num_workers=0):\n",
    "    \"\"\"\n",
    "    mode:\n",
    "      - \"best\":  cfg.out.best_ckpt ë¡œë“œ\n",
    "      - \"last\":  cfg.out.last_ckpt ë¡œë“œ(ì—†ìœ¼ë©´ ckpt_dirì˜ ìµœëŒ€ epoch_* ìë™íƒìƒ‰)\n",
    "      - \"path\":  ì¸ìë¡œ ì£¼ëŠ” ckpt_path ë¡œë“œ\n",
    "    \"\"\"\n",
    "    # ---------- 0) sanity ----------\n",
    "    mode = str(mode).lower()\n",
    "    assert mode in (\"best\",\"last\",\"path\"), \"mode must be one of {'best','last','path'}\"\n",
    "    if mode == \"path\":\n",
    "        assert ckpt_path and os.path.exists(ckpt_path), f\"ckpt_path not found: {ckpt_path}\"\n",
    "\n",
    "    # ---------- 1) ë°ì´í„°/ì „ì²˜ë¦¬ ì¤€ë¹„ ----------\n",
    "    if prepared is not None:\n",
    "        clock  = prepared[\"clock\"]\n",
    "        tables = prepared[\"tables\"]\n",
    "        zone_ids = prepared[\"zone_ids\"]\n",
    "        hashes = prepared[\"hashes\"]\n",
    "        stats  = prepared[\"stats\"]\n",
    "        si     = prepared[\"si\"]\n",
    "    else:\n",
    "        clock, tables, zone_ids, hashes = prepare_data(cfg)\n",
    "\n",
    "        # (ì˜µì…˜) POI ìºì‹œ/ê³„ì‚°\n",
    "        if getattr(cfg.data, \"use_poi\", False) and os.path.exists(cfg.data.paths.get(\"poi\",\"\")):\n",
    "            try:\n",
    "                ensure_poi_counts(tables, cfg)\n",
    "            except ModuleNotFoundError:\n",
    "                print(\"[warn] scikit-learn not installed; skip POI\"); cfg.data.use_poi = False\n",
    "\n",
    "        # ìƒ˜í”Œ ì¸ë±ìŠ¤ + í‘œì¤€í™”\n",
    "        si, stats = build_sample_indices(tables, clock, zone_ids, cfg)\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ ë¶„í•  ì²´í¬\n",
    "    if \"test\" not in si or si[\"test\"].empty:\n",
    "        raise RuntimeError(\"[run_test] no test samples found in si['test'].\")\n",
    "\n",
    "    # ---------- 2) Collate & (baselineì¼ ë•Œ) Moirai ìºì‹œ ----------\n",
    "    model_mode = str(getattr(cfg.model, \"mode\", \"baseline\")).lower()\n",
    "    if model_mode == \"baseline\":\n",
    "        keys_te = make_moirai_keys(si[\"test\"], hashes, stats, cfg)\n",
    "        moirai_df, moirai_emb, emb_dim = ensure_moirai_cache(keys_te, tables, cfg, hashes, stats)\n",
    "        cfg.model.D_moirai = int(emb_dim)\n",
    "\n",
    "        key2row = {}\n",
    "        for i, (_, r) in enumerate(moirai_df.iterrows()):\n",
    "            key2row[tuple(r[KEY_COLS])] = i\n",
    "\n",
    "        collate_fn = collate_fn_builder_baseline(moirai_emb, key2row, hashes, stats)\n",
    "    else:\n",
    "        collate_fn = collate_fn_builder_finetune()\n",
    "\n",
    "    # ---------- 3) Test Loader ----------\n",
    "    bs = int(batch_size or cfg.train.batch_size)\n",
    "    test_ds = EVDataset(si[\"test\"], tables, zone_ids, cfg, stats=stats)\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=bs,\n",
    "        shuffle=False,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # ---------- 4) ì²´í¬í¬ì¸íŠ¸ ì„ íƒ ----------\n",
    "    load_path = None\n",
    "    if mode == \"best\":\n",
    "        load_path = cfg.out.best_ckpt\n",
    "        if not os.path.exists(load_path):\n",
    "            raise FileNotFoundError(f\"[run_test] best_ckpt not found: {load_path}\")\n",
    "    elif mode == \"last\":\n",
    "        lp = cfg.out.last_ckpt\n",
    "        if os.path.exists(lp):\n",
    "            load_path = lp\n",
    "        else:\n",
    "            # ckpt_dirì—ì„œ ê°€ì¥ í° epoch_* íƒìƒ‰\n",
    "            ckpt_dir = cfg.out.ckpt_dir\n",
    "            best_n, best_path = -1, None\n",
    "            if os.path.isdir(ckpt_dir):\n",
    "                for fn in os.listdir(ckpt_dir):\n",
    "                    if fn.startswith(\"epoch_\") and (fn.endswith(\".ckpt\") or fn.endswith(\".pt\")):\n",
    "                        try:\n",
    "                            n = int(fn.split(\"_\")[1].split(\".\")[0])\n",
    "                            if n > best_n:\n",
    "                                best_n, best_path = n, os.path.join(ckpt_dir, fn)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "            if best_path is None:\n",
    "                raise FileNotFoundError(\"[run_test] no last checkpoint found (last_ckpt missing and no epoch_*.ckpt).\")\n",
    "            load_path = best_path\n",
    "    else:  # mode == \"path\"\n",
    "        load_path = ckpt_path\n",
    "\n",
    "    # ---------- 5) ëª¨ë¸ ë¡œë“œ ----------\n",
    "    model = EVForecastModel(cfg).to(DEVICE)\n",
    "    ckpt = torch.load(load_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    # ---------- 6) í‰ê°€ ----------\n",
    "    # test()ëŠ” masked MSEë¡œ loss ê³„ì‚° + compute_metricsë¡œ 4ì¢… ì§€í‘œ ë¦¬í„´\n",
    "    result = test(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        criterion=torch.nn.functional.mse_loss,\n",
    "        metrics=tuple(getattr(cfg.train, \"metrics\", [\"RMSE\",\"MAPE\",\"RAE\",\"MAE\"]))\n",
    "    )\n",
    "\n",
    "    # ë©”íƒ€ì •ë³´ ì¶”ê°€\n",
    "    out = {\n",
    "        \"mode\": mode,\n",
    "        \"ckpt_path\": load_path,\n",
    "        \"n_test_samples\": len(test_ds),\n",
    "        \"batch_size\": bs,\n",
    "        **result,\n",
    "    }\n",
    "\n",
    "    return out\n",
    "cfg = SimpleNamespace(\n",
    "    exec = SimpleNamespace(\n",
    "        new_model_train = True,\n",
    "        poi_shared_dir = \"poi_cache_global\",\n",
    "        moirai_shared_dir = \"moirai_cache_global\",\n",
    "    ),\n",
    "    data = SimpleNamespace(\n",
    "        L = 24, H = 1, pred_offset=3,\n",
    "        baseline_occ_only = True,\n",
    "        moirai_channels = [\"occ\"], # occ, dur, etc...\n",
    "        train_range = (\"2022-09-01\", \"2022-10-31\"),\n",
    "        val_range   = (\"2022-11-01\", \"2022-11-15\"),\n",
    "        test_range  = (\"2022-11-16\", \"2022-11-30\"),\n",
    "        paths = PATHS,\n",
    "        use_poi = True,\n",
    "        poi_radius_beta = 0.7, poi_rmin = 300.0, poi_rmax = 2000.0,\n",
    "        sample_stride = 1, min_spatial_neighbors = 1,\n",
    "        spatial_usekeys = ('occ','dur','vol')\n",
    "    ),\n",
    "    moirai = SimpleNamespace(\n",
    "        model_id = \"Salesforce/moirai-1.0-R-base\",\n",
    "        patch_sizes = [1],\n",
    "        pool = \"mean\",\n",
    "        emb_dim = 768,\n",
    "        batch_size = 128,\n",
    "        cache_file = \"moirai_cache.pt\",   # ì‹¤ì œ ì €ì¥ì€ out.embed_dir ì•„ë˜\n",
    "    ),\n",
    "    model = SimpleNamespace(\n",
    "        D_moirai = 768,     # â† ì‹¤ì œ ì„ë² ë”© ì°¨ì›ìœ¼ë¡œ ëŸ°íƒ€ì„ì— ê°±ì‹ ë¨\n",
    "        D_model  = 256,\n",
    "        dropout  = 0.1,\n",
    "        nhead    = 8,\n",
    "        head_hidden = 512,\n",
    "        nonneg_head = False,  # í‘œì¤€í™” íƒ€ê¹ƒ â†’ ìŒìˆ˜ í—ˆìš©\n",
    "        mode = \"baseline\",\n",
    "        HIDDEN = 64, KERNEL_SIZE = 5,\n",
    "        fusion_layers = 1,\n",
    "        head_kind = \"linear\",\n",
    "    ),\n",
    "    train = SimpleNamespace(\n",
    "        epochs = 3, batch_size = 64,\n",
    "        lr = 1e-3, weight_decay = 1e-4,\n",
    "        optimizer = \"adamw\",\n",
    "        scheduler = None,#\"onecycle\",\n",
    "        clip_grad = 1.0,\n",
    "        early_stop_patience = 5,\n",
    "        metrics = [\"RMSE\",\"MAPE\",\"RAE\", \"MAE\", ],\n",
    "    ),\n",
    "    out = SimpleNamespace(\n",
    "        run_id = \"baseline_occOnly\",\n",
    "        run_dir = \"runs/baseline_occOnly\",\n",
    "        history_dir = \"runs/baseline_occOnly/history\",\n",
    "        ckpt_dir = \"runs/baseline_occOnly/checkpoints\",\n",
    "        embed_dir = \"runs/baseline_occOnly/embeddings\",\n",
    "        artifacts_dir = \"runs/baseline_occOnly/artifacts\",\n",
    "        config_dump = \"runs/baseline_occOnly/config.yaml\",\n",
    "        train_log_csv = \"runs/baseline_occOnly/history/train_log.csv\",\n",
    "        summary_json = \"runs/baseline_occOnly/history/summary.json\",\n",
    "        best_ckpt = \"runs/baseline_occOnly/checkpoints/best.ckpt\",\n",
    "        last_ckpt = \"runs/baseline_occOnly/checkpoints/last.ckpt\",\n",
    "        epoch_ckpt_tmpl = \"runs/baseline_occOnly/checkpoints/epoch_{:04d}.ckpt\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "for d in [cfg.out.run_dir, cfg.out.history_dir, cfg.out.ckpt_dir, cfg.out.embed_dir, cfg.out.artifacts_dir]:\n",
    "    os.makedirs(d, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[norm_version] cd544fe327329a25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_375074/3627107336.py:563: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  moirai_emb = torch.load(emb_path, map_location=\"cpu\")  # í…ì„œë§Œ ë¡œë“œ â†’ ì•ˆì „\n",
      "[MoiraiCache] encode: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1247/1247 [14:40<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1/3] LR=1.000e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6192/6192 [17:59<00:00,  5.74it/s, loss=0.0110, n_valid=51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Loss: 0.0390 | iters: 6192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1449/1449 [03:55<00:00,  6.16it/s, loss=0.0264, n_valid=3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Val  Loss: 0.0091 | RMSE:0.0912 | MAPE:15.5635 | RAE:0.1045 | MAE:0.0624\n",
      " âœ… Best model updated @ epoch 1 (ValLoss 0.0091)\n",
      "\n",
      "[Epoch 2/3] LR=1.000e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6192/6192 [17:47<00:00,  5.80it/s, loss=0.0091, n_valid=51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Loss: 0.0119 | iters: 6192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1449/1449 [03:53<00:00,  6.21it/s, loss=0.0026, n_valid=3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Val  Loss: 0.0020 | RMSE:0.0426 | MAPE:8.5801 | RAE:0.0440 | MAE:0.0263\n",
      " âœ… Best model updated @ epoch 2 (ValLoss 0.0020)\n",
      "\n",
      "[Epoch 3/3] LR=1.000e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6192/6192 [17:47<00:00,  5.80it/s, loss=0.0033, n_valid=51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Loss: 0.0107 | iters: 6192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1449/1449 [03:52<00:00,  6.22it/s, loss=0.0138, n_valid=3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Val  Loss: 0.0031 | RMSE:0.0531 | MAPE:9.5594 | RAE:0.0561 | MAE:0.0347\n",
      "\n",
      "Done. Best epoch=2 | best val loss=0.0020 | time=4616.2s\n"
     ]
    }
   ],
   "source": [
    "prepared = run_train(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
