{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import random, math, json, hashlib\n",
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from contextlib import nullcontext\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import amp\n",
    "from tqdm.auto import tqdm\n",
    "import chinese_calendar as cc\n",
    "\n",
    "from uni2ts.model.moirai import MoiraiModule\n",
    "\n",
    "\n",
    "# Device / Seed\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "SEED = 0\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "# 경로 & 설정\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "DATA_DIR = ROOT / \"UrbanEV\" / \"data\"\n",
    "PATHS = {\n",
    "    \"occ\": str(DATA_DIR / \"occupancy.csv\"),\n",
    "    \"dur\": str(DATA_DIR / \"duration.csv\"),\n",
    "    \"vol\": str(DATA_DIR / \"volume.csv\"),\n",
    "    \"e_price\": str(DATA_DIR / \"e_price.csv\"),\n",
    "    \"s_price\": str(DATA_DIR / \"s_price.csv\"),\n",
    "    \"weather\": str(DATA_DIR / \"weather_central.csv\"),\n",
    "    \"inf\": str(DATA_DIR / \"inf.csv\"),\n",
    "    \"adj\": str(DATA_DIR / \"adj.csv\"),\n",
    "    \"dist\": str(DATA_DIR / \"distance.csv\"),\n",
    "    \"poi\": str(DATA_DIR / \"poi.csv\"),\n",
    "}\n",
    "\n",
    "cfg = SimpleNamespace(\n",
    "    exec = SimpleNamespace(\n",
    "        new_model_train = True,\n",
    "        poi_shared_dir = \"poi_cache_global\",\n",
    "        moirai_shared_dir = \"moirai_cache_global\",\n",
    "    ),\n",
    "    data = SimpleNamespace(\n",
    "        L = 24, H = 1, pred_offset=3,\n",
    "        baseline_occ_only = True,\n",
    "        train_range = (\"2022-09-01\", \"2022-09-03\"),\n",
    "        val_range   = (\"2022-09-05\", \"2022-09-06\"),\n",
    "        test_range  = (\"2022-09-07\", \"2022-09-08\"),\n",
    "        paths = PATHS,\n",
    "        use_poi = True,\n",
    "        poi_radius_beta = 0.7, poi_rmin = 300.0, poi_rmax = 2000.0,\n",
    "        sample_stride = 1, min_spatial_neighbors = 1,\n",
    "    ),\n",
    "    moirai = SimpleNamespace(\n",
    "        model_id = \"Salesforce/moirai-1.0-R-base\",\n",
    "        patch_sizes = [1],\n",
    "        pool = \"mean\",\n",
    "        emb_dim = 768,\n",
    "        batch_size = 64,\n",
    "        cache_file = \"moirai_cache.pt\",   # 실제 저장은 out.embed_dir 아래\n",
    "    ),\n",
    "    model = SimpleNamespace(\n",
    "        D_moirai = 768,     # ← 실제 임베딩 차원으로 런타임에 갱신됨\n",
    "        D_model  = 256,\n",
    "        dropout  = 0.1,\n",
    "        nhead    = 8,\n",
    "        head_hidden = 512,\n",
    "        nonneg_head = False,  # 표준화 타깃 → 음수 허용\n",
    "        mode = \"baseline\",\n",
    "        HIDDEN = 64, KERNEL_SIZE = 5,\n",
    "        fusion_layers = 1,\n",
    "        head_kind = \"linear\",\n",
    "    ),\n",
    "    train = SimpleNamespace(\n",
    "        epochs = 4, batch_size = 64,\n",
    "        lr = 1e-3, weight_decay = 1e-4,\n",
    "        optimizer = \"adamw\",\n",
    "        scheduler = \"onecycle\",\n",
    "        clip_grad = 1.0,\n",
    "        early_stop_patience = 5,\n",
    "        metrics = [\"MSE\", \"RMSE\", \"MAE\", \"sMAPE\"],\n",
    "    ),\n",
    "    out = SimpleNamespace(\n",
    "        run_id = \"baseline_occOnly\",\n",
    "        run_dir = \"runs/baseline_occOnly\",\n",
    "        history_dir = \"runs/baseline_occOnly/history\",\n",
    "        ckpt_dir = \"runs/baseline_occOnly/checkpoints\",\n",
    "        embed_dir = \"runs/baseline_occOnly/embeddings\",\n",
    "        artifacts_dir = \"runs/baseline_occOnly/artifacts\",\n",
    "        config_dump = \"runs/baseline_occOnly/config.yaml\",\n",
    "        train_log_csv = \"runs/baseline_occOnly/history/train_log.csv\",\n",
    "        summary_json = \"runs/baseline_occOnly/history/summary.json\",\n",
    "        best_ckpt = \"runs/baseline_occOnly/checkpoints/best.ckpt\",\n",
    "        last_ckpt = \"runs/baseline_occOnly/checkpoints/last.ckpt\",\n",
    "        epoch_ckpt_tmpl = \"runs/baseline_occOnly/checkpoints/epoch_{:04d}.ckpt\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "for d in [cfg.out.run_dir, cfg.out.history_dir, cfg.out.ckpt_dir, cfg.out.embed_dir, cfg.out.artifacts_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def build_clock(train_range, test_range):\n",
    "    start = pd.to_datetime(train_range[0])\n",
    "    end   = pd.to_datetime(test_range[1]) + pd.Timedelta(hours=23)\n",
    "    return pd.date_range(start=start, end=end, freq=\"h\")\n",
    "\n",
    "def read_ts_csv(path):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    return df.sort_index().apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "def reindex_local(df, clock):  return df.reindex(clock).fillna(0)\n",
    "def reindex_price(df, clock):  return df.reindex(clock).ffill().bfill()\n",
    "\n",
    "def reindex_weather(df, clock):\n",
    "    out = df.reindex(clock)\n",
    "    cont_cols = [c for c in out.columns if c.lower() not in (\"nrain\",\"rain\",\"n_rain\")]\n",
    "    rain_cols = [c for c in out.columns if c.lower() in (\"nrain\",\"rain\",\"n_rain\")]\n",
    "    if cont_cols: out[cont_cols] = out[cont_cols].interpolate(\"time\").ffill().bfill()\n",
    "    if rain_cols: out[rain_cols] = out[rain_cols].ffill().bfill().clip(lower=0).astype(int)\n",
    "    return out\n",
    "\n",
    "def load_inf(path):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    cols = [\"longitude\",\"latitude\",\"charge_count\",\"area\",\"perimeter\"]\n",
    "    for c in cols: df[c] = pd.to_numeric(df.get(c, np.nan), errors=\"coerce\")\n",
    "    df.index = df.index.astype(str)\n",
    "    return df[cols]\n",
    "\n",
    "def load_matrix(path, as_float=False):\n",
    "    df = pd.read_csv(path)\n",
    "    df.index = df.columns.astype(str)\n",
    "    df.columns = df.columns.astype(str)\n",
    "    if df.index.duplicated().any():\n",
    "        print(f\"[warn] duplicated index in {path}, keeping first\")\n",
    "        df = df.loc[~df.index.duplicated(keep='first')]\n",
    "    if df.columns.duplicated().any():\n",
    "        print(f\"[warn] duplicated columns in {path}, keeping first\")\n",
    "        df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
    "    df = df.astype(float if as_float else int)\n",
    "    np.fill_diagonal(df.values, 0)\n",
    "    return df\n",
    "\n",
    "def precompute_time(clock):\n",
    "    hrs  = clock.hour; dows = clock.dayofweek\n",
    "    hour_sin = np.sin(2*np.pi*hrs/24).astype(np.float32)\n",
    "    hour_cos = np.cos(2*np.pi*hrs/24).astype(np.float32)\n",
    "    dow_sin  = np.sin(2*np.pi*dows/7).astype(np.float32)\n",
    "    dow_cos  = np.cos(2*np.pi*dows/7).astype(np.float32)\n",
    "    is_weekend = ((dows==5)|(dows==6)).astype(np.float32)\n",
    "    is_holiday = np.array([float(cc.is_holiday(ts.date())) for ts in clock], dtype=np.float32)\n",
    "    return pd.DataFrame({\n",
    "        \"hour_sin\":hour_sin,\"hour_cos\":hour_cos,\n",
    "        \"dow_sin\":dow_sin,\"dow_cos\":dow_cos,\n",
    "        \"is_weekend\":is_weekend,\"is_holiday_cn\":is_holiday\n",
    "    }, index=clock)\n",
    "\n",
    "def prepare_data(cfg):\n",
    "    clock = build_clock(cfg.data.train_range, cfg.data.test_range)\n",
    "    occ = reindex_local(read_ts_csv(cfg.data.paths[\"occ\"]), clock)\n",
    "    dur = reindex_local(read_ts_csv(cfg.data.paths[\"dur\"]), clock)\n",
    "    vol = reindex_local(read_ts_csv(cfg.data.paths[\"vol\"]), clock)\n",
    "    epr = reindex_price(read_ts_csv(cfg.data.paths[\"e_price\"]), clock)\n",
    "    spr = reindex_price(read_ts_csv(cfg.data.paths[\"s_price\"]), clock)\n",
    "    wth = reindex_weather(read_ts_csv(cfg.data.paths[\"weather\"]), clock)\n",
    "    inf = load_inf(cfg.data.paths[\"inf\"])\n",
    "    adj = load_matrix(cfg.data.paths[\"adj\"])\n",
    "    dist= load_matrix(cfg.data.paths[\"dist\"], as_float=True)\n",
    "\n",
    "    tables = {\"occ\":occ,\"dur\":dur,\"vol\":vol,\"e_price\":epr,\"s_price\":spr,\n",
    "              \"weather\":wth,\"inf\":inf,\"adj\":adj,\"distance\":dist,\"time\":precompute_time(clock)}\n",
    "\n",
    "    if cfg.data.use_poi and os.path.exists(cfg.data.paths[\"poi\"]):\n",
    "        tables[\"poi_raw\"] = pd.read_csv(cfg.data.paths[\"poi\"])\n",
    "\n",
    "    zone_ids = sorted(list(set(occ.columns) & set(epr.columns) & set(spr.columns)))\n",
    "    if len(zone_ids) != len(set(zone_ids)): raise ValueError(\"zone_ids duplicated\")\n",
    "\n",
    "    for k in [\"occ\",\"dur\",\"vol\",\"e_price\",\"s_price\"]:\n",
    "        tables[k] = tables[k][zone_ids]\n",
    "    tables[\"adj\"]      = tables[\"adj\"].reindex(index=zone_ids, columns=zone_ids, fill_value=0)\n",
    "    tables[\"distance\"] = tables[\"distance\"].reindex(index=zone_ids, columns=zone_ids, fill_value=float('inf'))\n",
    "    tables[\"inf\"]      = tables[\"inf\"].reindex(index=zone_ids)\n",
    "\n",
    "    datahash  = hashlib.sha256(str(cfg.data.paths).encode()).hexdigest()[:16]\n",
    "    clockhash = hashlib.sha256(str(clock).encode()).hexdigest()[:16]\n",
    "    hashes = SimpleNamespace(datahash=datahash, clockhash=clockhash)\n",
    "    return clock, tables, zone_ids, hashes\n",
    "\n",
    "def compute_fit_stats(tables, train_times, zone_ids):\n",
    "    stats = {}\n",
    "    for key in [\"occ\",\"e_price\",\"s_price\"]:\n",
    "        df = tables[key].loc[train_times, zone_ids]\n",
    "        mu, sd = float(np.nanmean(df.values)), float(np.nanstd(df.values)) or 1.0\n",
    "        stats[key] = {\"mean\":mu,\"std\":sd}\n",
    "    wdf = tables[\"weather\"].loc[train_times]\n",
    "    for c in [\"T\",\"P0\",\"P\",\"U\",\"Td\"]:\n",
    "        if c in wdf.columns:\n",
    "            mu, sd = float(np.nanmean(wdf[c])), float(np.nanstd(wdf[c])) or 1.0\n",
    "        else:\n",
    "            mu, sd = 0.0, 1.0\n",
    "        stats[c] = {\"mean\":mu,\"std\":sd}\n",
    "    inf = tables[\"inf\"]\n",
    "    for c in [\"charge_count\",\"area\",\"perimeter\"]:\n",
    "        vals = pd.to_numeric(inf[c], errors=\"coerce\").values\n",
    "        mu, sd = float(np.nanmean(vals)), float(np.nanstd(vals)) or 1.0\n",
    "        stats[c] = {\"mean\":mu,\"std\":sd}\n",
    "    stats_json = json.dumps(stats, sort_keys=True)\n",
    "    stats[\"norm_version\"] = hashlib.sha256(stats_json.encode()).hexdigest()[:16]\n",
    "    return stats\n",
    "\n",
    "def standardize_tables(tables, stats):\n",
    "    for key in [\"occ\",\"e_price\",\"s_price\"]:\n",
    "        mu, sd = stats[key][\"mean\"], stats[key][\"std\"]\n",
    "        tables[key] = ((tables[key] - mu) / sd).astype(np.float32)\n",
    "    w = tables[\"weather\"]\n",
    "    for c in [\"T\",\"P0\",\"P\",\"U\",\"Td\"]:\n",
    "        if c in w.columns:\n",
    "            mu, sd = stats[c][\"mean\"], stats[c][\"std\"]\n",
    "            w[c] = ((w[c] - mu) / sd).astype(np.float32)\n",
    "\n",
    "def build_sample_indices(tables, clock, zone_ids, cfg):\n",
    "    L, H = cfg.data.L, cfg.data.H\n",
    "    O = int(getattr(cfg.data, \"pred_offset\", 1))  \n",
    "\n",
    "    # ----- split 라벨링 -----\n",
    "    split_sr = pd.Series(index=clock, data=\"none\")\n",
    "    tr_s, tr_e = pd.to_datetime(cfg.data.train_range[0]), pd.to_datetime(cfg.data.train_range[1])\n",
    "    va_s, va_e = pd.to_datetime(cfg.data.val_range[0]),   pd.to_datetime(cfg.data.val_range[1])\n",
    "    te_s, te_e = pd.to_datetime(cfg.data.test_range[0]),  pd.to_datetime(cfg.data.test_range[1])\n",
    "    split_sr.loc[(split_sr.index>=tr_s)&(split_sr.index<=tr_e)] = \"train\"\n",
    "    split_sr.loc[(split_sr.index>=va_s)&(split_sr.index<=va_e)] = \"val\"\n",
    "    split_sr.loc[(split_sr.index>=te_s)&(split_sr.index<=te_e)] = \"test\"\n",
    "\n",
    "    # ----- 통계/표준화 -----\n",
    "    train_times = split_sr[split_sr==\"train\"].index\n",
    "    stats = compute_fit_stats(tables, train_times, zone_ids)\n",
    "    standardize_tables(tables, stats)\n",
    "\n",
    "    # ----- 입력창 유효 마스크 (L 연속) -----\n",
    "    local_valid  = tables[\"occ\"].notna().rolling(L).sum().eq(L).shift(-(L-1), fill_value=False)\n",
    "    price_valid  = (\n",
    "        tables[\"e_price\"].notna().rolling(L).sum().eq(L) &\n",
    "        tables[\"s_price\"].notna().rolling(L).sum().eq(L)\n",
    "    ).shift(-(L-1), fill_value=False)\n",
    "\n",
    "    w_cont  = tables[\"weather\"][[\"T\",\"P0\",\"P\",\"U\",\"Td\"]].notna().all(axis=1).rolling(L).sum().eq(L)\n",
    "    w_valid = w_cont.shift(-(L-1), fill_value=False)\n",
    "    w_valid = pd.DataFrame(np.repeat(w_valid.values[:,None], len(zone_ids), axis=1),\n",
    "                           index=clock, columns=zone_ids)\n",
    "\n",
    "    # ----- 타깃 유효 마스크 (t0+O부터 H개) -----\n",
    "    if H == 1:\n",
    "        # 한 시점만: t0+O 가 존재/유효해야 함\n",
    "        target_valid = tables[\"occ\"].notna().shift(-O, fill_value=False)\n",
    "    else:\n",
    "        # 연속 H개: 오프셋만큼 당긴 뒤 rolling(H)로 H개 연속 체크\n",
    "        target_valid = (\n",
    "            tables[\"occ\"].notna()\n",
    "            .shift(-O, fill_value=False)         # t0 기준 O시간 뒤로 정렬\n",
    "            .rolling(H).sum().eq(H)              # H개 연속 True\n",
    "            .shift(-(H-1), fill_value=False)     # 시작점 기준으로 되돌림\n",
    "        )\n",
    "\n",
    "    combined = local_valid & price_valid & w_valid & target_valid\n",
    "\n",
    "    # ----- split별 시작점 필터링 -----\n",
    "    out = {}\n",
    "    for split in [\"train\",\"val\",\"test\"]:\n",
    "        s_mask = (split_sr == split)\n",
    "\n",
    "        # 입력창 L개가 split 내부에 모두 있어야 함\n",
    "        in_start = s_mask.rolling(L).sum().eq(L).shift(-(L-1), fill_value=False)\n",
    "\n",
    "        # 타깃의 마지막 시점(t0 + O + (H-1))도 같은 split 내부여야 안전\n",
    "        last_tgt_shift = -(O + (H - 1))   # H=1이면 -(O)\n",
    "        tgt_ok = s_mask.shift(last_tgt_shift, fill_value=False)\n",
    "\n",
    "        time_ok = in_start & tgt_ok\n",
    "        valid = combined.copy()\n",
    "        valid[~time_ok] = False\n",
    "\n",
    "        flat = valid.stack().reset_index()\n",
    "        flat = flat[flat[0]].drop(columns=0)\n",
    "        flat.columns = [\"t_start\",\"zone_id\"]\n",
    "        flat[\"zone_idx\"] = flat[\"zone_id\"].map({z:i for i,z in enumerate(zone_ids)})\n",
    "        flat[\"L\"] = L\n",
    "        flat[\"H\"] = H\n",
    "        flat[\"occ_only\"] = int(cfg.data.baseline_occ_only)\n",
    "        flat[\"split\"] = split\n",
    "        out[split] = flat.reset_index(drop=True)\n",
    "\n",
    "    return out, stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POI utilities \n",
    "def compute_zone_radius(inf, beta=0.7, r_min=300, r_max=2000):\n",
    "    A = inf[\"area\"].to_numpy(); P = inf[\"perimeter\"].to_numpy()\n",
    "    r_area  = np.sqrt(np.clip(A,0,None)/np.pi)\n",
    "    r_perim = P/(2*np.pi)\n",
    "    r = beta*r_area + (1-beta)*r_perim\n",
    "    return pd.Series(np.clip(r, r_min, r_max), index=inf.index)\n",
    "\n",
    "def build_poi_counts(inf, poi, radius):\n",
    "    from sklearn.neighbors import BallTree\n",
    "    poi_lat = np.deg2rad(poi[\"latitude\"].values)\n",
    "    poi_lon = np.deg2rad(poi[\"longitude\"].values)\n",
    "    tree = BallTree(np.c_[poi_lat, poi_lon], metric=\"haversine\")\n",
    "    types = poi[\"primary_types\"].str.lower()\n",
    "\n",
    "    life_mask = types.str.contains(\"lifestyle services\")\n",
    "    bres_mask = types.str.contains(\"business and residential\")\n",
    "    food_mask = types.str.contains(\"food and beverage services\")\n",
    "\n",
    "    zlat = np.deg2rad(inf[\"latitude\"].values); zlon = np.deg2rad(inf[\"longitude\"].values)\n",
    "    out = np.zeros((len(inf),3), dtype=int)\n",
    "    for i in tqdm(range(len(inf)), desc=\"POI\"):\n",
    "        r_rad = radius.iloc[i] / 6371000.0\n",
    "        idxs = tree.query_radius([[zlat[i], zlon[i]]], r=r_rad)[0]\n",
    "        out[i,0] = life_mask.iloc[idxs].sum()\n",
    "        out[i,1] = bres_mask.iloc[idxs].sum()\n",
    "        out[i,2] = food_mask.iloc[idxs].sum()\n",
    "    return pd.DataFrame(out, index=inf.index,\n",
    "                        columns=[\"poi_lifestyle\",\"poi_business_residential\",\"poi_food_beverage\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moirai cache (keying fixed)\n",
    "KEY_COLS = [\"zone_id\",\"t_start_iso\",\"L\",\"occ_only\",\"model_id\",\"psig\",\"pool\",\"clock_hash\",\"norm_version\",\"datahash\"]\n",
    "\n",
    "def make_moirai_keys(si, hashes, stats, cfg):\n",
    "    df = si.copy()\n",
    "    df[\"t_start_iso\"] = df[\"t_start\"].dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    df[\"L\"] = df[\"L\"].astype(int)\n",
    "    df[\"occ_only\"] = df[\"occ_only\"].astype(int)\n",
    "    df[\"model_id\"] = cfg.moirai.model_id\n",
    "    df[\"psig\"] = \"-\".join(map(str, cfg.moirai.patch_sizes))\n",
    "    df[\"pool\"] = cfg.moirai.pool\n",
    "    df[\"clock_hash\"] = hashes.clockhash\n",
    "    df[\"norm_version\"] = stats[\"norm_version\"]\n",
    "    df[\"datahash\"] = hashes.datahash\n",
    "    return df[KEY_COLS]\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_batch(x_np, backbone, patch_sizes, pool):\n",
    "    x = torch.from_numpy(x_np).to(DEVICE).float()  # (B, C, L) ; 여기선 C=1\n",
    "    B, C, L = x.shape\n",
    "    target = x.transpose(1,2)  # (B, L, C)\n",
    "    observed_mask = torch.ones_like(target, dtype=torch.bool)\n",
    "    prediction_mask = torch.zeros(B, L, dtype=torch.bool, device=DEVICE)\n",
    "    sample_id = torch.arange(B, device=DEVICE).unsqueeze(1).expand(B, L)\n",
    "    time_id   = torch.arange(L, device=DEVICE).unsqueeze(0).expand(B, L)\n",
    "    variate_id= torch.zeros(B, L, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    hs_list = []\n",
    "    for ps in patch_sizes:\n",
    "        patch_size = torch.full((B, L), ps, dtype=torch.long, device=DEVICE)\n",
    "        try:\n",
    "            hs = backbone.encode(target=target, observed_mask=observed_mask, patch_size=patch_size)\n",
    "        except AttributeError:\n",
    "            output = backbone(target=target, observed_mask=observed_mask,\n",
    "                              prediction_mask=prediction_mask, sample_id=sample_id,\n",
    "                              time_id=time_id, variate_id=variate_id, patch_size=patch_size)\n",
    "            hs = output if isinstance(output, torch.Tensor) else getattr(output, \"mean\", output.sample())\n",
    "        if hs.dim()==3: hs = hs.mean(dim=1) if pool==\"mean\" else hs.max(dim=1)[0]\n",
    "        elif hs.dim()!=2: raise ValueError(f\"Unexpected hs shape: {hs.shape}\")\n",
    "        hs_list.append(hs)\n",
    "    emb = torch.stack(hs_list).mean(dim=0)  # (B, D_emb)\n",
    "    return emb.cpu().numpy()\n",
    "\n",
    "def _sanitize(s:str)->str:\n",
    "    return \"\".join(ch if (str(ch).isalnum() or ch in \"-._\") else \"_\" for ch in str(s))\n",
    "\n",
    "def _make_cache_path_with_dim(cfg, model_id, patch_sig, pool, emb_dim, hashes, stats):\n",
    "    sig = {\"model\":model_id,\"psig\":patch_sig,\"pool\":pool,\"edim\":str(emb_dim),\n",
    "           \"clock\":hashes.clockhash[:8],\"norm\":stats[\"norm_version\"][:8],\"data\":hashes.datahash[:8]}\n",
    "    fname = \"moirai_\" + \"_\".join(f\"{k}-{_sanitize(v)}\" for k,v in sig.items()) + \".pt\"\n",
    "    return os.path.join(cfg.out.embed_dir, fname)\n",
    "\n",
    "def ensure_moirai_cache(keys_df, tables, cfg, hashes, stats):\n",
    "    backbone = MoiraiModule.from_pretrained(cfg.moirai.model_id).to(DEVICE).eval()\n",
    "    patch_sig = \"-\".join(map(str, cfg.moirai.patch_sizes)); pool = cfg.moirai.pool\n",
    "\n",
    "    # 실제 emb_dim 프로빙\n",
    "    first_row = keys_df.iloc[0]\n",
    "    z, t0 = first_row[\"zone_id\"], pd.to_datetime(first_row[\"t_start_iso\"])\n",
    "    x_probe = np.expand_dims(tables[\"occ\"].loc[slice(t0, t0+pd.Timedelta(hours=cfg.data.L-1)), z].values.astype(np.float32), 0)  # (1,L)\n",
    "    x_probe = x_probe[None, ...]  # (1,1,L)\n",
    "    probe_np = encode_batch(x_probe, backbone, cfg.moirai.patch_sizes, pool)  # (1,E)\n",
    "    emb_dim = int(probe_np.shape[1])\n",
    "\n",
    "    cache_file = _make_cache_path_with_dim(cfg, cfg.moirai.model_id, patch_sig, pool, emb_dim, hashes, stats)\n",
    "    os.makedirs(os.path.dirname(cache_file), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(cache_file):\n",
    "        moirai_df, moirai_emb = torch.load(cache_file, map_location=\"cpu\")\n",
    "        if moirai_emb.ndim!=2 or moirai_emb.shape[1]!=emb_dim:\n",
    "            print(f\"[MoiraiCache] dim mismatch in file: {moirai_emb.shape} vs emb_dim={emb_dim} → reset\")\n",
    "            moirai_df = pd.DataFrame(columns=keys_df.columns)\n",
    "            moirai_emb = torch.zeros(0, emb_dim, dtype=torch.float32)\n",
    "    else:\n",
    "        moirai_df = pd.DataFrame(columns=keys_df.columns)\n",
    "        moirai_emb = torch.zeros(0, emb_dim, dtype=torch.float32)\n",
    "\n",
    "    key2row = {tuple(row[KEY_COLS]): i for i, (_, row) in enumerate(moirai_df.iterrows())}\n",
    "    todo_rows = [r for _, r in keys_df.iterrows() if tuple(r[KEY_COLS]) not in key2row]\n",
    "\n",
    "    if len(todo_rows) > 0:\n",
    "        B = cfg.moirai.batch_size\n",
    "        for i in tqdm(range(0, len(todo_rows), B), desc=\"[MoiraiCache] encode\"):\n",
    "            batch_rows = todo_rows[i:i+B]\n",
    "            xs = []\n",
    "            for r in batch_rows:\n",
    "                z = r[\"zone_id\"]; t0 = pd.to_datetime(r[\"t_start_iso\"])\n",
    "                occ = tables[\"occ\"].loc[slice(t0, t0+pd.Timedelta(hours=cfg.data.L-1)), z].values.astype(np.float32)\n",
    "                xs.append(occ[None, :])  # (1,L)\n",
    "            x_np = np.stack(xs).astype(np.float32)  # (b,1,L)\n",
    "\n",
    "            y_np = encode_batch(x_np, backbone, cfg.moirai.patch_sizes, pool)  # (b,E)\n",
    "            new_emb = torch.from_numpy(y_np).float()   # CPU\n",
    "\n",
    "            if new_emb.shape[1] != moirai_emb.shape[1]:\n",
    "                print(f\"[MoiraiCache] runtime dim mismatch: cache={moirai_emb.shape[1]} vs new={new_emb.shape[1]} → reset\")\n",
    "                moirai_df = pd.DataFrame(columns=keys_df.columns)\n",
    "                moirai_emb = torch.zeros(0, new_emb.shape[1], dtype=torch.float32)\n",
    "\n",
    "            batch_df = pd.DataFrame(batch_rows, columns=keys_df.columns)\n",
    "            moirai_df = pd.concat([moirai_df, batch_df], ignore_index=True)\n",
    "            moirai_emb = torch.cat([moirai_emb, new_emb], dim=0)\n",
    "\n",
    "            torch.save((moirai_df, moirai_emb), cache_file + \".tmp\")\n",
    "        os.replace(cache_file + \".tmp\", cache_file)\n",
    "\n",
    "    return moirai_df, moirai_emb, emb_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature builders\n",
    "def fb_local(tables, z, t0, L, occ_only=True):\n",
    "    win = slice(t0, t0 + pd.Timedelta(hours=L-1))\n",
    "    occ = tables[\"occ\"].loc[win, z].values.astype(np.float32)\n",
    "    return occ[None, :]  # (1, L)\n",
    "\n",
    "def fb_price(tables, z, t0, L):\n",
    "    win = slice(t0, t0 + pd.Timedelta(hours=L-1))\n",
    "    epr = tables[\"e_price\"].loc[win, z].values.astype(np.float32)\n",
    "    spr = tables[\"s_price\"].loc[win, z].values.astype(np.float32)\n",
    "    return np.stack([epr, spr]).astype(np.float32)\n",
    "\n",
    "def fb_weather(tables, t0, L):\n",
    "    win = slice(t0, t0 + pd.Timedelta(hours=L-1))\n",
    "    w = tables[\"weather\"].loc[win]\n",
    "    cont = w[[\"T\",\"P0\",\"P\",\"U\",\"Td\"]].values.T.astype(np.float32)\n",
    "    rain = w.get(\"nRAIN\", pd.Series(0, index=w.index)).clip(0,3).values\n",
    "    rain_oh = np.zeros((4, L), dtype=np.float32); rain_oh[rain, np.arange(L)] = 1.0\n",
    "    return np.concatenate([cont, rain_oh], axis=0).astype(np.float32)\n",
    "\n",
    "def fb_spatial(tables, z, t0, L, zone_ids):\n",
    "    win = slice(t0, t0 + pd.Timedelta(hours=L-1))\n",
    "    occ_win = tables[\"occ\"].loc[win].values.astype(np.float32)\n",
    "    zi = zone_ids.index(z)\n",
    "    D = tables[\"distance\"].values.astype(np.float32)\n",
    "    W = 1.0 / (D + 1e-6); W = W / (W.sum(axis=1, keepdims=True) + 1e-6)\n",
    "    mean_occ = occ_win @ W[zi]\n",
    "    self_occ = occ_win[:, zi]\n",
    "    gap = self_occ - mean_occ\n",
    "    ratio = np.clip(self_occ / (mean_occ + 1e-6), 0, 5)\n",
    "    return np.stack([mean_occ, gap, ratio]).astype(np.float32)\n",
    "\n",
    "def fb_static(tables, z):\n",
    "    row = tables[\"inf\"].loc[z]\n",
    "    meta = row[[\"charge_count\",\"area\",\"perimeter\"]].values.astype(np.float32)\n",
    "    poi  = tables[\"poi_counts\"].loc[z].values.astype(np.float32) if \"poi_counts\" in tables else np.zeros(3, dtype=np.float32)\n",
    "    return np.concatenate([meta, poi]).astype(np.float32)\n",
    "\n",
    "def fb_time(tables, t0, L):\n",
    "    win = slice(t0, t0 + pd.Timedelta(hours=L-1))\n",
    "    return tables[\"time\"].loc[win].values.T.astype(np.float32)\n",
    "\n",
    "def fb_target(tables, z, t0, H, offset=None):\n",
    "    if offset is None:\n",
    "        offset = getattr(cfg.data, \"pred_offset\", 1)  # 기본 1\n",
    "    start = t0 + pd.Timedelta(hours=offset)\n",
    "    end   = start + pd.Timedelta(hours=H-1)\n",
    "    if H == 1:\n",
    "        y = tables[\"occ\"].loc[start, z].astype(np.float32)\n",
    "        y = np.array([y], dtype=np.float32)\n",
    "        mask = np.isfinite(y)\n",
    "        return y, mask\n",
    "    else:\n",
    "        y_idx = slice(start, end)\n",
    "        y = tables[\"occ\"].loc[y_idx, z].values.astype(np.float32)\n",
    "        mask = np.isfinite(y)\n",
    "        return y, mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset / Collate\n",
    "class EVDataset(Dataset):\n",
    "    def __init__(self, df, tables, zone_ids):\n",
    "        self.df = df.reset_index(drop=True); self.tables = tables; self.zone_ids = list(zone_ids)\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.df.iloc[idx]\n",
    "        z, t0, L, H = r[\"zone_id\"], r[\"t_start\"], int(r[\"L\"]), int(r[\"H\"])\n",
    "        x_weather = fb_weather(self.tables, t0, L)\n",
    "        x_price   = fb_price(self.tables, z, t0, L)\n",
    "        x_spatial = fb_spatial(self.tables, z, t0, L, self.zone_ids)\n",
    "        x_static  = fb_static(self.tables, z)\n",
    "        x_time    = fb_time(self.tables, t0, L)\n",
    "        y, mask   = fb_target(self.tables, z, t0, H)\n",
    "        return {\n",
    "            \"zone_id\": z,\n",
    "            \"x_weather\": x_weather, \"x_price\": x_price, \"x_spatial\": x_spatial,\n",
    "            \"x_static\": x_static, \"x_time\": x_time, \"y\": y, \"mask\": mask,\n",
    "            \"moirai_key\": {\n",
    "                \"zone_id\": z, \"t_start_iso\": pd.to_datetime(t0).strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                \"L\": L, \"occ_only\": 1, \"model_id\": cfg.moirai.model_id,\n",
    "                \"psig\": \"-\".join(map(str, cfg.moirai.patch_sizes)), \"pool\": cfg.moirai.pool,\n",
    "                \"clock_hash\": \"\", \"norm_version\":\"\", \"datahash\":\"\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "def collate_fn_builder(moirai_emb, key2row, hashes, stats):\n",
    "    def collate_fn(batch):\n",
    "        to_f32 = lambda xs: torch.from_numpy(np.stack(xs)).float().to(DEVICE)\n",
    "        out = {\n",
    "            \"x_weather\": to_f32([b[\"x_weather\"] for b in batch]),\n",
    "            \"x_price\":   to_f32([b[\"x_price\"]   for b in batch]),\n",
    "            \"x_spatial\": to_f32([b[\"x_spatial\"] for b in batch]),\n",
    "            \"x_static\":  to_f32([b[\"x_static\"]  for b in batch]),\n",
    "            \"x_time\":    to_f32([b[\"x_time\"]    for b in batch]),\n",
    "            \"y\":         to_f32([b[\"y\"]         for b in batch]),\n",
    "            \"mask\": torch.from_numpy(np.stack([b[\"mask\"] for b in batch])).to(DEVICE)\n",
    "        }\n",
    "        rows = []\n",
    "        for b in batch:\n",
    "            mk = dict(b[\"moirai_key\"])\n",
    "            mk[\"clock_hash\"] = hashes.clockhash; mk[\"norm_version\"] = stats[\"norm_version\"]; mk[\"datahash\"] = hashes.datahash\n",
    "            rows.append([mk[k] for k in KEY_COLS])\n",
    "        keys_df = pd.DataFrame(rows, columns=KEY_COLS)\n",
    "        idxs = [key2row[tuple(r)] for _, r in keys_df.iterrows()]\n",
    "        out[\"h_local_moirai\"] = moirai_emb[idxs].to(DEVICE).float()\n",
    "        return out\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class EVForecastModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        D   = cfg.model.D_model\n",
    "        D_m = cfg.model.D_moirai\n",
    "        p   = cfg.model.dropout\n",
    "\n",
    "        self.adapter = nn.Sequential(nn.Linear(D_m, D), nn.GELU(), nn.Dropout(p), nn.Linear(D, D))\n",
    "        ks = cfg.model.KERNEL_SIZE; pad = ks//2\n",
    "\n",
    "        def conv_block(cin):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(cin, 64, ks, padding=pad), nn.GELU(),\n",
    "                nn.Conv1d(64, 64, ks, padding=pad), nn.GELU()\n",
    "            )\n",
    "        self.enc_weather = conv_block(9)\n",
    "        self.enc_price   = conv_block(2)\n",
    "        self.enc_spatial = conv_block(3)\n",
    "        self.enc_time    = conv_block(6)\n",
    "\n",
    "        self.enc_static  = nn.Sequential(nn.Linear(6, 128), nn.GELU(), nn.Dropout(p), nn.Linear(128, D))\n",
    "        self.proj_weather= nn.Linear(64, D)\n",
    "        self.proj_price  = nn.Linear(64, D)\n",
    "        self.proj_spatial= nn.Linear(64, D)\n",
    "        self.proj_time   = nn.Linear(64, D)\n",
    "\n",
    "        def align(): return nn.Sequential(nn.LayerNorm(D), nn.Dropout(p), nn.Linear(D, D))\n",
    "        self.align_local = align(); self.align_weather = align(); self.align_price = align()\n",
    "        self.align_spatial = align(); self.align_static = align(); self.align_time = align()\n",
    "\n",
    "        self.mha  = nn.MultiheadAttention(D, cfg.model.nhead, dropout=p, batch_first=True)\n",
    "        self.head = nn.Linear(D, cfg.data.H)\n",
    "        self.use_softplus = bool(getattr(cfg.model, \"nonneg_head\", False))\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        h_local   = self.adapter(batch[\"h_local_moirai\"])\n",
    "        h_weather = self.proj_weather(self.enc_weather(batch[\"x_weather\"]).mean(-1))\n",
    "        h_price   = self.proj_price  (self.enc_price  (batch[\"x_price\"  ]).mean(-1))\n",
    "        h_spatial = self.proj_spatial(self.enc_spatial(batch[\"x_spatial\"]).mean(-1))\n",
    "        h_time    = self.proj_time   (self.enc_time   (batch[\"x_time\"   ]).mean(-1))\n",
    "        h_static  = self.enc_static(batch[\"x_static\"])\n",
    "\n",
    "        def gate(align, h): return torch.sigmoid(align(h)) * h\n",
    "        h_local_a   = gate(self.align_local,   h_local)\n",
    "        h_weather_a = gate(self.align_weather, h_weather)\n",
    "        h_price_a   = gate(self.align_price,   h_price)\n",
    "        h_spatial_a = gate(self.align_spatial, h_spatial)\n",
    "        h_static_a  = gate(self.align_static,  h_static)\n",
    "        h_time_a    = gate(self.align_time,    h_time)\n",
    "\n",
    "        tokens = torch.stack([h_weather_a, h_price_a, h_spatial_a, h_static_a, h_time_a], dim=1)\n",
    "        q = h_local_a.unsqueeze(1)\n",
    "        h, _ = self.mha(q, tokens, tokens)\n",
    "        y_hat = self.head(h.squeeze(1))\n",
    "        return self.softplus(y_hat) if self.use_softplus else y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Val\n",
    "def safe_save(obj, path):\n",
    "    tmp = path + \".tmp\"; torch.save(obj, tmp); os.replace(tmp, path)\n",
    "\n",
    "def train_epoch(model, loader, opt, sched, scaler, use_amp):\n",
    "    model.train(); total = 0.0\n",
    "    amp_ctx = amp.autocast(device_type=\"cuda\", enabled=use_amp) if use_amp else nullcontext()\n",
    "    for batch in tqdm(loader, desc=\"train\"):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with amp_ctx:\n",
    "            y_hat = model(batch)\n",
    "            loss = F.mse_loss(y_hat[batch[\"mask\"]], batch[\"y\"][batch[\"mask\"]])\n",
    "        if use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            if cfg.train.clip_grad and cfg.train.clip_grad>0:\n",
    "                scaler.unscale_(opt); torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.train.clip_grad)\n",
    "            scaler.step(opt); scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            if cfg.train.clip_grad and cfg.train.clip_grad>0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.train.clip_grad)\n",
    "            opt.step()\n",
    "        if sched is not None: sched.step()\n",
    "        total += loss.item()\n",
    "    return total / max(1, len(loader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_epoch(model, loader, use_amp=False):\n",
    "    model.eval(); total = 0.0\n",
    "    amp_ctx = amp.autocast(device_type=\"cuda\", enabled=use_amp) if use_amp else nullcontext()\n",
    "    for batch in tqdm(loader, desc=\"valid\"):\n",
    "        with amp_ctx:\n",
    "            y_hat = model(batch)\n",
    "            loss = F.mse_loss(y_hat[batch[\"mask\"]], batch[\"y\"][batch[\"mask\"]])\n",
    "        total += loss.item()\n",
    "    return total / max(1, len(loader))\n",
    "\n",
    "def run_train(cfg):\n",
    "    clock, tables, zone_ids, hashes = prepare_data(cfg)\n",
    "\n",
    "    # POI (optional)\n",
    "    try:\n",
    "        if cfg.data.use_poi:\n",
    "            from sklearn.neighbors import BallTree  # import check\n",
    "            radius = compute_zone_radius(tables[\"inf\"], beta=cfg.data.poi_radius_beta,\n",
    "                                         r_min=cfg.data.poi_rmin, r_max=cfg.data.poi_rmax)\n",
    "            tables[\"poi_counts\"] = build_poi_counts(tables[\"inf\"], tables[\"poi_raw\"], radius)\n",
    "    except ModuleNotFoundError:\n",
    "        print(\"[warn] scikit-learn not installed; skip POI\"); cfg.data.use_poi = False\n",
    "\n",
    "    si, stats = build_sample_indices(tables, clock, zone_ids, cfg)\n",
    "    print(f\"[norm_version] {stats['norm_version']}\")\n",
    "\n",
    "    keys_train_val = make_moirai_keys(pd.concat([si[\"train\"], si[\"val\"]], ignore_index=True), hashes, stats, cfg)\n",
    "    moirai_df, moirai_emb, emb_dim = ensure_moirai_cache(keys_train_val, tables, cfg, hashes, stats)\n",
    "\n",
    "    # Moirai 임베딩 차원 반영\n",
    "    cfg.model.D_moirai = int(emb_dim)\n",
    "    print(f\"[Moirai] emb_dim={emb_dim} -> adapter: {cfg.model.D_moirai} → {cfg.model.D_model}\")\n",
    "\n",
    "    key2row = {tuple(r[KEY_COLS]): i for i, (_, r) in enumerate(moirai_df.iterrows())}\n",
    "\n",
    "    train_ds = EVDataset(si[\"train\"], tables, zone_ids)\n",
    "    val_ds   = EVDataset(si[\"val\"],   tables, zone_ids)\n",
    "    collate_fn = collate_fn_builder(moirai_emb, key2row, hashes, stats)\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.train.batch_size, shuffle=True,  collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=cfg.train.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = EVForecastModel(cfg).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.train.lr, weight_decay=cfg.train.weight_decay)\n",
    "    if str(cfg.train.scheduler).lower() == \"onecycle\":\n",
    "        sched = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=cfg.train.lr,\n",
    "                                                    epochs=cfg.train.epochs,\n",
    "                                                    steps_per_epoch=max(1,len(train_loader)))\n",
    "    else:\n",
    "        sched = None\n",
    "\n",
    "    use_amp = (DEVICE==\"cuda\" and torch.cuda.is_available())\n",
    "    scaler  = amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    # ----- Train with per-epoch + best/last ckpt -----\n",
    "    best_val = float(\"inf\"); saved_epochs = []; keep_last_k = 0  # 0: 모두 보존\n",
    "    for epoch in range(cfg.train.epochs):\n",
    "        train_loss = train_epoch(model, train_loader, opt, sched, scaler, use_amp=use_amp)\n",
    "        val_loss   = val_epoch(model, val_loader)\n",
    "\n",
    "        # per-epoch checkpoint payload\n",
    "        payload = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"epoch\": epoch+1,\n",
    "            \"val_loss\": float(val_loss),\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"D_moirai\": int(cfg.model.D_moirai),\n",
    "            \"cfg\": cfg.__dict__,\n",
    "        }\n",
    "        ep_path = cfg.out.epoch_ckpt_tmpl.format(epoch+1)\n",
    "        safe_save(payload, ep_path)        # epoch별 저장\n",
    "        safe_save(payload, cfg.out.last_ckpt)  # last 갱신\n",
    "\n",
    "        saved_epochs.append(ep_path)\n",
    "        if keep_last_k > 0 and len(saved_epochs) > keep_last_k:\n",
    "            old = saved_epochs.pop(0)\n",
    "            try: os.remove(old)\n",
    "            except FileNotFoundError: pass\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            safe_save(payload, cfg.out.best_ckpt)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{cfg.train.epochs}] \"\n",
    "              f\"train={train_loss:.4f} | val={val_loss:.4f} | best={best_val:.4f}\")\n",
    "\n",
    "    return {\"clock\": clock, \"tables\": tables, \"zone_ids\": zone_ids, \"hashes\": hashes, \"stats\": stats, \"si\": si}\n",
    "\n",
    "def load_model_from_ckpt(cfg, ckpt_path):\n",
    "    model = EVForecastModel(cfg).to(DEVICE)\n",
    "    payload = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    if \"D_moirai\" in payload and int(payload[\"D_moirai\"]) != int(cfg.model.D_moirai):\n",
    "        cfg.model.D_moirai = int(payload[\"D_moirai\"])\n",
    "        model = EVForecastModel(cfg).to(DEVICE)\n",
    "    model.load_state_dict(payload[\"state_dict\"], strict=True)\n",
    "    model.eval()\n",
    "    return model, payload\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_eval(cfg, si, tables, zone_ids, hashes, stats, which=\"best\", path_override=None):\n",
    "    if which==\"best\":\n",
    "        ckpt_path = cfg.out.best_ckpt\n",
    "    elif which==\"last\":\n",
    "        ckpt_path = cfg.out.last_ckpt\n",
    "    elif which==\"path\" and path_override:\n",
    "        ckpt_path = path_override\n",
    "    else:\n",
    "        raise ValueError(\"which ∈ {'best','last','path(with path_override)'}\")\n",
    "\n",
    "    model, meta = load_model_from_ckpt(cfg, ckpt_path)\n",
    "    print(f\"[Eval] ckpt: {ckpt_path} | epoch={meta.get('epoch')} | val_loss={meta.get('val_loss')}\")\n",
    "\n",
    "    # 테스트용 Moirai 키/캐시 준비 (독립 실행도 고려)\n",
    "    keys_test = make_moirai_keys(si[\"test\"], hashes, stats, cfg)\n",
    "    moirai_df_t, moirai_emb_t, emb_dim_t = ensure_moirai_cache(keys_test, tables, cfg, hashes, stats)\n",
    "\n",
    "    # 모델의 D_moirai가 다르면 재생성\n",
    "    if int(emb_dim_t) != int(cfg.model.D_moirai):\n",
    "        cfg.model.D_moirai = int(emb_dim_t)\n",
    "        model, meta = load_model_from_ckpt(cfg, ckpt_path)\n",
    "\n",
    "    key2row_t = {tuple(r[KEY_COLS]): i for i, (_, r) in enumerate(moirai_df_t.iterrows())}\n",
    "    collate_fn_t = collate_fn_builder(moirai_emb_t, key2row_t, hashes, stats)\n",
    "\n",
    "    test_ds = EVDataset(si[\"test\"], tables, zone_ids)\n",
    "    test_loader = DataLoader(test_ds, batch_size=cfg.train.batch_size, shuffle=False, collate_fn=collate_fn_t)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(test_loader, desc=\"test\"):\n",
    "        y_hat = model(batch)\n",
    "        loss = F.mse_loss(y_hat[batch[\"mask\"]], batch[\"y\"][batch[\"mask\"]])\n",
    "        total_loss += loss.item()\n",
    "    test_mse = total_loss / max(1, len(test_loader))\n",
    "    print(f\"[Test:{which}] MSE={test_mse:.4f}\")\n",
    "    return {\"test_mse\": test_mse, \"ckpt\": ckpt_path, \"epoch\": meta.get(\"epoch\"), \"val_loss\": meta.get(\"val_loss\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared = run_train(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Best 모델로 테스트\n",
    "res_best = run_eval(cfg,\n",
    "                    prepared[\"si\"], prepared[\"tables\"], prepared[\"zone_ids\"],\n",
    "                    prepared[\"hashes\"], prepared[\"stats\"],\n",
    "                    which=\"best\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Last 모델로 테스트\n",
    "res_last = run_eval(cfg,\n",
    "                    prepared[\"si\"], prepared[\"tables\"], prepared[\"zone_ids\"],\n",
    "                    prepared[\"hashes\"], prepared[\"stats\"],\n",
    "                    which=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) 임의 에포크 경로로 테스트 (예: epoch_0002)\n",
    "some_ckpt = cfg.out.epoch_ckpt_tmpl.format(2)\n",
    "if os.path.exists(some_ckpt):\n",
    "    res_path = run_eval(cfg,\n",
    "                        prepared[\"si\"], prepared[\"tables\"], prepared[\"zone_ids\"],\n",
    "                        prepared[\"hashes\"], prepared[\"stats\"],\n",
    "                        which=\"path\", path_override=some_ckpt)\n",
    "else:\n",
    "    print(f\"[Info] {some_ckpt} not found; skip path-based eval.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
